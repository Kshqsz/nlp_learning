# build_chinese_sft_dataset.py
"""
ç”Ÿæˆé«˜è´¨é‡ä¸­æ–‡ SFT æ··åˆæ•°æ®é›†ï¼ˆçº¦ 100MBï¼‰
æ··åˆæ¥æºï¼š
  - Firefly (é€šç”¨ä»»åŠ¡)
  - BelleGroup/train_2M_CN (æŒ‡ä»¤è·Ÿéš)
  - Chinese-Alpaca-2 (åŸºç¡€æŒ‡ä»¤)
  - CMMLU æŒ‡ä»¤åŒ– (å­¦æœ¯/æ¨ç†ï¼Œæå‡ C-Eval)

è¾“å‡ºï¼šchinese_sft_100m.jsonl ï¼ˆæ¯è¡Œä¸€ä¸ª {"input": "...", "target": "..."}ï¼‰
"""

import os
import json
import random
from datasets import load_dataset
from tqdm import tqdm

# è®¾ç½® HF é•œåƒåŠ é€Ÿ
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# è¾“å‡ºè·¯å¾„
OUTPUT_FILE = "chinese_sft_100m.jsonl"
TARGET_SIZE_BYTES = 100 * 1024 * 1024  # 100 MB

# é‡‡æ ·æ¯”ä¾‹ï¼ˆå¯è°ƒæ•´ï¼‰
CONFIG = {
    "firefly": {"weight": 0.35, "max_samples": 40000},
    "belle": {"weight": 0.30, "max_samples": 35000},
    "alpaca": {"weight": 0.20, "max_samples": 20000},
    "cmmlu": {"weight": 0.15, "max_samples": 15000},
}

def format_sample(source, item):
    """ç»Ÿä¸€æ ¼å¼ä¸º {'input': str, 'target': str}"""
    if source == "firefly":
        return {"input": item["input"], "target": item["target"]}
    
    elif source == "belle":
        # Belle: å¯èƒ½æ˜¯å¤šè½®å¯¹è¯æˆ–å•è½®
        if "conversations" in item:
            # å¤šè½®ï¼šå–ç¬¬ä¸€è½® user + ç¬¬ä¸€è½® assistant
            user_msg = next((msg["value"] for msg in item["conversations"] if msg["from"] == "human"), "")
            asst_msg = next((msg["value"] for msg in item["conversations"] if msg["from"] == "gpt"), "")
            return {"input": user_msg, "target": asst_msg}
        else:
            return {"input": item.get("instruction", "") + "\n" + item.get("input", ""), 
                    "target": item["output"]}
    
    elif source == "alpaca":
        inp = item["instruction"]
        if item.get("input", "").strip():
            inp += "\n" + item["input"]
        return {"input": inp, "target": item["output"]}
    
    elif source == "cmmlu":
        return {"input": item["question"], "target": item["answer_text"]}
    else:
        return None

def load_cmmlu_sft(max_samples=15000):
    """åŠ è½½å¹¶æŒ‡ä»¤åŒ– CMMLU æ•°æ®ï¼ˆä»…éƒ¨åˆ†å­¦ç§‘ï¼‰"""
    print("ğŸ“¥ åŠ è½½ CMMLU å¹¶è½¬æ¢ä¸º SFT æ ¼å¼...")
    
    # é€‰æ‹©å¯¹ C-Eval å½±å“å¤§çš„å­¦ç§‘
    subjects = [
        "high_school_physics", "high_school_chemistry", "high_school_biology",
        "college_physics", "college_chemistry", "college_biology",
        "chinese_history", "world_history", "high_school_geography",
        "high_school_mathematics", "college_mathematics", "economics",
        "law", "computer_science"
    ]
    
    all_items = []
    for subject in tqdm(subjects, desc="Processing CMMLU subjects"):
        try:
            ds = load_dataset("hails/cmmlu", subject, split="test")
        except Exception as e:
            print(f"âš ï¸ è·³è¿‡ {subject}: {e}")
            continue
        
        for row in ds:
            if len(all_items) >= max_samples:
                break
            choices = [row["A"], row["B"], row["C"], row["D"]]
            options = "\n".join([f"{chr(65+i)}. {c}" for i, c in enumerate(choices)])
            question = f"ã€å­¦ç§‘ã€‘{subject}\nã€é—®é¢˜ã€‘{row['Question']}\n{options}\n\nè¯·ä»”ç»†åˆ†æå¹¶é€‰å‡ºå”¯ä¸€æ­£ç¡®ç­”æ¡ˆã€‚"
            answer = row["Answer"]
            explanation = f"æ­£ç¡®ç­”æ¡ˆæ˜¯ï¼š{answer}"
            all_items.append({
                "question": question,
                "answer_text": explanation
            })
        if len(all_items) >= max_samples:
            break
    
    random.shuffle(all_items)
    return all_items[:max_samples]

def main():
    random.seed(42)
    
    print("=" * 60)
    print("ğŸš€ æ„å»ºé«˜è´¨é‡ä¸­æ–‡ SFT æ··åˆæ•°æ®é›†ï¼ˆç›®æ ‡ â‰ˆ100MBï¼‰")
    print("=" * 60)
    
    all_samples = []

    # 1. Firefly
    print("\nğŸ“¥ åŠ è½½ Firefly...")
    firefly = load_dataset("YeungNLP/firefly-train-1.1M", split="train")
    firefly_samples = [format_sample("firefly", item) for item in 
                       tqdm(random.sample(list(firefly), CONFIG["firefly"]["max_samples"]))]
    all_samples.extend(firefly_samples)

    # 2. Belle 2M CN
    print("\nğŸ“¥ åŠ è½½ BelleGroup/train_2M_CN...")
    belle = load_dataset("BelleGroup/train_2M_CN", split="train")
    belle_samples = [format_sample("belle", item) for item in 
                     tqdm(random.sample(list(belle), CONFIG["belle"]["max_samples"]))]
    all_samples.extend(belle_samples)

    # 3. Chinese-Alpaca-2
    print("\nğŸ“¥ åŠ è½½ Chinese-Alpaca-2...")
    alpaca = load_dataset("silk-road/alpaca-data-gpt4-chinese", split="train")
    alpaca_samples = [format_sample("alpaca", item) for item in 
                      tqdm(random.sample(list(alpaca), min(CONFIG["alpaca"]["max_samples"], len(alpaca))))]
    all_samples.extend(alpaca_samples)

    # 4. CMMLU (æŒ‡ä»¤åŒ–)
    print("\nğŸ“¥ æ„å»º CMMLU-SFT...")
    cmmlu_samples = load_cmmlu_sft(CONFIG["cmmlu"]["max_samples"])
    cmmlu_formatted = [format_sample("cmmlu", item) for item in cmmlu_samples]
    all_samples.extend(cmmlu_formatted)

    # æ‰“ä¹±é¡ºåº
    random.shuffle(all_samples)
    
    # å†™å…¥æ–‡ä»¶å¹¶æ§åˆ¶å¤§å°
    print(f"\nğŸ’¾ å†™å…¥åˆ° {OUTPUT_FILE} ...")
    current_size = 0
    count = 0
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for sample in tqdm(all_samples, desc="Writing samples"):
            if sample is None or not sample.get("input") or not sample.get("target"):
                continue
            line = json.dumps(sample, ensure_ascii=False) + "\n"
            if current_size + len(line.encode("utf-8")) > TARGET_SIZE_BYTES:
                break
            f.write(line)
            current_size += len(line.encode("utf-8"))
            count += 1

    print(f"\nâœ… å®Œæˆï¼å…±å†™å…¥ {count} æ¡æ ·æœ¬ï¼Œæ–‡ä»¶å¤§å°: {current_size / 1024 / 1024:.1f} MB")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {os.path.abspath(OUTPUT_FILE)}")

if __name__ == "__main__":
    main()