from datasets import load_dataset
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import TrainingArguments, Trainer
import numpy as np
import evaluate
import os
import torch

# æ¨¡å‹ä¿å­˜è·¯å¾„
MODEL_PATH = "./my_bert_model"

# åŠ è½½tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")

# æ£€æŸ¥æ˜¯å¦æœ‰å·²è®­ç»ƒçš„æ¨¡å‹
if os.path.exists(MODEL_PATH):
    print("âœ… å‘ç°å·²è®­ç»ƒçš„æ¨¡å‹ï¼Œç›´æ¥åŠ è½½...")
    model = BertForSequenceClassification.from_pretrained(MODEL_PATH)
else:
    print("â³ æœªå‘ç°å·²è®­ç»ƒçš„æ¨¡å‹ï¼Œå¼€å§‹è®­ç»ƒ...")
    
    dataset = load_dataset("lansinuote/ChnSentiCorp")

    def tokenize(example):
        return tokenizer(
            example["text"],
            truncation = True,
            padding = "max_length",
            max_length = 128
        )

    tokenized_dataset = dataset.map(tokenize, batched = True)

    # åŠ è½½é¢„è®­ç»ƒçš„bert
    model = BertForSequenceClassification.from_pretrained(
        "bert-base-chinese",
        num_labels = 2
    )

    accuracy = evaluate.load("accuracy")

    def compute_metrics(p):
        logits = p.predictions
        preds = np.argmax(logits, axis = 1)
        return accuracy.compute(predictions = preds, references = p.label_ids)

    training_args = TrainingArguments(
        output_dir = "./results",
        eval_strategy = "epoch",
        per_device_train_batch_size = 8,
        per_device_eval_batch_size = 8,
        num_train_epochs = 1,
        logging_steps = 20
    )

    trainer = Trainer(
        model = model,
        args = training_args,
        train_dataset = tokenized_dataset["train"],
        eval_dataset = tokenized_dataset["test"],
        tokenizer = tokenizer,
        compute_metrics = compute_metrics
    )

    trainer.train()
    
    # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
    trainer.save_model(MODEL_PATH)
    tokenizer.save_pretrained(MODEL_PATH)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {MODEL_PATH}")

# æµ‹è¯•é¢„æµ‹
model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
model.to("cpu")  # ç§»åˆ°CPUä¸Šè¿›è¡Œæ¨ç†

text = "è¿™ä¸ªç”µå½±éå¸¸å¥½çœ‹ï¼Œæˆ‘å¾ˆå–œæ¬¢ï¼"                       
inputs = tokenizer(text, return_tensors="pt", truncation = True, padding = True)
logits = model(**inputs).logits
pred = logits.argmax(dim=1).item()
print(f"æµ‹è¯•æ–‡æœ¬ï¼š'{text}'")
print("é¢„æµ‹ï¼š", "æ­£é¢ ğŸ˜Š" if pred == 1 else "è´Ÿé¢ ğŸ˜")
