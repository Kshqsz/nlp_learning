# eval_benchmarks.py
"""
å¤§æ¨¡å‹ç»¼åˆè¯„æµ‹è„šæœ¬

åŒ…å«å¤šä¸ªå¸¸ç”¨åŸºå‡†æµ‹è¯•ï¼š
  - MMLU: å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼ˆ57ä¸ªå­¦ç§‘ï¼‰
  - C-Eval: ä¸­æ–‡çŸ¥è¯†è¯„æµ‹
  - HellaSwag: å¸¸è¯†æ¨ç†
  - ARC: ç§‘å­¦æ¨ç†
  - TruthfulQA: çœŸå®æ€§è¯„æµ‹
  - WinoGrande: å¸¸è¯†æ¨ç†ï¼ˆä»£è¯æ¶ˆæ­§ï¼‰

ä½¿ç”¨æ–¹å¼ï¼š
  python eval_benchmarks.py                    # è¯„æµ‹é¢„è®­ç»ƒæ¨¡å‹
  python eval_benchmarks.py --model sft        # è¯„æµ‹ LoRA SFT æ¨¡å‹
  python eval_benchmarks.py --compare          # å¯¹æ¯”æ‰€æœ‰æ¨¡å‹
  python eval_benchmarks.py --quick            # å¿«é€Ÿæ¨¡å¼ï¼ˆæ¯ä¸ªåŸºå‡†å°‘é‡é‡‡æ ·ï¼‰
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
import argparse
import json
from tqdm import tqdm
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from collections import defaultdict
from datetime import datetime

# ===== é…ç½® =====
ORIGINAL_MODEL_PATH = "/public/huggingface-models/Qwen/Qwen3-1.7B"
PRETRAIN_MODEL_PATH = "./qwen3_1.7b_pretrain"
LORA_SFT_PATH = "./qwen3_1.7b_lora_sft"

# å¿«é€Ÿæµ‹è¯•çš„ MMLU å­¦ç§‘
QUICK_MMLU_SUBJECTS = [
    "high_school_mathematics",
    "high_school_physics", 
    "high_school_computer_science",
    "machine_learning",
    "logical_fallacies",
]

# å¿«é€Ÿæµ‹è¯•çš„ C-Eval å­¦ç§‘
QUICK_CEVAL_SUBJECTS = [
    "computer_network",
    "operating_system",
    "discrete_mathematics",
    "high_school_physics",
    "high_school_chemistry",
]


class ModelLoader:
    """æ¨¡å‹åŠ è½½å™¨"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.current_model_type = None
    
    def load(self, model_type="pretrain"):
        """åŠ è½½æ¨¡å‹"""
        if self.current_model_type == model_type and self.model is not None:
            return self.model, self.tokenizer
        
        # é‡Šæ”¾ä¹‹å‰çš„æ¨¡å‹
        if self.model is not None:
            del self.model
            torch.cuda.empty_cache()
        
        print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {model_type}")
        
        tokenizer = AutoTokenizer.from_pretrained(ORIGINAL_MODEL_PATH, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        if model_type == "original":
            model_path = ORIGINAL_MODEL_PATH
            model = AutoModelForCausalLM.from_pretrained(
                model_path, trust_remote_code=True,
                torch_dtype=torch.bfloat16, device_map="auto"
            )
        elif model_type == "pretrain":
            model_path = PRETRAIN_MODEL_PATH if os.path.exists(PRETRAIN_MODEL_PATH) else ORIGINAL_MODEL_PATH
            model = AutoModelForCausalLM.from_pretrained(
                model_path, trust_remote_code=True,
                torch_dtype=torch.bfloat16, device_map="auto"
            )
        elif model_type == "sft":
            base_path = PRETRAIN_MODEL_PATH if os.path.exists(PRETRAIN_MODEL_PATH) else ORIGINAL_MODEL_PATH
            base_model = AutoModelForCausalLM.from_pretrained(
                base_path, trust_remote_code=True,
                torch_dtype=torch.bfloat16, device_map="auto"
            )
            model = PeftModel.from_pretrained(base_model, LORA_SFT_PATH)
        else:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹ç±»å‹: {model_type}")
        
        model.eval()
        self.model = model
        self.tokenizer = tokenizer
        self.current_model_type = model_type
        
        return model, tokenizer


def get_logits_for_choices(model, tokenizer, prompt, choices):
    """è·å–å„é€‰é¡¹çš„ logits åˆ†æ•°"""
    inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits[0, -1, :]
    
    choice_scores = []
    for choice in choices:
        # è·å–é€‰é¡¹é¦–å­—æ¯çš„ token id
        token_id = tokenizer.encode(choice, add_special_tokens=False)[0]
        choice_scores.append(logits[token_id].item())
    
    return choice_scores


def evaluate_mmlu(model, tokenizer, num_samples=100, subjects=None):
    """
    MMLU è¯„æµ‹ (Massive Multitask Language Understanding)
    è‹±æ–‡å¤šä»»åŠ¡çŸ¥è¯†è¯„æµ‹ï¼Œ57ä¸ªå­¦ç§‘
    """
    print("\nğŸ“Š è¯„æµ‹ MMLU...")
    
    if subjects is None:
        subjects = QUICK_MMLU_SUBJECTS
    
    all_correct = 0
    all_total = 0
    subject_results = {}
    
    for subject in tqdm(subjects, desc="MMLU"):
        try:
            dataset = load_dataset("cais/mmlu", subject, split="test", trust_remote_code=True)
            dev_set = load_dataset("cais/mmlu", subject, split="dev", trust_remote_code=True)
        except Exception as e:
            print(f"   è·³è¿‡ {subject}: {e}")
            continue
        
        # Few-shot ç¤ºä¾‹
        few_shot = ""
        for i in range(min(5, len(dev_set))):
            item = dev_set[i]
            few_shot += f"Question: {item['question']}\n"
            for j, c in enumerate(item['choices']):
                few_shot += f"{chr(65+j)}. {c}\n"
            few_shot += f"Answer: {chr(65 + item['answer'])}\n\n"
        
        # è¯„æµ‹
        if num_samples and len(dataset) > num_samples:
            dataset = dataset.select(range(num_samples))
        
        correct = 0
        for item in dataset:
            prompt = few_shot + f"Question: {item['question']}\n"
            for j, c in enumerate(item['choices']):
                prompt += f"{chr(65+j)}. {c}\n"
            prompt += "Answer:"
            
            scores = get_logits_for_choices(model, tokenizer, prompt, ["A", "B", "C", "D"])
            pred = scores.index(max(scores))
            if pred == item['answer']:
                correct += 1
        
        acc = correct / len(dataset) if len(dataset) > 0 else 0
        subject_results[subject] = acc
        all_correct += correct
        all_total += len(dataset)
    
    overall_acc = all_correct / all_total if all_total > 0 else 0
    return overall_acc * 100, subject_results


def evaluate_ceval(model, tokenizer, num_samples=100, subjects=None):
    """
    C-Eval è¯„æµ‹
    ä¸­æ–‡çŸ¥è¯†è¯„æµ‹åŸºå‡†
    """
    print("\nğŸ“Š è¯„æµ‹ C-Eval...")
    
    if subjects is None:
        subjects = QUICK_CEVAL_SUBJECTS
    
    all_correct = 0
    all_total = 0
    
    for subject in tqdm(subjects, desc="C-Eval"):
        try:
            dataset = load_dataset("ceval/ceval-exam", subject, split="val", trust_remote_code=True)
            dev_set = load_dataset("ceval/ceval-exam", subject, split="dev", trust_remote_code=True)
        except Exception as e:
            print(f"   è·³è¿‡ {subject}: {e}")
            continue
        
        # Few-shot ç¤ºä¾‹
        few_shot = ""
        for i in range(min(5, len(dev_set))):
            item = dev_set[i]
            few_shot += f"é—®é¢˜ï¼š{item['question']}\n"
            few_shot += f"A. {item['A']}\nB. {item['B']}\nC. {item['C']}\nD. {item['D']}\n"
            few_shot += f"ç­”æ¡ˆï¼š{item['answer']}\n\n"
        
        if num_samples and len(dataset) > num_samples:
            dataset = dataset.select(range(num_samples))
        
        correct = 0
        for item in dataset:
            prompt = few_shot + f"é—®é¢˜ï¼š{item['question']}\n"
            prompt += f"A. {item['A']}\nB. {item['B']}\nC. {item['C']}\nD. {item['D']}\n"
            prompt += "ç­”æ¡ˆï¼š"
            
            scores = get_logits_for_choices(model, tokenizer, prompt, ["A", "B", "C", "D"])
            pred_idx = scores.index(max(scores))
            pred = chr(65 + pred_idx)
            if pred == item['answer']:
                correct += 1
        
        all_correct += correct
        all_total += len(dataset)
    
    overall_acc = all_correct / all_total if all_total > 0 else 0
    return overall_acc * 100


def evaluate_hellaswag(model, tokenizer, num_samples=200):
    """
    HellaSwag è¯„æµ‹
    å¸¸è¯†æ¨ç†ï¼šé€‰æ‹©æœ€åˆç†çš„å¥å­ç»­å†™
    """
    print("\nğŸ“Š è¯„æµ‹ HellaSwag...")
    
    try:
        dataset = load_dataset("Rowan/hellaswag", split="validation", trust_remote_code=True)
    except Exception as e:
        print(f"   åŠ è½½å¤±è´¥: {e}")
        return None
    
    if num_samples and len(dataset) > num_samples:
        dataset = dataset.select(range(num_samples))
    
    correct = 0
    for item in tqdm(dataset, desc="HellaSwag"):
        ctx = item['ctx']
        endings = item['endings']
        label = int(item['label'])
        
        # è®¡ç®—æ¯ä¸ªç»­å†™çš„å›°æƒ‘åº¦
        scores = []
        for ending in endings:
            text = ctx + " " + ending
            inputs = tokenizer(text, return_tensors="pt")
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model(**inputs, labels=inputs["input_ids"])
                # è´Ÿ loss ä½œä¸ºåˆ†æ•°ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
                scores.append(-outputs.loss.item())
        
        pred = scores.index(max(scores))
        if pred == label:
            correct += 1
    
    return correct / len(dataset) * 100


def evaluate_arc(model, tokenizer, num_samples=200, difficulty="easy"):
    """
    ARC è¯„æµ‹ (AI2 Reasoning Challenge)
    ç§‘å­¦æ¨ç†é€‰æ‹©é¢˜
    """
    print(f"\nğŸ“Š è¯„æµ‹ ARC-{difficulty}...")
    
    try:
        config = "ARC-Easy" if difficulty == "easy" else "ARC-Challenge"
        dataset = load_dataset("allenai/ai2_arc", config, split="test", trust_remote_code=True)
    except Exception as e:
        print(f"   åŠ è½½å¤±è´¥: {e}")
        return None
    
    if num_samples and len(dataset) > num_samples:
        dataset = dataset.select(range(num_samples))
    
    correct = 0
    for item in tqdm(dataset, desc=f"ARC-{difficulty}"):
        question = item['question']
        choices = item['choices']
        answer_key = item['answerKey']
        
        # æ„å»º prompt
        prompt = f"Question: {question}\n"
        choice_labels = choices['label']
        choice_texts = choices['text']
        for label, text in zip(choice_labels, choice_texts):
            prompt += f"{label}. {text}\n"
        prompt += "Answer:"
        
        # è·å–é¢„æµ‹
        scores = get_logits_for_choices(model, tokenizer, prompt, choice_labels)
        pred_idx = scores.index(max(scores))
        pred = choice_labels[pred_idx]
        
        if pred == answer_key:
            correct += 1
    
    return correct / len(dataset) * 100


def evaluate_winogrande(model, tokenizer, num_samples=200):
    """
    WinoGrande è¯„æµ‹
    å¸¸è¯†æ¨ç†ï¼šä»£è¯æ¶ˆæ­§
    """
    print("\nğŸ“Š è¯„æµ‹ WinoGrande...")
    
    try:
        dataset = load_dataset("allenai/winogrande", "winogrande_xl", split="validation", trust_remote_code=True)
    except Exception as e:
        print(f"   åŠ è½½å¤±è´¥: {e}")
        return None
    
    if num_samples and len(dataset) > num_samples:
        dataset = dataset.select(range(num_samples))
    
    correct = 0
    for item in tqdm(dataset, desc="WinoGrande"):
        sentence = item['sentence']
        option1 = item['option1']
        option2 = item['option2']
        answer = item['answer']  # "1" æˆ– "2"
        
        # å°† _ æ›¿æ¢ä¸ºé€‰é¡¹ï¼Œè®¡ç®—å›°æƒ‘åº¦
        scores = []
        for option in [option1, option2]:
            text = sentence.replace("_", option)
            inputs = tokenizer(text, return_tensors="pt")
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model(**inputs, labels=inputs["input_ids"])
                scores.append(-outputs.loss.item())
        
        pred = "1" if scores[0] > scores[1] else "2"
        if pred == answer:
            correct += 1
    
    return correct / len(dataset) * 100


def evaluate_truthfulqa(model, tokenizer, num_samples=200):
    """
    TruthfulQA è¯„æµ‹
    æµ‹è¯•æ¨¡å‹ç”ŸæˆçœŸå®ç­”æ¡ˆçš„èƒ½åŠ›ï¼ˆMC1 å¤šé€‰ä¸€ï¼‰
    """
    print("\nğŸ“Š è¯„æµ‹ TruthfulQA...")
    
    try:
        dataset = load_dataset("truthfulqa/truthful_qa", "multiple_choice", split="validation", trust_remote_code=True)
    except Exception as e:
        print(f"   åŠ è½½å¤±è´¥: {e}")
        return None
    
    if num_samples and len(dataset) > num_samples:
        dataset = dataset.select(range(num_samples))
    
    correct = 0
    for item in tqdm(dataset, desc="TruthfulQA"):
        question = item['question']
        mc1_targets = item['mc1_targets']
        choices = mc1_targets['choices']
        labels = mc1_targets['labels']  # 1 è¡¨ç¤ºæ­£ç¡®ç­”æ¡ˆ
        
        # æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„ç´¢å¼•
        correct_idx = labels.index(1)
        
        # è®¡ç®—æ¯ä¸ªé€‰é¡¹çš„åˆ†æ•°
        scores = []
        for choice in choices:
            text = f"Question: {question}\nAnswer: {choice}"
            inputs = tokenizer(text, return_tensors="pt")
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model(**inputs, labels=inputs["input_ids"])
                scores.append(-outputs.loss.item())
        
        pred = scores.index(max(scores))
        if pred == correct_idx:
            correct += 1
    
    return correct / len(dataset) * 100


def run_all_benchmarks(model, tokenizer, quick=False):
    """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
    results = {}
    
    # è®¾ç½®é‡‡æ ·æ•°é‡
    n_mmlu = 50 if quick else 100
    n_ceval = 50 if quick else 100
    n_other = 100 if quick else 200
    
    # MMLU
    mmlu_acc, _ = evaluate_mmlu(model, tokenizer, num_samples=n_mmlu)
    results['MMLU'] = mmlu_acc
    
    # C-Eval
    ceval_acc = evaluate_ceval(model, tokenizer, num_samples=n_ceval)
    results['C-Eval'] = ceval_acc
    
    # HellaSwag
    hellaswag_acc = evaluate_hellaswag(model, tokenizer, num_samples=n_other)
    if hellaswag_acc is not None:
        results['HellaSwag'] = hellaswag_acc
    
    # ARC-Easy
    arc_easy_acc = evaluate_arc(model, tokenizer, num_samples=n_other, difficulty="easy")
    if arc_easy_acc is not None:
        results['ARC-Easy'] = arc_easy_acc
    
    # ARC-Challenge
    arc_challenge_acc = evaluate_arc(model, tokenizer, num_samples=n_other, difficulty="challenge")
    if arc_challenge_acc is not None:
        results['ARC-Challenge'] = arc_challenge_acc
    
    # WinoGrande
    winogrande_acc = evaluate_winogrande(model, tokenizer, num_samples=n_other)
    if winogrande_acc is not None:
        results['WinoGrande'] = winogrande_acc
    
    # TruthfulQA
    truthfulqa_acc = evaluate_truthfulqa(model, tokenizer, num_samples=n_other)
    if truthfulqa_acc is not None:
        results['TruthfulQA'] = truthfulqa_acc
    
    return results


def print_results_table(all_results):
    """æ‰“å°å¯¹æ¯”ç»“æœè¡¨æ ¼"""
    print("\n")
    print("=" * 80)
    print("ğŸ“Š ç»¼åˆè¯„æµ‹ç»“æœå¯¹æ¯”è¡¨")
    print("=" * 80)
    
    # è·å–æ‰€æœ‰åŸºå‡†åç§°
    benchmarks = set()
    for results in all_results.values():
        benchmarks.update(results.keys())
    benchmarks = sorted(benchmarks)
    
    # æ‰“å°è¡¨å¤´
    models = list(all_results.keys())
    header = f"{'Benchmark':<15}"
    for model in models:
        header += f" | {model:>12}"
    header += " |"
    
    print("-" * len(header))
    print(header)
    print("-" * len(header))
    
    # æ‰“å°æ¯ä¸ªåŸºå‡†çš„ç»“æœ
    for benchmark in benchmarks:
        row = f"{benchmark:<15}"
        for model in models:
            if benchmark in all_results[model]:
                score = all_results[model][benchmark]
                row += f" | {score:>11.1f}%"
            else:
                row += f" | {'N/A':>12}"
        row += " |"
        print(row)
    
    print("-" * len(header))
    
    # è®¡ç®—å¹³å‡åˆ†
    avg_row = f"{'Average':<15}"
    for model in models:
        scores = [v for v in all_results[model].values() if v is not None]
        avg = sum(scores) / len(scores) if scores else 0
        avg_row += f" | {avg:>11.1f}%"
    avg_row += " |"
    print(avg_row)
    print("=" * len(header))
    
    # æ‰“å° ASCII æŸ±çŠ¶å›¾
    print("\nğŸ“ˆ å¹³å‡åˆ†æŸ±çŠ¶å›¾:")
    print("-" * 50)
    for model in models:
        scores = [v for v in all_results[model].values() if v is not None]
        avg = sum(scores) / len(scores) if scores else 0
        bar_len = int(avg / 2)  # ç¼©æ”¾åˆ° 50 å­—ç¬¦å®½åº¦
        bar = "â–ˆ" * bar_len
        print(f"{model:<12} |{bar} {avg:.1f}%")
    print("-" * 50)


def compare_models(quick=False):
    """å¯¹æ¯”æ‰€æœ‰æ¨¡å‹"""
    print("=" * 80)
    print("ğŸ”¬ å¤§æ¨¡å‹ç»¼åˆè¯„æµ‹")
    print(f"ğŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    loader = ModelLoader()
    all_results = {}
    
    model_types = []
    
    # æ£€æŸ¥å“ªäº›æ¨¡å‹å­˜åœ¨
    if os.path.exists(ORIGINAL_MODEL_PATH):
        model_types.append(("original", "Original"))
    
    if os.path.exists(PRETRAIN_MODEL_PATH):
        model_types.append(("pretrain", "Pretrain"))
    elif os.path.exists(ORIGINAL_MODEL_PATH):
        # å¦‚æœæ²¡æœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼Œç”¨åŸå§‹æ¨¡å‹ä»£æ›¿
        model_types.append(("pretrain", "Pretrain"))
    
    if os.path.exists(LORA_SFT_PATH):
        model_types.append(("sft", "LoRA-SFT"))
    
    for model_type, model_name in model_types:
        print(f"\n{'='*60}")
        print(f"ğŸ”„ è¯„æµ‹æ¨¡å‹: {model_name}")
        print("=" * 60)
        
        try:
            model, tokenizer = loader.load(model_type)
            results = run_all_benchmarks(model, tokenizer, quick)
            all_results[model_name] = results
        except Exception as e:
            print(f"âŒ {model_name} è¯„æµ‹å¤±è´¥: {e}")
            continue
    
    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    if all_results:
        print_results_table(all_results)
        
        # ä¿å­˜ç»“æœ
        output_file = f"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return all_results


def evaluate_single_model(model_type, quick=False):
    """è¯„æµ‹å•ä¸ªæ¨¡å‹"""
    loader = ModelLoader()
    model, tokenizer = loader.load(model_type)
    results = run_all_benchmarks(model, tokenizer, quick)
    
    # æ‰“å°ç»“æœ
    print("\n" + "=" * 50)
    print(f"ğŸ“Š {model_type} æ¨¡å‹è¯„æµ‹ç»“æœ")
    print("=" * 50)
    print(f"{'Benchmark':<20} {'Score':>10}")
    print("-" * 32)
    for benchmark, score in results.items():
        if score is not None:
            print(f"{benchmark:<20} {score:>9.1f}%")
    
    scores = [v for v in results.values() if v is not None]
    avg = sum(scores) / len(scores) if scores else 0
    print("-" * 32)
    print(f"{'Average':<20} {avg:>9.1f}%")
    print("=" * 50)
    
    return results


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å¤§æ¨¡å‹ç»¼åˆè¯„æµ‹")
    parser.add_argument("--model", type=str, default="pretrain",
                        choices=["original", "pretrain", "sft"],
                        help="è¦è¯„æµ‹çš„æ¨¡å‹")
    parser.add_argument("--compare", action="store_true",
                        help="å¯¹æ¯”æ‰€æœ‰æ¨¡å‹")
    parser.add_argument("--quick", action="store_true",
                        help="å¿«é€Ÿæ¨¡å¼ï¼ˆå‡å°‘é‡‡æ ·æ•°é‡ï¼‰")
    parser.add_argument("--benchmark", type=str, default=None,
                        choices=["mmlu", "ceval", "hellaswag", "arc", "winogrande", "truthfulqa"],
                        help="åªè¿è¡ŒæŒ‡å®šçš„åŸºå‡†æµ‹è¯•")
    
    args = parser.parse_args()
    
    if args.compare:
        compare_models(args.quick)
    elif args.benchmark:
        # å•ç‹¬è¿è¡ŒæŸä¸ªåŸºå‡†
        loader = ModelLoader()
        model, tokenizer = loader.load(args.model)
        
        if args.benchmark == "mmlu":
            acc, _ = evaluate_mmlu(model, tokenizer)
            print(f"\nMMLU: {acc:.1f}%")
        elif args.benchmark == "ceval":
            acc = evaluate_ceval(model, tokenizer)
            print(f"\nC-Eval: {acc:.1f}%")
        elif args.benchmark == "hellaswag":
            acc = evaluate_hellaswag(model, tokenizer)
            print(f"\nHellaSwag: {acc:.1f}%")
        elif args.benchmark == "arc":
            acc_easy = evaluate_arc(model, tokenizer, difficulty="easy")
            acc_hard = evaluate_arc(model, tokenizer, difficulty="challenge")
            print(f"\nARC-Easy: {acc_easy:.1f}%")
            print(f"ARC-Challenge: {acc_hard:.1f}%")
        elif args.benchmark == "winogrande":
            acc = evaluate_winogrande(model, tokenizer)
            print(f"\nWinoGrande: {acc:.1f}%")
        elif args.benchmark == "truthfulqa":
            acc = evaluate_truthfulqa(model, tokenizer)
            print(f"\nTruthfulQA: {acc:.1f}%")
    else:
        evaluate_single_model(args.model, args.quick)
