# eval_mmlu.py
"""
MMLU (Massive Multitask Language Understanding) è¯„æµ‹è„šæœ¬

MMLU åŒ…å« 57 ä¸ªå­¦ç§‘çš„é€‰æ‹©é¢˜ï¼Œæµ‹è¯•æ¨¡å‹çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›
å­¦ç§‘æ¶µç›–ï¼šSTEMã€äººæ–‡ã€ç¤¾ä¼šç§‘å­¦ã€å…¶ä»–ï¼ˆæ³•å¾‹ã€åŒ»å­¦ç­‰ï¼‰

è¯„æµ‹æ–¹å¼ï¼š
  - ç»™æ¨¡å‹ä¸€ä¸ªå¤šé€‰é¢˜ï¼ˆA/B/C/Dï¼‰
  - æ¨¡å‹è¾“å‡ºç­”æ¡ˆï¼Œè®¡ç®—å‡†ç¡®ç‡

ä½¿ç”¨æ–¹å¼ï¼š
  python eval_mmlu.py                          # è¯„æµ‹é¢„è®­ç»ƒæ¨¡å‹
  python eval_mmlu.py --model sft              # è¯„æµ‹ LoRA SFT æ¨¡å‹
  python eval_mmlu.py --model original         # è¯„æµ‹åŸå§‹æ¨¡å‹
  python eval_mmlu.py --subjects all           # è¯„æµ‹æ‰€æœ‰å­¦ç§‘ï¼ˆæ…¢ï¼‰
  python eval_mmlu.py --subjects stem          # åªè¯„æµ‹ STEM å­¦ç§‘
  python eval_mmlu.py --num_samples 100        # æ¯ä¸ªå­¦ç§‘é‡‡æ ·æ•°é‡
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
import argparse
from tqdm import tqdm
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from collections import defaultdict

# ===== é…ç½® =====
ORIGINAL_MODEL_PATH = "/public/huggingface-models/Qwen/Qwen3-1.7B"
PRETRAIN_MODEL_PATH = "./qwen3_1.7b_pretrain"
LORA_SFT_PATH = "./qwen3_1.7b_lora_sft"

# MMLU å­¦ç§‘åˆ†ç±»
MMLU_SUBJECTS = {
    "stem": [
        "abstract_algebra", "anatomy", "astronomy", "college_biology",
        "college_chemistry", "college_computer_science", "college_mathematics",
        "college_physics", "computer_security", "conceptual_physics",
        "electrical_engineering", "elementary_mathematics", "high_school_biology",
        "high_school_chemistry", "high_school_computer_science",
        "high_school_mathematics", "high_school_physics", "high_school_statistics",
        "machine_learning"
    ],
    "humanities": [
        "formal_logic", "high_school_european_history", "high_school_us_history",
        "high_school_world_history", "international_law", "jurisprudence",
        "logical_fallacies", "moral_disputes", "moral_scenarios", "philosophy",
        "prehistory", "professional_law", "world_religions"
    ],
    "social_sciences": [
        "econometrics", "high_school_geography", "high_school_government_and_politics",
        "high_school_macroeconomics", "high_school_microeconomics", "high_school_psychology",
        "human_sexuality", "professional_psychology", "public_relations", "security_studies",
        "sociology", "us_foreign_policy"
    ],
    "other": [
        "business_ethics", "clinical_knowledge", "college_medicine", "global_facts",
        "human_aging", "management", "marketing", "medical_genetics", "miscellaneous",
        "nutrition", "professional_accounting", "professional_medicine", "virology"
    ]
}

# å¿«é€Ÿæµ‹è¯•ç”¨çš„ä»£è¡¨æ€§å­¦ç§‘
QUICK_SUBJECTS = [
    "high_school_mathematics",
    "high_school_physics", 
    "high_school_computer_science",
    "college_computer_science",
    "machine_learning",
    "logical_fallacies",
    "world_religions",
    "high_school_geography",
    "marketing",
    "clinical_knowledge",
]


def load_model(model_type="pretrain"):
    """åŠ è½½æ¨¡å‹"""
    print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {model_type}")
    
    # ç¡®å®š tokenizer è·¯å¾„
    tokenizer_path = ORIGINAL_MODEL_PATH
    
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    if model_type == "original":
        model_path = ORIGINAL_MODEL_PATH
        print(f"   ä½¿ç”¨åŸå§‹æ¨¡å‹: {model_path}")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
    elif model_type == "pretrain":
        model_path = PRETRAIN_MODEL_PATH
        if not os.path.exists(model_path):
            print(f"   âš ï¸ é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹")
            model_path = ORIGINAL_MODEL_PATH
        print(f"   ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
    elif model_type == "sft":
        # åŠ è½½ LoRA SFT æ¨¡å‹
        base_path = PRETRAIN_MODEL_PATH if os.path.exists(PRETRAIN_MODEL_PATH) else ORIGINAL_MODEL_PATH
        print(f"   åŸºåº§æ¨¡å‹: {base_path}")
        print(f"   LoRA æƒé‡: {LORA_SFT_PATH}")
        
        base_model = AutoModelForCausalLM.from_pretrained(
            base_path,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        model = PeftModel.from_pretrained(base_model, LORA_SFT_PATH)
    else:
        raise ValueError(f"æœªçŸ¥æ¨¡å‹ç±»å‹: {model_type}")
    
    model.eval()
    return model, tokenizer


def format_mmlu_prompt(question, choices, few_shot_examples=None):
    """
    æ„å»º MMLU è¯„æµ‹ prompt
    
    æ ¼å¼ï¼š
    Question: xxx
    A. xxx
    B. xxx
    C. xxx
    D. xxx
    Answer:
    """
    prompt = ""
    
    # æ·»åŠ  few-shot ç¤ºä¾‹ï¼ˆå¯é€‰ï¼‰
    if few_shot_examples:
        for ex in few_shot_examples:
            prompt += f"Question: {ex['question']}\n"
            for i, choice in enumerate(ex['choices']):
                prompt += f"{chr(65+i)}. {choice}\n"
            prompt += f"Answer: {chr(65 + ex['answer'])}\n\n"
    
    # æ·»åŠ å½“å‰é—®é¢˜
    prompt += f"Question: {question}\n"
    for i, choice in enumerate(choices):
        prompt += f"{chr(65+i)}. {choice}\n"
    prompt += "Answer:"
    
    return prompt


def get_model_answer(model, tokenizer, prompt):
    """
    è·å–æ¨¡å‹çš„ç­”æ¡ˆï¼ˆA/B/C/Dï¼‰
    ä½¿ç”¨ logits æ¯”è¾ƒæ–¹æ³•ï¼Œæ›´å‡†ç¡®
    """
    inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits[0, -1, :]  # æœ€åä¸€ä¸ª token çš„ logits
    
    # è·å– A, B, C, D å¯¹åº”çš„ token id
    choices = ["A", "B", "C", "D"]
    choice_ids = [tokenizer.encode(c, add_special_tokens=False)[0] for c in choices]
    
    # æ¯”è¾ƒè¿™å››ä¸ªé€‰é¡¹çš„ logits
    choice_logits = [logits[cid].item() for cid in choice_ids]
    predicted_idx = choice_logits.index(max(choice_logits))
    
    return predicted_idx


def evaluate_subject(model, tokenizer, subject, num_samples=None, num_few_shot=5):
    """è¯„æµ‹å•ä¸ªå­¦ç§‘"""
    try:
        # åŠ è½½æ•°æ®é›†
        dataset = load_dataset("cais/mmlu", subject, split="test")
        dev_dataset = load_dataset("cais/mmlu", subject, split="dev")
    except Exception as e:
        print(f"   âš ï¸ åŠ è½½ {subject} å¤±è´¥: {e}")
        return None, 0
    
    # å‡†å¤‡ few-shot ç¤ºä¾‹
    few_shot_examples = []
    for i in range(min(num_few_shot, len(dev_dataset))):
        few_shot_examples.append({
            "question": dev_dataset[i]["question"],
            "choices": dev_dataset[i]["choices"],
            "answer": dev_dataset[i]["answer"]
        })
    
    # é‡‡æ ·æµ‹è¯•æ•°æ®
    if num_samples and num_samples < len(dataset):
        indices = list(range(min(num_samples, len(dataset))))
        dataset = dataset.select(indices)
    
    correct = 0
    total = len(dataset)
    
    for item in dataset:
        question = item["question"]
        choices = item["choices"]
        answer = item["answer"]  # 0, 1, 2, 3 å¯¹åº” A, B, C, D
        
        prompt = format_mmlu_prompt(question, choices, few_shot_examples)
        predicted = get_model_answer(model, tokenizer, prompt)
        
        if predicted == answer:
            correct += 1
    
    accuracy = correct / total if total > 0 else 0
    return accuracy, total


def evaluate_mmlu(model, tokenizer, subjects="quick", num_samples=100):
    """è¯„æµ‹ MMLU"""
    print("\n" + "=" * 60)
    print("ğŸ“Š MMLU è¯„æµ‹")
    print("=" * 60)
    
    # ç¡®å®šè¦è¯„æµ‹çš„å­¦ç§‘
    if subjects == "all":
        subject_list = []
        for cat_subjects in MMLU_SUBJECTS.values():
            subject_list.extend(cat_subjects)
    elif subjects == "quick":
        subject_list = QUICK_SUBJECTS
    elif subjects in MMLU_SUBJECTS:
        subject_list = MMLU_SUBJECTS[subjects]
    else:
        subject_list = [subjects]  # å•ä¸ªå­¦ç§‘
    
    print(f"è¯„æµ‹å­¦ç§‘æ•°: {len(subject_list)}")
    print(f"æ¯å­¦ç§‘é‡‡æ ·: {num_samples if num_samples else 'å…¨éƒ¨'}")
    
    results = {}
    category_results = defaultdict(list)
    
    for subject in tqdm(subject_list, desc="è¯„æµ‹è¿›åº¦"):
        accuracy, total = evaluate_subject(model, tokenizer, subject, num_samples)
        if accuracy is not None:
            results[subject] = {"accuracy": accuracy, "total": total}
            
            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            for cat, cat_subjects in MMLU_SUBJECTS.items():
                if subject in cat_subjects:
                    category_results[cat].append(accuracy)
                    break
    
    # æ‰“å°ç»“æœ
    print("\n" + "-" * 60)
    print("ğŸ“ˆ å„å­¦ç§‘å‡†ç¡®ç‡")
    print("-" * 60)
    
    for subject, result in sorted(results.items(), key=lambda x: x[1]["accuracy"], reverse=True):
        acc = result["accuracy"] * 100
        total = result["total"]
        print(f"  {subject:40s}: {acc:5.1f}% ({total} samples)")
    
    # åˆ†ç±»åˆ«ç»Ÿè®¡
    print("\n" + "-" * 60)
    print("ğŸ“Š å„ç±»åˆ«å¹³å‡å‡†ç¡®ç‡")
    print("-" * 60)
    
    total_acc = []
    for cat, accs in category_results.items():
        if accs:
            avg_acc = sum(accs) / len(accs) * 100
            total_acc.extend(accs)
            print(f"  {cat:20s}: {avg_acc:5.1f}% ({len(accs)} subjects)")
    
    # æ€»ä½“å‡†ç¡®ç‡
    if total_acc:
        overall_acc = sum(total_acc) / len(total_acc) * 100
        print("\n" + "=" * 60)
        print(f"ğŸ¯ MMLU æ€»ä½“å‡†ç¡®ç‡: {overall_acc:.1f}%")
        print("=" * 60)
    
    return results, overall_acc if total_acc else 0


def compare_models(subjects="quick", num_samples=50):
    """å¯¹æ¯”ä¸åŒæ¨¡å‹çš„ MMLU åˆ†æ•°"""
    print("=" * 60)
    print("ğŸ”¬ MMLU æ¨¡å‹å¯¹æ¯”è¯„æµ‹")
    print("=" * 60)
    
    results = {}
    
    # è¯„æµ‹åŸå§‹æ¨¡å‹
    print("\n" + "=" * 60)
    print("1ï¸âƒ£ åŸå§‹æ¨¡å‹")
    print("=" * 60)
    try:
        model, tokenizer = load_model("original")
        _, acc = evaluate_mmlu(model, tokenizer, subjects, num_samples)
        results["original"] = acc
        del model
        torch.cuda.empty_cache()
    except Exception as e:
        print(f"âŒ åŸå§‹æ¨¡å‹è¯„æµ‹å¤±è´¥: {e}")
    
    # è¯„æµ‹é¢„è®­ç»ƒæ¨¡å‹
    print("\n" + "=" * 60)
    print("2ï¸âƒ£ é¢„è®­ç»ƒæ¨¡å‹")
    print("=" * 60)
    try:
        model, tokenizer = load_model("pretrain")
        _, acc = evaluate_mmlu(model, tokenizer, subjects, num_samples)
        results["pretrain"] = acc
        del model
        torch.cuda.empty_cache()
    except Exception as e:
        print(f"âŒ é¢„è®­ç»ƒæ¨¡å‹è¯„æµ‹å¤±è´¥: {e}")
    
    # è¯„æµ‹ SFT æ¨¡å‹
    print("\n" + "=" * 60)
    print("3ï¸âƒ£ LoRA SFT æ¨¡å‹")
    print("=" * 60)
    try:
        model, tokenizer = load_model("sft")
        _, acc = evaluate_mmlu(model, tokenizer, subjects, num_samples)
        results["sft"] = acc
        del model
        torch.cuda.empty_cache()
    except Exception as e:
        print(f"âŒ SFT æ¨¡å‹è¯„æµ‹å¤±è´¥: {e}")
    
    # æ‰“å°å¯¹æ¯”ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š MMLU å¯¹æ¯”ç»“æœ")
    print("=" * 60)
    print(f"{'æ¨¡å‹':<15} {'MMLU å‡†ç¡®ç‡':>15}")
    print("-" * 30)
    for model_name, acc in results.items():
        print(f"{model_name:<15} {acc:>14.1f}%")
    
    return results


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="MMLU è¯„æµ‹")
    parser.add_argument("--model", type=str, default="pretrain",
                        choices=["original", "pretrain", "sft"],
                        help="è¦è¯„æµ‹çš„æ¨¡å‹")
    parser.add_argument("--subjects", type=str, default="quick",
                        choices=["all", "quick", "stem", "humanities", "social_sciences", "other"],
                        help="è¦è¯„æµ‹çš„å­¦ç§‘")
    parser.add_argument("--num_samples", type=int, default=100,
                        help="æ¯ä¸ªå­¦ç§‘çš„é‡‡æ ·æ•°é‡")
    parser.add_argument("--compare", action="store_true",
                        help="å¯¹æ¯”æ‰€æœ‰æ¨¡å‹")
    
    args = parser.parse_args()
    
    if args.compare:
        compare_models(args.subjects, args.num_samples)
    else:
        model, tokenizer = load_model(args.model)
        evaluate_mmlu(model, tokenizer, args.subjects, args.num_samples)
