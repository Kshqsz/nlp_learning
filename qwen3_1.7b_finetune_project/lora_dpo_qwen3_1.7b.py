# lora_dpo_qwen3_1.7b.py
"""
Qwen3-1.7B LoRA DPOï¼ˆç›´æ¥åå¥½ä¼˜åŒ–ï¼‰

åœ¨ LoRA SFT æ¨¡å‹åŸºç¡€ä¸Šè¿›è¡Œ DPO åå¥½å¯¹é½
è®©æ¨¡å‹å­¦ä¼šç”Ÿæˆäººç±»æ›´åå¥½çš„å›ç­”

DPO ä¼˜åŠ¿ï¼š
  - ä¸éœ€è¦è®­ç»ƒå¥–åŠ±æ¨¡å‹
  - ä¸éœ€è¦ PPO å¼ºåŒ–å­¦ä¹ 
  - ç›´æ¥ä»åå¥½æ•°æ®å­¦ä¹ 
  - è®­ç»ƒç¨³å®šï¼Œæ•ˆæœå¥½

è¿è¡Œæ–¹å¼ï¼š
  python lora_dpo_qwen3_1.7b.py
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from peft import LoraConfig, get_peft_model, PeftModel, TaskType
from trl import DPOTrainer, DPOConfig

# ===== é…ç½® =====
# åŸå§‹æ¨¡å‹è·¯å¾„ï¼ˆç”¨äºåŠ è½½ tokenizerï¼‰
ORIGINAL_MODEL_PATH = "/public/huggingface-models/Qwen/Qwen3-1.7B"

# SFT æ¨¡å‹è·¯å¾„ï¼ˆDPO çš„åŸºåº§ï¼‰
# æ–¹å¼1ï¼šä½¿ç”¨é¢„è®­ç»ƒ + LoRA SFT çš„æ¨¡å‹
BASE_MODEL_PATH = "/root/data/hsk-models/qwen3_1.7b_pretrain"  # é¢„è®­ç»ƒåçš„æ¨¡å‹
LORA_SFT_PATH = "/root/data/hsk-models/qwen3_1.7b_lora_sft"    # LoRA SFT æƒé‡

# å¦‚æœæ²¡æœ‰é¢„è®­ç»ƒï¼Œå¯ä»¥ç›´æ¥ç”¨åŸå§‹æ¨¡å‹ + LoRA SFT
# BASE_MODEL_PATH = ORIGINAL_MODEL_PATH
# LORA_SFT_PATH = "/root/data/hsk-models/qwen3_1.7b_lora_sft"

OUTPUT_DIR = "/root/data/hsk-models/qwen3_1.7b_lora_dpo"
MAX_LENGTH = 512
BATCH_SIZE = 2
GRADIENT_ACCUMULATION_STEPS = 4  # æœ‰æ•ˆ batch = 8
LEARNING_RATE = 5e-5             # DPO é€šå¸¸ç”¨è¾ƒå°å­¦ä¹ ç‡
NUM_EPOCHS = 1
NUM_SAMPLES = 5000               # DPO æ•°æ®é‡

# LoRA é…ç½®ï¼ˆä¸ SFT ä¿æŒä¸€è‡´ï¼‰
LORA_R = 16
LORA_ALPHA = 32
LORA_DROPOUT = 0.05
TARGET_MODULES = [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj",
]

# DPO é…ç½®
DPO_BETA = 0.1  # KL æ•£åº¦æƒ©ç½šç³»æ•°


def load_model_and_tokenizer():
    """åŠ è½½ SFT åçš„æ¨¡å‹ä½œä¸º DPO çš„åŸºåº§"""
    print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
    
    # ä»åŸå§‹æ¨¡å‹åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(ORIGINAL_MODEL_PATH, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"  # DPO éœ€è¦ left padding
    
    # æ£€æŸ¥åŸºåº§æ¨¡å‹
    if os.path.exists(BASE_MODEL_PATH) and os.path.exists(os.path.join(BASE_MODEL_PATH, "config.json")):
        model_path = BASE_MODEL_PATH
        print(f"   âœ… ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
    else:
        model_path = ORIGINAL_MODEL_PATH
        print(f"   âš ï¸ é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹: {model_path}")
    
    # åŠ è½½åŸºåº§æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
    )
    model = model.cuda()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ SFT LoRA æƒé‡
    if os.path.exists(LORA_SFT_PATH):
        print(f"   ğŸ”§ åŠ è½½ LoRA SFT æƒé‡: {LORA_SFT_PATH}")
        # åŠ è½½ SFT LoRA å¹¶åˆå¹¶åˆ°åŸºåº§æ¨¡å‹
        model = PeftModel.from_pretrained(model, LORA_SFT_PATH)
        model = model.merge_and_unload()  # åˆå¹¶ LoRA åˆ°åŸºåº§
        print("   âœ… LoRA æƒé‡å·²åˆå¹¶")
    else:
        print(f"   âš ï¸ LoRA SFT æƒé‡ä¸å­˜åœ¨: {LORA_SFT_PATH}")
        print("   å°†ç›´æ¥åœ¨åŸºåº§æ¨¡å‹ä¸Šè¿›è¡Œ DPO")
    
    return model, tokenizer


def load_dpo_dataset(tokenizer):
    """åŠ è½½ DPO åå¥½æ•°æ®é›†"""
    print("\nğŸ“Š åŠ è½½ DPO æ•°æ®...")
    
    # å°è¯•åŠ è½½ä¸åŒçš„åå¥½æ•°æ®é›†
    dataset = None
    
    # æ–¹å¼1ï¼šå°è¯•åŠ è½½æœ¬åœ°æ•°æ®
    local_dpo_path = "./dpo_zh.jsonl"
    if os.path.exists(local_dpo_path):
        print(f"   ä½¿ç”¨æœ¬åœ°æ•°æ®: {local_dpo_path}")
        dataset = load_dataset("json", data_files=local_dpo_path, split="train")
    
    # æ–¹å¼2ï¼šå°è¯•åŠ è½½ HuggingFace æ•°æ®é›†
    if dataset is None:
        try:
            print("   å°è¯•åŠ è½½ HuggingFace åå¥½æ•°æ®é›†...")
            # ä¸­æ–‡åå¥½æ•°æ®é›†
            dataset = load_dataset(
                "beyond/rlhf-reward-single-round-trans_chinese",
                split="train",
                trust_remote_code=True
            )
            print("   âœ… åŠ è½½ rlhf-reward-single-round-trans_chinese")
        except Exception as e:
            print(f"   âš ï¸ åŠ è½½å¤±è´¥: {e}")
    
    # æ–¹å¼3ï¼šå°è¯•å¦ä¸€ä¸ªæ•°æ®é›†
    if dataset is None:
        try:
            dataset = load_dataset(
                "Anthropic/hh-rlhf",
                split="train",
                trust_remote_code=True
            )
            print("   âœ… åŠ è½½ Anthropic/hh-rlhf")
        except Exception as e:
            print(f"   âš ï¸ åŠ è½½å¤±è´¥: {e}")
    
    if dataset is None:
        raise ValueError("æ— æ³•åŠ è½½åå¥½æ•°æ®é›†ï¼Œè¯·å‡†å¤‡æœ¬åœ°æ•°æ®æ–‡ä»¶ dpo_data.jsonl")
    
    # é‡‡æ ·
    if len(dataset) > NUM_SAMPLES:
        dataset = dataset.shuffle(seed=42).select(range(NUM_SAMPLES))
    
    print(f"   âœ… åŠ è½½ {len(dataset)} æ¡æ•°æ®")
    return dataset


def preprocess_dpo_data(examples, tokenizer):
    """
    é¢„å¤„ç† DPO æ•°æ®
    DPO éœ€è¦ä¸‰å…ƒç»„ï¼š(prompt, chosen, rejected)
    """
    processed = {
        "prompt": [],
        "chosen": [],
        "rejected": [],
    }
    
    # æ ¹æ®æ•°æ®é›†æ ¼å¼å¤„ç†
    # æ ¼å¼1: å·²æœ‰ prompt, chosen, rejected å­—æ®µ
    if "prompt" in examples and "chosen" in examples and "rejected" in examples:
        for prompt, chosen, rejected in zip(examples["prompt"], examples["chosen"], examples["rejected"]):
            if len(prompt) > 300 or len(chosen) > 400 or len(rejected) > 400:
                continue
            processed["prompt"].append(prompt)
            processed["chosen"].append(chosen)
            processed["rejected"].append(rejected)
    
    # æ ¼å¼2: rlhf-reward æ ¼å¼ (prompt, response, label)
    elif "prompt" in examples and "response" in examples:
        # éœ€è¦æˆå¯¹å¤„ç†ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
        prompts = examples.get("prompt", [])
        responses = examples.get("response", [])
        labels = examples.get("label", [])
        
        # æŒ‰ prompt åˆ†ç»„
        prompt_responses = {}
        for p, r, l in zip(prompts, responses, labels):
            if p not in prompt_responses:
                prompt_responses[p] = {"chosen": None, "rejected": None}
            if l == 1:
                prompt_responses[p]["chosen"] = r
            else:
                prompt_responses[p]["rejected"] = r
        
        for p, resp in prompt_responses.items():
            if resp["chosen"] and resp["rejected"]:
                if len(p) > 300:
                    continue
                processed["prompt"].append(p)
                processed["chosen"].append(resp["chosen"])
                processed["rejected"].append(resp["rejected"])
    
    # æ ¼å¼3: hh-rlhf æ ¼å¼
    elif "chosen" in examples and "rejected" in examples:
        for chosen, rejected in zip(examples["chosen"], examples["rejected"]):
            # ä» chosen/rejected ä¸­æå– prompt
            # hh-rlhf æ ¼å¼: "Human: xxx\n\nAssistant: xxx"
            if "Human:" in chosen and "Assistant:" in chosen:
                parts = chosen.split("Assistant:")
                if len(parts) >= 2:
                    prompt = parts[0].replace("Human:", "").strip()
                    chosen_resp = parts[-1].strip()
                    
                    rej_parts = rejected.split("Assistant:")
                    rejected_resp = rej_parts[-1].strip() if len(rej_parts) >= 2 else rejected
                    
                    if len(prompt) > 300:
                        continue
                    
                    processed["prompt"].append(prompt)
                    processed["chosen"].append(chosen_resp)
                    processed["rejected"].append(rejected_resp)
    
    return processed


def format_for_dpo(example):
    """
    å°†æ•°æ®æ ¼å¼åŒ–ä¸º DPO Trainer éœ€è¦çš„æ ¼å¼
    ä½¿ç”¨ ChatML å¯¹è¯æ ¼å¼
    """
    prompt = example["prompt"]
    chosen = example["chosen"]
    rejected = example["rejected"]
    
    # æ„å»º ChatML æ ¼å¼
    formatted_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    formatted_chosen = f"{chosen}<|im_end|>"
    formatted_rejected = f"{rejected}<|im_end|>"
    
    return {
        "prompt": formatted_prompt,
        "chosen": formatted_chosen,
        "rejected": formatted_rejected,
    }


def main():
    print("=" * 60)
    print("ğŸš€ Qwen3-1.7B LoRA DPO åå¥½å¯¹é½")
    print("=" * 60)
    print(f"åŸºåº§æ¨¡å‹: {BASE_MODEL_PATH}")
    print(f"SFT LoRA: {LORA_SFT_PATH}")
    print(f"DPO Beta: {DPO_BETA}")
    print(f"LoRA Rank: {LORA_R}, Alpha: {LORA_ALPHA}")
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_model_and_tokenizer()
    
    # æ‰“å°æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"\nåŸºåº§æ¨¡å‹å‚æ•°é‡: {total_params / 1e9:.2f}B")
    
    # é…ç½®æ–°çš„ LoRAï¼ˆç”¨äº DPOï¼‰
    print("\nğŸ”§ é…ç½® DPO LoRA...")
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=LORA_DROPOUT,
        target_modules=TARGET_MODULES,
        bias="none",
    )
    
    # åº”ç”¨ LoRA
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # åŠ è½½æ•°æ®
    raw_dataset = load_dpo_dataset(tokenizer)
    
    # é¢„å¤„ç†æ•°æ®
    print("\nğŸ”„ å¤„ç† DPO æ•°æ®...")
    dataset = raw_dataset.map(
        lambda x: preprocess_dpo_data(x, tokenizer),
        batched=True,
        remove_columns=raw_dataset.column_names,
        desc="Preprocessing",
        num_proc=4,
    )
    
    # è¿‡æ»¤ç©ºæ ·æœ¬
    dataset = dataset.filter(lambda x: len(x["prompt"]) > 0 and len(x["chosen"]) > 0 and len(x["rejected"]) > 0)
    print(f"âœ… æœ‰æ•ˆæ ·æœ¬: {len(dataset)}")
    
    # æ ¼å¼åŒ–ä¸º ChatML
    dataset = dataset.map(format_for_dpo, desc="Formatting")
    
    # æ‰“å°æ ·æœ¬ç¤ºä¾‹
    if len(dataset) > 0:
        print("\nğŸ“ æ•°æ®æ ·ä¾‹:")
        print(f"   Prompt: {dataset[0]['prompt'][:100]}...")
        print(f"   Chosen: {dataset[0]['chosen'][:100]}...")
        print(f"   Rejected: {dataset[0]['rejected'][:100]}...")
    
    # DPO è®­ç»ƒé…ç½®
    dpo_config = DPOConfig(
        output_dir=OUTPUT_DIR,
        
        # è®­ç»ƒå‚æ•°
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        
        # å­¦ä¹ ç‡
        learning_rate=LEARNING_RATE,
        lr_scheduler_type="cosine",
        warmup_ratio=0.1,
        
        # DPO å‚æ•°
        beta=DPO_BETA,
        max_length=MAX_LENGTH,
        max_prompt_length=256,
        
        # æ˜¾å­˜ä¼˜åŒ–
        bf16=True,
        gradient_checkpointing=True,
        gradient_checkpointing_kwargs={"use_reentrant": False},
        
        # æ—¥å¿—å’Œä¿å­˜
        logging_steps=10,
        save_steps=500,
        save_total_limit=2,
        
        # å…¶ä»–
        report_to="none",
        remove_unused_columns=False,
    )
    
    # åˆ›å»º DPO Trainer
    print("\n" + "=" * 60)
    print("ğŸ‹ï¸ å¼€å§‹ DPO è®­ç»ƒ")
    print("=" * 60)
    print(f"   è®­ç»ƒæ ·æœ¬: {len(dataset)}")
    print(f"   Batch Size: {BATCH_SIZE}")
    print(f"   æ¢¯åº¦ç´¯ç§¯: {GRADIENT_ACCUMULATION_STEPS}")
    print(f"   æœ‰æ•ˆ Batch: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")
    print(f"   Learning Rate: {LEARNING_RATE}")
    print(f"   DPO Beta: {DPO_BETA}")
    print(f"   Epochs: {NUM_EPOCHS}")
    
    if torch.cuda.is_available():
        print(f"   è®­ç»ƒå‰æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    
    # åˆ›å»ºå‚è€ƒæ¨¡å‹ï¼ˆç”¨äºè®¡ç®— KL æ•£åº¦ï¼‰
    # DPOTrainer ä¼šè‡ªåŠ¨å¤„ç†å‚è€ƒæ¨¡å‹
    trainer = DPOTrainer(
        model=model,
        ref_model=None,  # ä½¿ç”¨ model çš„å‰¯æœ¬ä½œä¸ºå‚è€ƒæ¨¡å‹
        args=dpo_config,
        train_dataset=dataset,
        tokenizer=tokenizer,
    )
    
    # è®­ç»ƒ
    trainer.train()
    
    # ä¿å­˜
    print(f"\nğŸ’¾ ä¿å­˜ LoRA æƒé‡åˆ° {OUTPUT_DIR}...")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    if torch.cuda.is_available():
        peak_memory = torch.cuda.max_memory_allocated() / 1024**3
        print(f"\nğŸ“Š æ˜¾å­˜å³°å€¼: {peak_memory:.2f} GB")
    
    print("\nâœ… LoRA DPO è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ æƒé‡å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
    
    # æµ‹è¯•
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯• DPO æ¨¡å‹")
    print("=" * 60)
    
    test_questions = [
        "å¦‚ä½•ä¿æŒå¥åº·çš„ç”Ÿæ´»æ–¹å¼ï¼Ÿ",
        "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„åº”ç”¨",
        "å­¦ä¹ ç¼–ç¨‹æœ‰ä»€ä¹ˆå¥½çš„å»ºè®®ï¼Ÿ",
    ]
    
    model.eval()
    for question in test_questions:
        prompt = f"<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
        inputs = tokenizer(prompt, return_tensors="pt")
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        if "assistant" in response:
            response = response.split("assistant")[-1].strip()
        
        print(f"\nã€é—®é¢˜ã€‘{question}")
        print(f"ã€å›ç­”ã€‘{response[:300]}...")
    
    print("\n" + "=" * 60)
    print("ğŸ’¡ åç»­æ­¥éª¤")
    print("=" * 60)
    print("""
1. æµ‹è¯• DPO æ¨¡å‹æ•ˆæœ:
   python test_dpo.py

2. å¯¹æ¯” SFT å’Œ DPO æ¨¡å‹:
   python eval_benchmarks.py --compare

3. å¦‚éœ€åˆå¹¶æ‰€æœ‰ LoRA æƒé‡:
   # SFT LoRA + DPO LoRA å¯ä»¥é€šè¿‡å¤šæ¬¡ merge åˆå¹¶
""")


if __name__ == "__main__":
    main()
