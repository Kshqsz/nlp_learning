# lora_ppo_qwen3_1.7b.py
"""
Qwen3-1.7B LoRA PPOï¼ˆè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼‰

ä½¿ç”¨ PPO ç®—æ³•å’Œå¥–åŠ±æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ å¯¹é½
è®©æ¨¡å‹å­¦ä¹ ç”Ÿæˆé«˜å¥–åŠ±çš„å›ç­”

PPO ä¼˜åŠ¿ï¼š
  - ç›¸æ¯” REINFORCE æ–¹å·®æ›´ä½
  - é€šè¿‡åˆ†é’Ÿè£å‰ªé™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦
  - è®­ç»ƒæ›´ç¨³å®š
  - æ•ˆæœæ›´å¥½

è¿è¡Œæ–¹å¼ï¼š
  python lora_ppo_qwen3_1.7b.py
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, PeftModel, TaskType
from trl import PPOTrainer, PPOConfig
import json

# ===== é…ç½® =====
# æ¨¡å‹è·¯å¾„
SFT_MODEL_PATH = "/root/data/hsk-models/qwen3_1.7b_lora_sft"  # SFT æ¨¡å‹
REWARD_MODEL_PATH = "/root/data/hsk-models/qwen3_1.7b_reward_model"  # å¥–åŠ±æ¨¡å‹
ORIGINAL_MODEL_PATH = "/public/huggingface-models/Qwen/Qwen3-1.7B"  # åŸå§‹æ¨¡å‹ï¼ˆç”¨äº tokenizerï¼‰

# è¾“å‡ºè·¯å¾„
OUTPUT_DIR = "/root/data/hsk-models/qwen3_1.7b_lora_ppo"

# è¶…å‚æ•°
MAX_LENGTH = 512
BATCH_SIZE = 2
GRADIENT_ACCUMULATION_STEPS = 4  # æœ‰æ•ˆ batch = 4
LEARNING_RATE = 1e-5  # PPO é€šå¸¸ç”¨è¾ƒå°å­¦ä¹ ç‡
NUM_EPOCHS = 1
NUM_SAMPLES = 2000  # PPO æ•°æ®é‡

# PPO ç‰¹å®šå‚æ•°
PPO_EPOCHS = 4
PPO_CLIP_RANGE = 0.2
PPO_CLIP_RANGE_VALUE = 0.2
PPO_VALUE_COEFF = 0.1
PPO_ENTROPY_COEFF = 0.01
PPO_GAMMA = 0.99
PPO_LAMBDA = 0.95

# LoRA é…ç½®
LORA_R = 16
LORA_ALPHA = 32
LORA_DROPOUT = 0.1
TARGET_MODULES = [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj",
]


def load_ppo_dataset():
    """åŠ è½½ PPO è®­ç»ƒæ•°æ®ï¼ˆåªéœ€è¦ promptsï¼‰"""
    print("\nğŸ“Š åŠ è½½ PPO è®­ç»ƒæ•°æ®...")
    
    # ä» SFT æ•°æ®ä¸­æå– prompts
    sft_data_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "dpo_zh.jsonl")
    
    if os.path.exists(sft_data_path):
        print(f"   ä½¿ç”¨ SFT æ•°æ®æå– prompts: {sft_data_path}")
        dataset = load_dataset("json", data_files=sft_data_path, split="train")
        
        # ä» DPO/SFT æ•°æ®ä¸­æå– prompt
        # å…¼å®¹å­—æ®µï¼š{input} æˆ– {question} æˆ– {prompt}
        def extract_prompt(example):
            if "input" in example and example["input"] is not None:
                prompt = example["input"]
            elif "question" in example and example["question"] is not None:
                prompt = example["question"]
            else:
                prompt = example.get("prompt", "")
            return {"prompt": prompt}
        
        dataset = dataset.map(extract_prompt, remove_columns=dataset.column_names)
        
        if len(dataset) > NUM_SAMPLES:
            dataset = dataset.shuffle(seed=42).select(range(NUM_SAMPLES))
        
        print(f"   âœ… åŠ è½½ {len(dataset)} æ¡ prompts")
        return dataset
    
    # åˆ›å»ºæ¼”ç¤ºæ•°æ®
    print("   âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°æ•°æ®ï¼Œåˆ›å»ºæ¼”ç¤º prompts...")
    
    demo_prompts = [
        "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
        "Python æœ€å¸¸è§çš„æ•°æ®ç»“æ„æœ‰å“ªäº›ï¼Ÿ",
        "å¦‚ä½•ä¼˜åŒ–ä»£ç æ€§èƒ½ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
        "å¦‚ä½•å­¦ä¹ ä¸€é—¨æ–°çš„ç¼–ç¨‹è¯­è¨€ï¼Ÿ",
        "äº‘è®¡ç®—æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
        "ä»‹ç»ä¸€ä¸‹ API è®¾è®¡çš„æœ€ä½³å®è·µ",
        "ä»€ä¹ˆæ˜¯å¾®æœåŠ¡æ¶æ„ï¼Ÿ",
    ]
    
    # æ‰©å±•æ¼”ç¤ºæ•°æ®
    prompts = []
    for _ in range(NUM_SAMPLES // len(demo_prompts)):
        prompts.extend(demo_prompts)
    
    from datasets import Dataset
    dataset = Dataset.from_dict({"prompt": prompts[:NUM_SAMPLES]})
    
    print(f"   âœ… åˆ›å»ºæ¼”ç¤ºæ•°æ®é›†ï¼š{len(dataset)} æ¡")
    return dataset


def load_reward_model(model_path, tokenizer):
    """åŠ è½½è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹"""
    print(f"\nğŸ“¦ åŠ è½½å¥–åŠ±æ¨¡å‹: {model_path}")
    
    # åŠ è½½æ¨¡å‹
    reward_model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )
    
    # æ·»åŠ  reward head
    hidden_size = reward_model.config.hidden_size
    
    class RewardHead(torch.nn.Module):
        def __init__(self, hidden_size):
            super().__init__()
            self.linear = torch.nn.Linear(hidden_size, 1)
        
        def forward(self, hidden_states):
            last_hidden_state = hidden_states[:, -1, :]
            return self.linear(last_hidden_state)
    
    reward_model.reward_head = RewardHead(hidden_size).to(reward_model.device)
    
    # å°è¯•åŠ è½½ reward head çš„æƒé‡
    reward_head_path = os.path.join(model_path, "reward_head.pt")
    if os.path.exists(reward_head_path):
        reward_model.reward_head.load_state_dict(torch.load(reward_head_path))
        print("   âœ… åŠ è½½ Reward Head æƒé‡")
    else:
        print("   âš ï¸ Reward Head æƒé‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
    
    reward_model.eval()
    return reward_model


def main():
    print("=" * 60)
    print("ğŸ¯ Qwen3-1.7B LoRA PPO å¼ºåŒ–å­¦ä¹ å¯¹é½")
    print("=" * 60)
    
    # åŠ è½½ tokenizer
    print("\nğŸ“¦ åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(ORIGINAL_MODEL_PATH, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"  # PPO éœ€è¦ left padding
    
    # åŠ è½½ç­–ç•¥æ¨¡å‹ï¼ˆSFT æ¨¡å‹ï¼‰
    print(f"\nğŸ“¦ åŠ è½½ç­–ç•¥æ¨¡å‹: {SFT_MODEL_PATH}")
    
    # é¦–å…ˆåŠ è½½åŸºåº§æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        ORIGINAL_MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        use_cache=False,
    )
    
    # å¦‚æœæœ‰ SFT LoRAï¼ŒåŠ è½½å¹¶åˆå¹¶
    if os.path.exists(SFT_MODEL_PATH):
        print(f"   ğŸ”§ åŠ è½½ LoRA SFT æƒé‡")
        model = PeftModel.from_pretrained(model, SFT_MODEL_PATH)
        model = model.merge_and_unload()
        print("   âœ… LoRA æƒé‡å·²åˆå¹¶")
    
    # ä¸º PPO åº”ç”¨æ–°çš„ LoRA
    print("\nğŸ”§ é…ç½® PPO LoRA...")
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=LORA_DROPOUT,
        target_modules=TARGET_MODULES,
        bias="none",
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # åˆ›å»ºå‚è€ƒæ¨¡å‹ï¼ˆç”¨äº KL æ•£åº¦è®¡ç®—ï¼‰
    print("\nğŸ“¦ åˆ›å»ºå‚è€ƒæ¨¡å‹...")
    ref_model = AutoModelForCausalLM.from_pretrained(
        ORIGINAL_MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        use_cache=False,
    )
    
    if os.path.exists(SFT_MODEL_PATH):
        ref_model = PeftModel.from_pretrained(ref_model, SFT_MODEL_PATH)
        ref_model = ref_model.merge_and_unload()
    
    ref_model.eval()
    
    # åŠ è½½å¥–åŠ±æ¨¡å‹
    reward_model = load_reward_model(REWARD_MODEL_PATH, tokenizer)
    
    # åŠ è½½æ•°æ®
    dataset = load_ppo_dataset()
    
    # é¢„å¤„ç†æ•°æ®
    print("\nğŸ”„ å¤„ç† PPO æ•°æ®...")
    
    def preprocess_ppo_data(examples, tokenizer):
        """é¢„å¤„ç† PPO æ•°æ®"""
        processed = {
            "prompt": [],
            "prompt_ids": [],
        }
        
        for prompt in examples.get("prompt", []):
            if len(prompt) > 300:
                continue
            
            # æ„å»º prompt
            formatted_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
            
            # Tokenize
            tokenized = tokenizer(
                formatted_prompt,
                max_length=256,
                truncation=True,
                padding=False,
                add_special_tokens=False,
            )
            
            processed["prompt"].append(formatted_prompt)
            processed["prompt_ids"].append(tokenized["input_ids"])
        
        return processed
    
    processed_dataset = dataset.map(
        lambda x: preprocess_ppo_data(x, tokenizer),
        batched=True,
        remove_columns=dataset.column_names,
        desc="Processing PPO data",
        num_proc=4,
    )
    
    processed_dataset = processed_dataset.filter(lambda x: len(x["prompt_ids"]) > 0)
    print(f"âœ… æœ‰æ•ˆæ ·æœ¬: {len(processed_dataset)}")
    
    # è‡ªå®šä¹‰ PPO å¥–åŠ±å‡½æ•°
    def reward_fn(model, prompt_ids, response_ids, tokenizer):
        """
        è®¡ç®—å¥–åŠ±åˆ†æ•°
        ä½¿ç”¨å¥–åŠ±æ¨¡å‹è¯„ä¼°ç”Ÿæˆçš„å›ç­”
        """
        # åˆå¹¶ prompt å’Œ response
        full_ids = prompt_ids + response_ids
        
        # æˆªæ–­
        if len(full_ids) > 512:
            full_ids = full_ids[:512]
        
        # è½¬æ¢ä¸º tensor
        input_ids = torch.tensor([full_ids], dtype=torch.long).to(model.device)
        attention_mask = torch.ones_like(input_ids)
        
        # è·å–å¥–åŠ±
        with torch.no_grad():
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=True,
            )
            hidden_states = outputs.hidden_states[-1]
            reward = model.reward_head(hidden_states).squeeze(-1).item()
        
        return reward
    
    # PPO è®­ç»ƒé…ç½®
    # TRL>=0.25 çš„ PPOConfig ä¸å†æ¥å— `model_name`ï¼Œå¹¶ä½¿ç”¨ `num_ppo_epochs/cliprange/kl_coef` ç­‰å­—æ®µ
    ppo_config = PPOConfig(
        learning_rate=LEARNING_RATE,
        batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        num_ppo_epochs=PPO_EPOCHS,
        kl_coef=0.05,
        cliprange=PPO_CLIP_RANGE,
        whiten_rewards=True,
        remove_unused_columns=False,
    )
    
    # åˆ›å»º PPO Trainer
    print("\n" + "=" * 60)
    print("ğŸ‹ï¸ å¼€å§‹ PPO è®­ç»ƒ")
    print("=" * 60)
    print(f"   è®­ç»ƒæ ·æœ¬: {len(processed_dataset)}")
    print(f"   Batch Size: {BATCH_SIZE}")
    print(f"   PPO Epochs: {PPO_EPOCHS}")
    print(f"   Learning Rate: {LEARNING_RATE}")
    print(f"   Clip Range: {PPO_CLIP_RANGE}")
    
    if torch.cuda.is_available():
        print(f"   è®­ç»ƒå‰æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    
    # ç®€åŒ–çš„ PPO è®­ç»ƒå®ç°
    # æ³¨æ„ï¼šTRL åº“çš„ PPOTrainer éœ€è¦ç‰¹å®šçš„æ•°æ®æ ¼å¼
    # è¿™é‡Œå®ç°ä¸€ä¸ªåŸºç¡€çš„ PPO å¾ªç¯
    
    print("\nğŸ’¡ PPO è®­ç»ƒå®ç°è¯´æ˜:")
    print("""
    ç”±äº TRL åº“çš„ PPOTrainer æœ‰ç‰¹å®šçš„æ•°æ®å’Œæ¨¡å‹è¦æ±‚ï¼Œ
    è¿™ä¸ªè„šæœ¬æä¾›äº†åŸºç¡€çš„ PPO æ¡†æ¶ã€‚
    
    å®Œæ•´çš„ PPO è®­ç»ƒéœ€è¦ï¼š
    1. ç”Ÿæˆé˜¶æ®µï¼šç”¨ç­–ç•¥æ¨¡å‹ç”Ÿæˆå›ç­”
    2. å¥–åŠ±è®¡ç®—ï¼šç”¨å¥–åŠ±æ¨¡å‹è¯„ä¼°å›ç­”
    3. PPO æ›´æ–°ï¼šè®¡ç®—ä¼˜åŠ¿å‡½æ•°å¹¶æ›´æ–°ç­–ç•¥
    
    ä¸ºäº†ä½¿ç”¨å®Œæ•´çš„ PPOï¼Œå»ºè®®ä½¿ç”¨ TRL åº“çš„ PPOTrainerï¼Œ
    éœ€è¦æŒ‰ç…§å…¶è¦æ±‚å‡†å¤‡æ•°æ®æ ¼å¼ã€‚
    """)
    
    # è¿™é‡Œè¿›è¡ŒåŸºç¡€çš„è®­ç»ƒå¾ªç¯ç¤ºæ„
    model.train()
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    
    # ç®€å•çš„è®­ç»ƒæ­¥éª¤
    num_training_steps = (len(processed_dataset) // BATCH_SIZE) * NUM_EPOCHS
    
    print(f"\n   é¢„è®¡è®­ç»ƒæ­¥æ•°: {num_training_steps}")
    print("\n   âš ï¸ å®Œæ•´ PPO å®ç°æ¨èä½¿ç”¨ TRL åº“çš„ PPOTrainer")
    print("   å‚è€ƒ: https://github.com/huggingface/trl")
    
    # ä¿å­˜é…ç½®
    print(f"\nğŸ’¾ ä¿å­˜ PPO æ¨¡å‹é…ç½®åˆ° {OUTPUT_DIR}...")
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    # ä¿å­˜é…ç½®
    config = {
        "model_type": "ppo_model",
        "sft_model": SFT_MODEL_PATH,
        "reward_model": REWARD_MODEL_PATH,
        "lora_r": LORA_R,
        "lora_alpha": LORA_ALPHA,
        "ppo_config": {
            "learning_rate": LEARNING_RATE,
            "batch_size": BATCH_SIZE,
            "ppo_epochs": PPO_EPOCHS,
            "clip_range": PPO_CLIP_RANGE,
        }
    }
    
    with open(os.path.join(OUTPUT_DIR, "ppo_config.json"), "w") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    if torch.cuda.is_available():
        peak_memory = torch.cuda.max_memory_allocated() / 1024**3
        print(f"\nğŸ“Š æ˜¾å­˜å³°å€¼: {peak_memory:.2f} GB")
    
    print("\nâœ… PPO æ¨¡å‹é…ç½®å®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
    
    print("\n" + "=" * 60)
    print("ğŸ’¡ å®Œæ•´ PPO å®ç°å»ºè®®")
    print("=" * 60)
    print("""
    ä¸ºäº†å®ç°å®Œæ•´çš„ PPO è®­ç»ƒï¼Œå»ºè®®ï¼š
    
    1. ä½¿ç”¨ TRL åº“çš„ PPOTrainerï¼š
       from trl import PPOTrainer, PPOConfig
       
    2. å‡†å¤‡æ•°æ®ä¸ºï¼š
       {
           "prompt": "ç”¨æˆ·è¾“å…¥",
           "input_ids": [token_ids],
       }
    
    3. å®šä¹‰å¥–åŠ±å‡½æ•°ï¼š
       def reward_fn(samples):
           # ä½¿ç”¨å¥–åŠ±æ¨¡å‹è¯„åˆ†
           return rewards
    
    4. è¿è¡Œ PPO è®­ç»ƒå¾ªç¯ï¼š
       for epoch in range(num_epochs):
           outputs = trainer.generate(...)
           rewards = reward_fn(outputs)
           trainer.step(rewards)
    
    å‚è€ƒå®ç°ï¼šTRL å®˜æ–¹æ–‡æ¡£
    https://huggingface.co/docs/trl/index
    """)


if __name__ == "__main__":
    main()
