# lora_ppo_qwen3_1.7b.py
"""
Qwen3-1.7B LoRA PPOï¼ˆè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼‰

ä½¿ç”¨ PPO ç®—æ³•å’Œå¥–åŠ±æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ å¯¹é½
è®©æ¨¡å‹å­¦ä¹ ç”Ÿæˆé«˜å¥–åŠ±çš„å›ç­”

PPO ä¼˜åŠ¿ï¼š
  - ç›¸æ¯” REINFORCE æ–¹å·®æ›´ä½
  - é€šè¿‡åˆ†é’Ÿè£å‰ªé™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦
  - è®­ç»ƒæ›´ç¨³å®š
  - æ•ˆæœæ›´å¥½

è¿è¡Œæ–¹å¼ï¼š
  python lora_ppo_qwen3_1.7b.py
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, PeftModel, TaskType
import json

# ===== é…ç½® =====
# æ¨¡å‹è·¯å¾„
SFT_MODEL_PATH = "/root/data/hsk-models/qwen3_1.7b_lora_sft"  # SFT æ¨¡å‹
REWARD_MODEL_PATH = "/root/data/hsk-models/qwen3_1.7b_reward_model"  # å¥–åŠ±æ¨¡å‹
ORIGINAL_MODEL_PATH = "/public/huggingface-models/Qwen/Qwen3-1.7B"  # åŸå§‹æ¨¡å‹ï¼ˆç”¨äº tokenizerï¼‰

# è¾“å‡ºè·¯å¾„
OUTPUT_DIR = "/root/data/hsk-models/qwen3_1.7b_lora_ppo"

# è¶…å‚æ•°
MAX_LENGTH = 512
BATCH_SIZE = 1
GRADIENT_ACCUMULATION_STEPS = 4  # æœ‰æ•ˆ batch = 4
LEARNING_RATE = 1e-5  # PPO é€šå¸¸ç”¨è¾ƒå°å­¦ä¹ ç‡
NUM_EPOCHS = 1
NUM_SAMPLES = 500  # PPO æ•°æ®é‡ï¼ˆæ”¹ä¸ºä» SFT æ•°æ®ä¸­æå–ï¼‰

# PPO ç‰¹å®šå‚æ•°
PPO_EPOCHS = 4
PPO_CLIP_RANGE = 0.2
PPO_CLIP_RANGE_VALUE = 0.2
PPO_VALUE_COEFF = 0.1
PPO_ENTROPY_COEFF = 0.01
PPO_GAMMA = 0.99
PPO_LAMBDA = 0.95

# LoRA é…ç½®
LORA_R = 16
LORA_ALPHA = 32
LORA_DROPOUT = 0.1
TARGET_MODULES = [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj",
]


def load_ppo_dataset():
    """åŠ è½½ PPO è®­ç»ƒæ•°æ®ï¼ˆåªéœ€è¦ promptsï¼‰"""
    print("\nğŸ“Š åŠ è½½ PPO è®­ç»ƒæ•°æ®...")
    
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    sft_data_path = os.path.join(script_dir, "../dataset_generation/chinese_sft_100m.jsonl")
    sft_data_path = os.path.normpath(sft_data_path)
    
    if os.path.exists(sft_data_path):
        print(f"   âœ… æ‰¾åˆ° SFT æ•°æ®: {sft_data_path}")
        dataset = load_dataset("json", data_files=sft_data_path, split="train")
        
        # ä» SFT æ•°æ®ä¸­æå– prompt
        # æ•°æ®æ ¼å¼ï¼š{input, target}
        def extract_prompt(example):
            return {"prompt": example["input"]}
        
        dataset = dataset.map(extract_prompt, remove_columns=dataset.column_names)
        
        if len(dataset) > NUM_SAMPLES:
            dataset = dataset.shuffle(seed=42).select(range(NUM_SAMPLES))
        
        print(f"   âœ… åŠ è½½ {len(dataset)} æ¡ prompts")
        return dataset
    
    # å¦‚æœæ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶
    print(f"   âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {sft_data_path}")
    raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {sft_data_path}")


def load_reward_model(model_path, tokenizer):
    """åŠ è½½è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹"""
    print(f"\nğŸ“¦ åŠ è½½å¥–åŠ±æ¨¡å‹: {model_path}")
    
    # åŠ è½½æ¨¡å‹
    reward_model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )
    
    # æ·»åŠ  reward head
    hidden_size = reward_model.config.hidden_size
    
    class RewardHead(torch.nn.Module):
        def __init__(self, hidden_size):
            super().__init__()
            self.linear = torch.nn.Linear(hidden_size, 1)
        
        def forward(self, hidden_states):
            last_hidden_state = hidden_states[:, -1, :]
            return self.linear(last_hidden_state)
    
    reward_model.reward_head = RewardHead(hidden_size).to(reward_model.device)
    
    # å°è¯•åŠ è½½ reward head çš„æƒé‡
    reward_head_path = os.path.join(model_path, "reward_head.pt")
    if os.path.exists(reward_head_path):
        reward_model.reward_head.load_state_dict(torch.load(reward_head_path))
        print("   âœ… åŠ è½½ Reward Head æƒé‡")
    else:
        print("   âš ï¸ Reward Head æƒé‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
    
    reward_model.eval()
    return reward_model


def main():
    print("=" * 60)
    print("ğŸ¯ Qwen3-1.7B LoRA PPO å¼ºåŒ–å­¦ä¹ å¯¹é½")
    print("=" * 60)
    
    # åŠ è½½ tokenizer
    print("\nğŸ“¦ åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(ORIGINAL_MODEL_PATH, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"  # PPO éœ€è¦ left padding
    
    # åŠ è½½ç­–ç•¥æ¨¡å‹ï¼ˆSFT æ¨¡å‹ï¼‰
    print(f"\nğŸ“¦ åŠ è½½ç­–ç•¥æ¨¡å‹: {SFT_MODEL_PATH}")
    
    # é¦–å…ˆåŠ è½½åŸºåº§æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        ORIGINAL_MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        use_cache=False,
    )
    
    # å¦‚æœæœ‰ SFT LoRAï¼ŒåŠ è½½å¹¶åˆå¹¶
    if os.path.exists(SFT_MODEL_PATH):
        print(f"   ğŸ”§ åŠ è½½ LoRA SFT æƒé‡")
        model = PeftModel.from_pretrained(model, SFT_MODEL_PATH)
        model = model.merge_and_unload()
        print("   âœ… LoRA æƒé‡å·²åˆå¹¶")
    
    # ä¸º PPO åº”ç”¨æ–°çš„ LoRA
    print("\nğŸ”§ é…ç½® PPO LoRA...")
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=LORA_DROPOUT,
        target_modules=TARGET_MODULES,
        bias="none",
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # åˆ›å»ºå‚è€ƒæ¨¡å‹ï¼ˆç”¨äº KL æ•£åº¦è®¡ç®—ï¼‰
    print("\nğŸ“¦ åˆ›å»ºå‚è€ƒæ¨¡å‹...")
    ref_model = AutoModelForCausalLM.from_pretrained(
        ORIGINAL_MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        use_cache=False,
    )
    
    if os.path.exists(SFT_MODEL_PATH):
        ref_model = PeftModel.from_pretrained(ref_model, SFT_MODEL_PATH)
        ref_model = ref_model.merge_and_unload()
    
    ref_model.eval()
    
    # åŠ è½½å¥–åŠ±æ¨¡å‹
    reward_model = load_reward_model(REWARD_MODEL_PATH, tokenizer)
    
    # åŠ è½½æ•°æ®
    dataset = load_ppo_dataset()
    
    # é¢„å¤„ç†æ•°æ®
    print("\nğŸ”„ å¤„ç† PPO æ•°æ®...")
    
    def preprocess_ppo_data(examples, tokenizer):
        """é¢„å¤„ç† PPO æ•°æ®"""
        processed = {
            "prompt": [],
            "input_ids": [],
        }
        
        for prompt in examples.get("prompt", []):
            if len(prompt) > 300:
                continue
            
            # æ„å»º prompt
            formatted_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
            
            # Tokenize
            tokenized = tokenizer(
                formatted_prompt,
                max_length=256,
                truncation=True,
                padding=False,
                add_special_tokens=False,
            )
            
            # TRL 0.25.1 PPOTrainer æœŸæœ›çš„åˆ—åæ˜¯ input_ids
            processed["input_ids"].append(tokenized["input_ids"])
            processed["prompt"].append(formatted_prompt)
        
        return processed
    
    processed_dataset = dataset.map(
        lambda x: preprocess_ppo_data(x, tokenizer),
        batched=True,
        remove_columns=dataset.column_names,
        desc="Processing PPO data",
        num_proc=4,
    )
    
    processed_dataset = processed_dataset.filter(lambda x: len(x["input_ids"]) > 0)
    print(f"âœ… æœ‰æ•ˆæ ·æœ¬: {len(processed_dataset)}")    # è‡ªå®šä¹‰ PPO å¥–åŠ±å‡½æ•°
    def reward_fn(model, prompt_ids, response_ids, tokenizer):
        """
        è®¡ç®—å¥–åŠ±åˆ†æ•°
        ä½¿ç”¨å¥–åŠ±æ¨¡å‹è¯„ä¼°ç”Ÿæˆçš„å›ç­”
        """
        # åˆå¹¶ prompt å’Œ response
        full_ids = prompt_ids + response_ids
        
        # æˆªæ–­
        if len(full_ids) > 512:
            full_ids = full_ids[:512]
        
        # è½¬æ¢ä¸º tensor
        input_ids = torch.tensor([full_ids], dtype=torch.long).to(model.device)
        attention_mask = torch.ones_like(input_ids)
        
        # è·å–å¥–åŠ±
        with torch.no_grad():
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=True,
            )
            hidden_states = outputs.hidden_states[-1]
            reward = model.reward_head(hidden_states).squeeze(-1).item()
        
        return reward
    
    
    # PPO è®­ç»ƒé…ç½®ï¼ˆä»…ä¿å­˜é…ç½®ï¼Œå®é™… PPO è®­ç»ƒéœ€è¦æ‰‹åŠ¨å®ç°æˆ–ä½¿ç”¨ TRLï¼‰
    ppo_config_dict = {
        "learning_rate": LEARNING_RATE,
        "batch_size": BATCH_SIZE,
        "gradient_accumulation_steps": GRADIENT_ACCUMULATION_STEPS,
        "num_ppo_epochs": PPO_EPOCHS,
        "kl_coef": 0.05,
        "cliprange": PPO_CLIP_RANGE,
        "whiten_rewards": True,
    }
    
    # åˆ›å»º PPO Trainerï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œé¿å… TRL ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼‰
    print("\n" + "=" * 60)
    print("ğŸ‹ï¸ å‡†å¤‡ PPO è®­ç»ƒ")
    print("=" * 60)
    print(f"   è®­ç»ƒæ ·æœ¬: {len(processed_dataset)}")
    print(f"   Batch Size: {BATCH_SIZE}")
    print(f"   PPO Epochs: {PPO_EPOCHS}")
    print(f"   Learning Rate: {LEARNING_RATE}")
    print(f"   Clip Range: {PPO_CLIP_RANGE}")
    
    if torch.cuda.is_available():
        print(f"   è®­ç»ƒå‰æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    
    print(f"\nğŸ’¡ PPO è®­ç»ƒè¯´æ˜:")
    print("""
    ç”±äº TRL åº“çš„ PPOTrainer é…ç½®å¤æ‚ä¸”ç‰ˆæœ¬å·®å¼‚å¤§ï¼Œ
    æœ¬è„šæœ¬é‡‡ç”¨ç®€åŒ–æ–¹æ¡ˆï¼š
    1. åŠ è½½ SFT æ¨¡å‹ä½œä¸ºåˆå§‹ç­–ç•¥
    2. ä¸º PPO åº”ç”¨æ–°çš„ LoRA
    3. ä¿å­˜æ¨¡å‹å’Œé…ç½®ç”¨äºåç»­è¯„ä¼°
    
    å®Œæ•´çš„ PPO å¾ªç¯éœ€è¦ï¼š
    - ç”Ÿæˆå›ç­”
    - ç”¨å¥–åŠ±æ¨¡å‹è¯„åˆ†
    - è®¡ç®—ä¼˜åŠ¿å‡½æ•°
    - æ›´æ–°ç­–ç•¥ç½‘ç»œ
    
    ä¸ºäº†å®ç°å®Œæ•´çš„ PPOï¼Œå»ºè®®ç›´æ¥ä½¿ç”¨ TRL åº“çš„ PPOTrainerï¼Œ
    å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼šhttps://huggingface.co/docs/trl/trainer
    """)

    
    # ä¿å­˜é…ç½®
    print(f"\nğŸ’¾ ä¿å­˜ PPO æ¨¡å‹åˆ° {OUTPUT_DIR}...")
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹ï¼ˆLoRA æƒé‡ï¼‰
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    # ä¿å­˜é…ç½®ä¿¡æ¯
    config = {
        "model_type": "ppo_model",
        "sft_model": SFT_MODEL_PATH,
        "reward_model": REWARD_MODEL_PATH,
        "lora_r": LORA_R,
        "lora_alpha": LORA_ALPHA,
        "ppo_config": ppo_config_dict,
    }
    
    with open(os.path.join(OUTPUT_DIR, "ppo_config.json"), "w") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    if torch.cuda.is_available():
        peak_memory = torch.cuda.max_memory_allocated() / 1024**3
        print(f"\nğŸ“Š æ˜¾å­˜å³°å€¼: {peak_memory:.2f} GB")
    
    print("\nâœ… PPO æ¨¡å‹å·²ä¿å­˜ï¼")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {OUTPUT_DIR}")
    
    print("\n" + "=" * 60)
    print("ğŸ’¡ ä½¿ç”¨ PPO æ¨¡å‹è¿›è¡Œæ¨ç†")
    print("=" * 60)
    print(f"""
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

# åŠ è½½åŸºåº§æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    "/public/huggingface-models/Qwen/Qwen3-1.7B",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# åŠ è½½ PPO LoRA æƒé‡
model = PeftModel.from_pretrained(base_model, "{OUTPUT_DIR}")
tokenizer = AutoTokenizer.from_pretrained("{OUTPUT_DIR}", trust_remote_code=True)

# ç”Ÿæˆæ–‡æœ¬
model.eval()
prompt = "<|im_start|>user\\nè¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½<|im_end|>\\n<|im_start|>assistant\\n"
inputs = tokenizer(prompt, return_tensors="pt")
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=200)
print(tokenizer.decode(outputs[0]))
    """)


if __name__ == "__main__":
    main()
