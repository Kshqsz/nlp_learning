# lora_sft_qwen3_1.7b.py
"""
Qwen3-1.7B LoRA SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰

åœ¨ç»§ç»­é¢„è®­ç»ƒçš„æ¨¡å‹åŸºç¡€ä¸Šï¼Œä½¿ç”¨ LoRA è¿›è¡Œç›‘ç£å¾®è°ƒ
è®©æ¨¡å‹å­¦ä¼šéµå¾ªæŒ‡ä»¤ã€è¿›è¡Œå¯¹è¯

LoRA ä¼˜åŠ¿ï¼š
  - åªè®­ç»ƒ ~1% çš„å‚æ•°
  - æ˜¾å­˜å ç”¨å°ï¼Œä¸éœ€è¦ CPU Offload
  - è®­ç»ƒé€Ÿåº¦å¿«ï¼ˆ~2-3s/stepï¼‰
  - æ•ˆæœæ¥è¿‘å…¨é‡å¾®

è¿è¡Œæ–¹å¼ï¼š
  python lora_sft_qwen3_1.7b.py
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
)

# ===== é…ç½® =====
# ä½¿ç”¨ç»§ç»­é¢„è®­ç»ƒåçš„æ¨¡å‹ä½œä¸ºåŸºåº§
BASE_MODEL_PATH = "/root/data/hsk-models/qwen3_1.7b_pretrain"  # é¢„è®­ç»ƒåçš„æ¨¡å‹
# å¦‚æœé¢„è®­ç»ƒè¿˜æ²¡å®Œæˆï¼Œå¯ä»¥å…ˆç”¨åŸå§‹æ¨¡å‹
# BASE_MODEL_PATH = "/public/huggingface-models/Qwen/Qwen3-1.7B"
OUTPUT_DIR = "/root/data/hsk-models/qwen3_1.7b_lora_sft"
MAX_LENGTH = 512

# ===== è¶…å‚æ•°é…ç½®ï¼ˆå·²ä¼˜åŒ–ï¼‰=====
BATCH_SIZE = 4               # å‡å° batch sizeï¼Œå¢åŠ æ›´æ–°æ¬¡æ•°
GRADIENT_ACCUMULATION_STEPS = 8  # æœ‰æ•ˆ batch = 32
LEARNING_RATE = 1e-4         # LoRA å¾®è°ƒæ¨èæ›´ä½å­¦ä¹ ç‡
NUM_EPOCHS = 3               # å¢åŠ è®­ç»ƒè½®æ¬¡
NUM_SAMPLES = 10000          # SFT æ•°æ®é‡
WARMUP_STEPS = 100           # å›ºå®š warmup æ­¥æ•°

# LoRA é…ç½®ï¼ˆå¢å¼ºè¡¨è¾¾èƒ½åŠ›ï¼‰
LORA_R = 64                  # å¢å¤§ LoRA ç§©ï¼Œæå‡è¡¨è¾¾èƒ½åŠ›
LORA_ALPHA = 128             # alpha é€šå¸¸è®¾ä¸º 2 * r
LORA_DROPOUT = 0.1           # å¢åŠ  dropout é˜²è¿‡æ‹Ÿåˆ
TARGET_MODULES = [          # Qwen3 çš„ç›®æ ‡æ¨¡å—
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj",
]


# ===== 1. åŠ è½½æ¨¡å‹å’Œ Tokenizer =====
print("=" * 60)
print("ğŸš€ Qwen3-1.7B LoRA SFT å¾®è°ƒ")
print("=" * 60)
print(f"åŸºåº§æ¨¡å‹: {BASE_MODEL_PATH}")
print(f"LoRA Rank: {LORA_R}, Alpha: {LORA_ALPHA}")

print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_PATH,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    use_cache=False,  # è®­ç»ƒæ—¶ç¦ç”¨ KV cache
)

# æ‰“å°åŸå§‹æ¨¡å‹å‚æ•°é‡
total_params = sum(p.numel() for p in model.parameters())
print(f"åŸºåº§æ¨¡å‹å‚æ•°é‡: {total_params / 1e9:.2f}B")


# ===== 2. é…ç½® LoRA =====
print("\nğŸ”§ é…ç½® LoRA...")

lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    lora_dropout=LORA_DROPOUT,
    target_modules=TARGET_MODULES,
    bias="none",
)

# åº”ç”¨ LoRA
model = get_peft_model(model, lora_config)

# å¯ç”¨ gradient checkpointingï¼ˆå¿…é¡»åœ¨åº”ç”¨ LoRA ä¹‹åï¼‰
model.enable_input_require_grads()  # å…³é”®ï¼šè®©è¾“å…¥éœ€è¦æ¢¯åº¦

# æ‰“å° LoRA ä¿¡æ¯
model.print_trainable_parameters()


# ===== 3. åŠ è½½ SFT æ•°æ®é›† =====
print("\nğŸ“Š åŠ è½½ SFT æ•°æ®...")

# ä»æœ¬åœ° JSONL æ–‡ä»¶åŠ è½½æ•°æ®
# æ•°æ®æ ¼å¼: {"input": "ç”¨æˆ·è¾“å…¥", "target": "æ¨¡å‹å›å¤"}
DATA_PATH = "./chinese_sft_100m.jsonl"

raw_dataset = load_dataset(
    "json",
    data_files=DATA_PATH,
    split="train"
)

# å¦‚æœæ•°æ®é‡å¤§äº NUM_SAMPLESï¼Œåªå–å‰ NUM_SAMPLES æ¡
if len(raw_dataset) > NUM_SAMPLES:
    raw_dataset = raw_dataset.select(range(NUM_SAMPLES))

print(f"âœ… åŠ è½½ {len(raw_dataset)} æ¡æ•°æ®")


# ===== 4. æ•°æ®é¢„å¤„ç† =====
def preprocess_function(examples):
    """
    SFT æ•°æ®å¤„ç†ï¼šæ„å»ºå¯¹è¯æ ¼å¼
    åªå¯¹ assistant çš„å›å¤è®¡ç®— loss
    
    æ•°æ®æ ¼å¼: {"input": "ç”¨æˆ·è¾“å…¥", "target": "æ¨¡å‹å›å¤"}
    å…³é”®ä¿®å¤ï¼šä½¿ç”¨ç›¸åŒçš„ tokenize å‚æ•°ç¡®ä¿é•¿åº¦ä¸€è‡´
    """
    input_ids_list = []
    labels_list = []
    attention_mask_list = []
    
    for inp, target in zip(examples["input"], examples["target"]):
        # è·³è¿‡è¿‡é•¿çš„æ ·æœ¬
        if len(inp) > 800 or len(target) > 800:
            continue
        
        # æ„å»º Qwen å¯¹è¯æ ¼å¼ (ChatML)
        prompt = f"<|im_start|>user\n{inp}<|im_end|>\n<|im_start|>assistant\n"
        response = f"{target}<|im_end|>"
        full_text = prompt + response
        
        # åˆ†åˆ« tokenize prompt å’Œå®Œæ•´æ–‡æœ¬ï¼ˆä½¿ç”¨ç›¸åŒçš„å‚æ•°ï¼ï¼‰
        prompt_ids = tokenizer(
            prompt,
            max_length=MAX_LENGTH,
            truncation=True,
            padding=False,
            add_special_tokens=False,  # ä¸æ·»åŠ ç‰¹æ®Š token
        )["input_ids"]
        
        full_ids = tokenizer(
            full_text,
            max_length=MAX_LENGTH,
            truncation=True,
            padding=False,
            add_special_tokens=False,  # ä¿æŒä¸€è‡´
        )["input_ids"]
        
        # æ„å»º labelsï¼šåªå¯¹ response éƒ¨åˆ†è®¡ç®— loss
        prompt_len = len(prompt_ids)
        labels = [-100] * prompt_len + full_ids[prompt_len:]
        
        # ç¡®ä¿é•¿åº¦å®Œå…¨ä¸€è‡´
        assert len(labels) == len(full_ids), f"Length mismatch: labels={len(labels)}, input_ids={len(full_ids)}"
        
        input_ids_list.append(full_ids)
        labels_list.append(labels)
        attention_mask_list.append([1] * len(full_ids))
    
    return {
        "input_ids": input_ids_list,
        "labels": labels_list,
        "attention_mask": attention_mask_list,
    }


print("\nğŸ”„ å¤„ç†æ•°æ®...")
dataset = raw_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=raw_dataset.column_names,
    desc="Processing SFT data",
    num_proc=4,
)

# è¿‡æ»¤ç©ºæ ·æœ¬
dataset = dataset.filter(lambda x: len(x["input_ids"]) > 0)
print(f"âœ… å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ ·æœ¬: {len(dataset)}")

# æ‰“å°å‡ ä¸ªæ ·æœ¬è¿›è¡ŒéªŒè¯
print("\nğŸ“‹ æ•°æ®æ ·æœ¬éªŒè¯:")
for i in range(min(3, len(dataset))):
    sample = dataset[i]
    labels = sample["labels"]
    non_ignore = [l for l in labels if l != -100]
    print(f"  æ ·æœ¬ {i}: input_ids é•¿åº¦={len(sample['input_ids'])}, "
          f"æœ‰æ•ˆ labels æ•°é‡={len(non_ignore)}, "
          f"æ¯”ä¾‹={len(non_ignore)/len(sample['input_ids'])*100:.1f}%")


# ===== 5. è®­ç»ƒé…ç½® =====
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    overwrite_output_dir=True,
    
    # è®­ç»ƒå‚æ•°
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    
    # å­¦ä¹ ç‡ï¼ˆä¼˜åŒ–åï¼‰
    learning_rate=LEARNING_RATE,
    lr_scheduler_type="cosine",
    warmup_steps=WARMUP_STEPS,  # ä½¿ç”¨å›ºå®šæ­¥æ•°è€Œéæ¯”ä¾‹
    weight_decay=0.01,  # æ·»åŠ æƒé‡è¡°å‡
    max_grad_norm=1.0,  # æ¢¯åº¦è£å‰ª
    
    # æ˜¾å­˜ä¼˜åŒ–
    bf16=True,
    gradient_checkpointing=True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    gradient_checkpointing_kwargs={"use_reentrant": False},  # æ¨èè®¾ç½®
    optim="adamw_torch",
    
    # æ—¥å¿—å’Œä¿å­˜
    logging_steps=10,  # æ›´é¢‘ç¹è®°å½•
    save_steps=200,    # æ›´é¢‘ç¹ä¿å­˜
    save_total_limit=3,
    
    # è¯„ä¼°
    eval_strategy="no",  # å¦‚æœ‰éªŒè¯é›†å¯æ”¹ä¸º "steps"
    
    # å…¶ä»–
    report_to="none",
    dataloader_num_workers=4,
    remove_unused_columns=False,  # ä¿ç•™æ‰€æœ‰åˆ—ï¼Œé¿å…æ•°æ®é—®é¢˜
    dataloader_pin_memory=True,
    seed=42,  # å›ºå®šéšæœºç§å­
)


# ===== 6. è‡ªå®šä¹‰ Data Collator =====
class SFTDataCollator:
    """
    è‡ªå®šä¹‰ Data Collatorï¼Œæ­£ç¡®å¤„ç† labels çš„ padding
    - input_ids ç”¨ pad_token_id padding
    - labels ç”¨ -100 paddingï¼ˆä¸è®¡ç®— lossï¼‰
    """
    def __init__(self, tokenizer, padding_side="right"):
        self.tokenizer = tokenizer
        self.padding_side = padding_side
    
    def __call__(self, features):
        # è·å–æœ€å¤§é•¿åº¦
        max_length = max(len(f["input_ids"]) for f in features)
        
        batch_input_ids = []
        batch_labels = []
        batch_attention_mask = []
        
        for feature in features:
            input_ids = feature["input_ids"]
            labels = feature["labels"]
            attention_mask = feature["attention_mask"]
            
            # è®¡ç®— padding é•¿åº¦
            padding_length = max_length - len(input_ids)
            
            if self.padding_side == "right":
                input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length
                labels = labels + [-100] * padding_length  # labels ç”¨ -100 padding
                attention_mask = attention_mask + [0] * padding_length
            else:
                input_ids = [self.tokenizer.pad_token_id] * padding_length + input_ids
                labels = [-100] * padding_length + labels
                attention_mask = [0] * padding_length + attention_mask
            
            batch_input_ids.append(input_ids)
            batch_labels.append(labels)
            batch_attention_mask.append(attention_mask)
        
        return {
            "input_ids": torch.tensor(batch_input_ids, dtype=torch.long),
            "labels": torch.tensor(batch_labels, dtype=torch.long),
            "attention_mask": torch.tensor(batch_attention_mask, dtype=torch.long),
        }


# ===== 7. å¼€å§‹è®­ç»ƒ =====
print("\n" + "=" * 60)
print("ğŸ‹ï¸ å¼€å§‹ LoRA SFT è®­ç»ƒ")
print("=" * 60)
print(f"   è®­ç»ƒæ ·æœ¬: {len(dataset)}")
print(f"   Batch Size: {BATCH_SIZE}")
print(f"   æ¢¯åº¦ç´¯ç§¯: {GRADIENT_ACCUMULATION_STEPS}")
print(f"   æœ‰æ•ˆ Batch: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")
print(f"   Learning Rate: {LEARNING_RATE}")
print(f"   LoRA Rank: {LORA_R}")
print(f"   Epochs: {NUM_EPOCHS}")
print(f"   é¢„è®¡æ€»æ­¥æ•°: {len(dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) * NUM_EPOCHS}")

if torch.cuda.is_available():
    print(f"   è®­ç»ƒå‰æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

# ä½¿ç”¨è‡ªå®šä¹‰ Data Collator
data_collator = SFTDataCollator(tokenizer=tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=data_collator,
)

# è®­ç»ƒ
trainer.train()


# ===== 8. ä¿å­˜ LoRA æƒé‡ =====
print(f"\nğŸ’¾ ä¿å­˜ LoRA æƒé‡åˆ° {OUTPUT_DIR}...")

model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

if torch.cuda.is_available():
    peak_memory = torch.cuda.max_memory_allocated() / 1024**3
    print(f"\nğŸ“Š æ˜¾å­˜å³°å€¼: {peak_memory:.2f} GB")

print("\nâœ… LoRA SFT è®­ç»ƒå®Œæˆï¼")
print(f"ğŸ“ LoRA æƒé‡å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")

# ===== 9. æµ‹è¯•å¯¹è¯æ•ˆæœ =====
print("\n" + "=" * 60)
print("ğŸ§ª æµ‹è¯•å¯¹è¯æ•ˆæœ")
print("=" * 60)

# æµ‹è¯•é—®é¢˜
test_questions = [
    "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²",
    "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
    "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
    "Python å’Œ Java æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
]

# æµ‹è¯•æ—¶å¯ç”¨ cache
model.config.use_cache = True
model.eval()

for question in test_questions:
    # æ„å»ºå¯¹è¯æ ¼å¼
    prompt = f"<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
    
    inputs = tokenizer(prompt, return_tensors="pt", add_special_tokens=False)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # æå– assistant å›å¤
    if "assistant" in response:
        response = response.split("assistant")[-1].strip()
    
    print(f"\nã€é—®é¢˜ã€‘{question}")
    print(f"ã€å›ç­”ã€‘{response[:300]}...")  # æˆªæ–­æ˜¾ç¤º


print("\n" + "=" * 60)
print("ğŸ’¡ åç»­æ­¥éª¤")
print("=" * 60)
print("""
1. å¦‚éœ€åˆå¹¶ LoRA æƒé‡åˆ°åŸºåº§æ¨¡å‹ï¼Œè¿è¡Œ:
   from peft import PeftModel
   merged = model.merge_and_unload()
   merged.save_pretrained("./qwen3_1.7b_sft_merged")

2. å¦‚éœ€è¿›ä¸€æ­¥è¿›è¡Œ DPO/RLHF å¯¹é½ï¼Œå¯åŸºäºæ­¤ LoRA æ¨¡å‹ç»§ç»­è®­ç»ƒ
""")

