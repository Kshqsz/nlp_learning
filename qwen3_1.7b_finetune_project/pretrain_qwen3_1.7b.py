# pretrain_qwen3_1.7b.py
"""
Qwen3-1.7B ç»§ç»­é¢„è®­ç»ƒï¼ˆDeepSpeed ZeRO-2 ä¼˜åŒ–ï¼‰

ç¡¬ä»¶è¦æ±‚ï¼šNVIDIA 4090D (24GB) - æ˜¾å­˜å……è£•
æ˜¾å­˜ä¼˜åŒ–ï¼š
  - DeepSpeed ZeRO-2: åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦
  - gradient_checkpointing: ç”¨è®¡ç®—æ¢æ˜¾å­˜
  - bf16: åŠç²¾åº¦è®­ç»ƒ

è¿è¡Œæ–¹å¼ï¼ˆäºŒé€‰ä¸€ï¼‰ï¼š
  python pretrain_qwen3_1.7b.py
  æˆ–
  deepspeed --num_gpus=1 pretrain_qwen3_1.7b.py

ç»§ç»­é¢„è®­ç»ƒ vs ä»é›¶é¢„è®­ç»ƒï¼š
  - ä»é›¶é¢„è®­ç»ƒï¼šéšæœºåˆå§‹åŒ–ï¼Œéœ€è¦æ•°ä¸‡äº¿ token
  - ç»§ç»­é¢„è®­ç»ƒï¼šåˆ©ç”¨å·²æœ‰çŸ¥è¯†ï¼Œæ•°ç™¾ä¸‡ token å³å¯å¢å¼ºç‰¹å®šé¢†åŸŸ
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)

# ===== é…ç½® =====
MODEL_NAME = "/public/huggingface-models/Qwen/Qwen3-1.7B"  # Qwen3 1.7B æ¨¡å‹
OUTPUT_DIR = "./qwen3_1.7b_pretrain"
MAX_LENGTH = 512          # 1.7B å¯ä»¥ç”¨æ›´é•¿åºåˆ—
BATCH_SIZE = 1            # 1.7B å¯ä»¥ç”¨æ›´å¤§ batch
GRADIENT_ACCUMULATION_STEPS = 8  # æœ‰æ•ˆ batch = 2
LEARNING_RATE = 1e-5      # ç»§ç»­é¢„è®­ç»ƒç”¨è¾ƒå°å­¦ä¹ ç‡
NUM_EPOCHS = 1
NUM_SAMPLES = 1000       # è®­ç»ƒæ ·æœ¬æ•°ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
SAVE_STEPS = 500
LOGGING_STEPS = 50

# ===== DeepSpeed é…ç½® =====
DEEPSPEED_CONFIG = {
    "zero_optimization": {
        "stage": 2,  # ZeRO-2ï¼šåˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦
        "offload_optimizer": {
            "device": "cpu",  # å¿…é¡»å¼€å¯ï¼Œå¦åˆ™ OOM
            "pin_memory": True,
        },
        "allgather_partitions": True,
        "allgather_bucket_size": 5e7,
        "reduce_scatter": True,
        "reduce_bucket_size": 5e7,
        "overlap_comm": True,
        "contiguous_gradients": True,
    },
    "bf16": {
        "enabled": True
    },
    "gradient_accumulation_steps": GRADIENT_ACCUMULATION_STEPS,
    "gradient_clipping": 1.0,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
}

# ===== 1. åŠ è½½æ¨¡å‹å’Œ Tokenizer =====
print("=" * 60)
print("ğŸš€ Qwen3-1.7B ç»§ç»­é¢„è®­ç»ƒ (DeepSpeed ZeRO-2)")
print("=" * 60)
print(f"æ¨¡å‹: {MODEL_NAME}")
print(f"åºåˆ—é•¿åº¦: {MAX_LENGTH}")
print(f"Batch Size: {BATCH_SIZE} Ã— {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")

print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# æ³¨æ„ï¼šä½¿ç”¨ DeepSpeed æ—¶ä¸è¦ç”¨ device_map="auto"ï¼ŒDeepSpeed ä¼šè‡ªå·±ç®¡ç†
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True,
    dtype="bfloat16",  # æ–°ç‰ˆ transformers ä½¿ç”¨ dtype
    low_cpu_mem_usage=True,  # é™ä½ CPU å†…å­˜å ç”¨
    # device_map="auto",  # DeepSpeed ä¸éœ€è¦è¿™ä¸ª
)

# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆç”¨è®¡ç®—æ¢æ˜¾å­˜ï¼Œå¿…é¡»å¼€å¯ï¼‰
model.gradient_checkpointing_enable()

# æ‰“å°æ¨¡å‹ä¿¡æ¯
total_params = sum(p.numel() for p in model.parameters())
print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
print(f"   å‚æ•°é‡: {total_params / 1e9:.2f}B")


# ===== 2. åŠ è½½æ•°æ®é›† =====
print("\nğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®...")

# ä»æœ¬åœ° JSON æ–‡ä»¶åŠ è½½ä¸­æ–‡ç»´åŸºç™¾ç§‘æ•°æ®
DATA_PATH = "./wikipedia-cn-20230720-filtered.json"  # æœ¬åœ° JSON æ–‡ä»¶è·¯å¾„

raw_dataset = load_dataset(
    "json",
    data_files=DATA_PATH,
    split="train"
)

# å¦‚æœæ•°æ®é‡å¤§äº NUM_SAMPLESï¼Œåªå–å‰ NUM_SAMPLES æ¡
if len(raw_dataset) > NUM_SAMPLES:
    raw_dataset = raw_dataset.select(range(NUM_SAMPLES))

text_column = "completion"  # JSON ä¸­çš„æ–‡æœ¬å­—æ®µåï¼Œæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹

print(f"âœ… åŠ è½½ {len(raw_dataset)} æ¡æ•°æ®")


# ===== 3. æ•°æ®é¢„å¤„ç† =====
def tokenize_function(examples):
    """
    é¢„è®­ç»ƒæ•°æ®å¤„ç†ï¼š
    - çº¯æ–‡æœ¬ï¼Œæ— å¯¹è¯æ ¼å¼
    - ç›´æ¥é¢„æµ‹ä¸‹ä¸€ä¸ª token
    """
    # ä½¿ç”¨ text_column æŒ‡å®šçš„å­—æ®µ
    texts = examples[text_column]
    
    # Tokenize
    tokenized = tokenizer(
        texts,
        truncation=True,
        max_length=MAX_LENGTH,
        padding=False,
        return_special_tokens_mask=True,
    )
    
    return tokenized


print("\nğŸ”„ å¤„ç†æ•°æ®...")
tokenized_dataset = raw_dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=raw_dataset.column_names,
    desc="Tokenizing",
    num_proc=4,  # å¤šè¿›ç¨‹åŠ é€Ÿ
)

# è¿‡æ»¤å¤ªçŸ­çš„æ ·æœ¬
tokenized_dataset = tokenized_dataset.filter(
    lambda x: len(x["input_ids"]) >= 64,
    desc="Filtering short samples"
)

print(f"âœ… å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ ·æœ¬: {len(tokenized_dataset)}")


# ===== 4. æ•°æ®æ•´ç†å™¨ =====
# ä½¿ç”¨ MLM=False çš„ DataCollatorï¼Œå³æ ‡å‡†çš„ CLMï¼ˆCausal LMï¼‰è®­ç»ƒ
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,  # å› æœè¯­è¨€æ¨¡å‹ï¼Œä¸æ˜¯ BERT çš„ MLM
)


# ===== 5. è®­ç»ƒé…ç½® (DeepSpeed) =====
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    overwrite_output_dir=True,
    
    # è®­ç»ƒå‚æ•°
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    
    # å­¦ä¹ ç‡
    learning_rate=LEARNING_RATE,
    lr_scheduler_type="cosine",
    warmup_ratio=0.05,
    
    # æ˜¾å­˜ä¼˜åŒ–
    bf16=True,
    gradient_checkpointing=True,
    
    # DeepSpeed é…ç½®
    deepspeed=DEEPSPEED_CONFIG,
    
    # æ—¥å¿—å’Œä¿å­˜
    logging_steps=LOGGING_STEPS,
    save_steps=SAVE_STEPS,
    save_total_limit=2,
    
    # å…¶ä»–
    report_to="none",
    dataloader_num_workers=4,
    remove_unused_columns=True,
)


# ===== 6. å¼€å§‹è®­ç»ƒ =====
print("\n" + "=" * 60)
print("ğŸ‹ï¸ å¼€å§‹ç»§ç»­é¢„è®­ç»ƒ (DeepSpeed ZeRO-2)")
print("=" * 60)
print(f"   è®­ç»ƒæ ·æœ¬: {len(tokenized_dataset)}")
print(f"   Batch Size: {BATCH_SIZE}")
print(f"   æ¢¯åº¦ç´¯ç§¯: {GRADIENT_ACCUMULATION_STEPS}")
print(f"   æœ‰æ•ˆ Batch: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")
print(f"   å­¦ä¹ ç‡: {LEARNING_RATE}")
print(f"   Epochs: {NUM_EPOCHS}")
print(f"   DeepSpeed: ZeRO-2 + CPU Offload")

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
)

# è®­ç»ƒ
trainer.train()


# ===== 7. ä¿å­˜æ¨¡å‹ =====
print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹åˆ° {OUTPUT_DIR}...")
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

# æ˜¾å­˜ç»Ÿè®¡
if torch.cuda.is_available():
    peak_memory = torch.cuda.max_memory_allocated() / 1024**3
    print(f"\nğŸ“Š æ˜¾å­˜å³°å€¼: {peak_memory:.2f} GB")

print("\nâœ… ç»§ç»­é¢„è®­ç»ƒå®Œæˆï¼")
print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")


# ===== 8. ç®€å•æµ‹è¯• =====
print("\n" + "=" * 60)
print("ğŸ§ª æµ‹è¯•ç”Ÿæˆæ•ˆæœ")
print("=" * 60)

# é‡æ–°åŠ è½½æ¨¡å‹æµ‹è¯•
del model
torch.cuda.empty_cache()

model = AutoModelForCausalLM.from_pretrained(
    OUTPUT_DIR,
    trust_remote_code=True,
    dtype="bfloat16",
    device_map="auto"
)

test_prompts = [
    "äººå·¥æ™ºèƒ½çš„å‘å±•",
    "ä¸­å›½çš„é¦–éƒ½åŒ—äº¬",
    "æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§",
]

for prompt in test_prompts:
    inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.pad_token_id,
        )
    
    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"\nè¾“å…¥: {prompt}")
    print(f"ç”Ÿæˆ: {generated}")
