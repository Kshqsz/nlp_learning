# test_dpo.py
"""
æµ‹è¯• LoRA DPO åçš„æ¨¡å‹

DPO åçš„æ¨¡å‹ä¸»è¦å¢å¼ºäº†ï¼š
- å›ç­”è´¨é‡ï¼ˆæ›´ç¬¦åˆäººç±»åå¥½ï¼‰
- å®‰å…¨æ€§å’Œæœ‰ç›Šæ€§
- å‡å°‘æœ‰å®³/ä¸å‡†ç¡®å†…å®¹

æµ‹è¯•æ–¹å¼ï¼šå¯¹æ¯” SFT å’Œ DPO æ¨¡å‹çš„å›ç­”è´¨é‡
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# ===== é…ç½® =====
ORIGINAL_MODEL_PATH = "/public/huggingface-models/Qwen/Qwen3-1.7B"
BASE_MODEL_PATH = "./qwen3_1.7b_pretrain"
LORA_SFT_PATH = "./qwen3_1.7b_lora_sft"
LORA_DPO_PATH = "./qwen3_1.7b_lora_dpo"


def load_tokenizer():
    """åŠ è½½ tokenizer"""
    tokenizer = AutoTokenizer.from_pretrained(ORIGINAL_MODEL_PATH, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return tokenizer


def load_base_model():
    """åŠ è½½åŸºåº§æ¨¡å‹"""
    if os.path.exists(BASE_MODEL_PATH):
        model_path = BASE_MODEL_PATH
    else:
        model_path = ORIGINAL_MODEL_PATH
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    return model


def load_sft_model(tokenizer):
    """åŠ è½½ SFT æ¨¡å‹"""
    print("ğŸ“¦ åŠ è½½ SFT æ¨¡å‹...")
    base_model = load_base_model()
    
    if os.path.exists(LORA_SFT_PATH):
        model = PeftModel.from_pretrained(base_model, LORA_SFT_PATH)
        print(f"   âœ… åŠ è½½ LoRA SFT: {LORA_SFT_PATH}")
    else:
        model = base_model
        print("   âš ï¸ SFT æƒé‡ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸºåº§æ¨¡å‹")
    
    model.eval()
    return model


def load_dpo_model(tokenizer):
    """åŠ è½½ DPO æ¨¡å‹"""
    print("ğŸ“¦ åŠ è½½ DPO æ¨¡å‹...")
    
    # å…ˆåŠ è½½åŸºåº§ + SFT
    base_model = load_base_model()
    
    if os.path.exists(LORA_SFT_PATH):
        model = PeftModel.from_pretrained(base_model, LORA_SFT_PATH)
        model = model.merge_and_unload()
        print(f"   âœ… åˆå¹¶ SFT LoRA")
    else:
        model = base_model
    
    # å†åŠ è½½ DPO LoRA
    if os.path.exists(LORA_DPO_PATH):
        model = PeftModel.from_pretrained(model, LORA_DPO_PATH)
        print(f"   âœ… åŠ è½½ DPO LoRA: {LORA_DPO_PATH}")
    else:
        raise FileNotFoundError(f"DPO æƒé‡ä¸å­˜åœ¨: {LORA_DPO_PATH}")
    
    model.eval()
    return model


def chat(model, tokenizer, question, max_new_tokens=256):
    """å¯¹è¯ç”Ÿæˆ"""
    prompt = f"<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
    
    inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    if "assistant" in response:
        response = response.split("assistant")[-1].strip()
    if "<|im_end|>" in response:
        response = response.split("<|im_end|>")[0].strip()
    
    return response


def compare_sft_dpo():
    """å¯¹æ¯” SFT å’Œ DPO æ¨¡å‹"""
    print("=" * 70)
    print("ğŸ”¬ SFT vs DPO æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    print("=" * 70)
    
    tokenizer = load_tokenizer()
    
    # æµ‹è¯•é—®é¢˜ï¼ˆåŒ…å«ä¸€äº›å¯èƒ½æœ‰åå¥½å·®å¼‚çš„é—®é¢˜ï¼‰
    test_questions = [
        # ä¸€èˆ¬çŸ¥è¯†é—®é¢˜
        "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹æœºå™¨å­¦ä¹ ",
        "å¦‚ä½•ä¿æŒå¥åº·çš„ç”Ÿæ´»ä¹ æƒ¯ï¼Ÿ",
        
        # å»ºè®®ç±»é—®é¢˜
        "æˆ‘æƒ³å­¦ä¹ ç¼–ç¨‹ï¼Œåº”è¯¥ä»å“ªé‡Œå¼€å§‹ï¼Ÿ",
        "å¦‚ä½•æé«˜å·¥ä½œæ•ˆç‡ï¼Ÿ",
        
        # å¯èƒ½æ¶‰åŠåå¥½çš„é—®é¢˜
        "å¦‚ä½•çœ‹å¾…äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Ÿ",
        "è¯·ç»™æˆ‘ä¸€äº›æ—¶é—´ç®¡ç†çš„å»ºè®®",
        
        # åˆ›æ„ç±»é—®é¢˜
        "å†™ä¸€ä¸ªå…³äºå‹è°Šçš„çŸ­å¥",
    ]
    
    # åŠ è½½ SFT æ¨¡å‹å¹¶æµ‹è¯•
    print("\n" + "=" * 70)
    print("ğŸ“ SFT æ¨¡å‹å›ç­”")
    print("=" * 70)
    
    sft_responses = {}
    try:
        sft_model = load_sft_model(tokenizer)
        for q in test_questions:
            resp = chat(sft_model, tokenizer, q)
            sft_responses[q] = resp
            print(f"\nã€é—®é¢˜ã€‘{q}")
            print(f"ã€SFTã€‘{resp[:300]}")
        
        del sft_model
        torch.cuda.empty_cache()
    except Exception as e:
        print(f"âŒ SFT æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    # åŠ è½½ DPO æ¨¡å‹å¹¶æµ‹è¯•
    print("\n" + "=" * 70)
    print("ğŸ“ DPO æ¨¡å‹å›ç­”")
    print("=" * 70)
    
    dpo_responses = {}
    try:
        dpo_model = load_dpo_model(tokenizer)
        for q in test_questions:
            resp = chat(dpo_model, tokenizer, q)
            dpo_responses[q] = resp
            print(f"\nã€é—®é¢˜ã€‘{q}")
            print(f"ã€DPOã€‘{resp[:300]}")
        
        del dpo_model
        torch.cuda.empty_cache()
    except Exception as e:
        print(f"âŒ DPO æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # å¹¶æ’å¯¹æ¯”
    print("\n" + "=" * 70)
    print("ğŸ“Š å¹¶æ’å¯¹æ¯”")
    print("=" * 70)
    
    for q in test_questions:
        print(f"\n{'='*60}")
        print(f"ã€é—®é¢˜ã€‘{q}")
        print("-" * 60)
        if q in sft_responses:
            print(f"ã€SFTã€‘{sft_responses[q][:200]}...")
        print("-" * 60)
        if q in dpo_responses:
            print(f"ã€DPOã€‘{dpo_responses[q][:200]}...")
    
    # æ€»ç»“
    print("\n" + "=" * 70)
    print("ğŸ“ˆ å¯¹æ¯”åˆ†æ")
    print("=" * 70)
    print("""
    DPO è®­ç»ƒåï¼Œæ¨¡å‹åº”è¯¥è¡¨ç°å‡ºï¼š
    
    1. âœ… æ›´æœ‰å¸®åŠ©çš„å›ç­”ï¼ˆç›´æ¥å›ç­”é—®é¢˜ï¼‰
    2. âœ… æ›´å®‰å…¨çš„å†…å®¹ï¼ˆé¿å…æœ‰å®³å»ºè®®ï¼‰
    3. âœ… æ›´å¥½çš„æ ¼å¼ï¼ˆæ¸…æ™°ã€æœ‰æ¡ç†ï¼‰
    4. âœ… æ›´ç¬¦åˆäººç±»åå¥½çš„è¯­æ°”
    
    å¦‚æœæ•ˆæœä¸æ˜æ˜¾ï¼Œå¯ä»¥å°è¯•ï¼š
    - å¢åŠ  DPO è®­ç»ƒæ•°æ®é‡
    - è°ƒæ•´ beta å‚æ•°ï¼ˆå¢å¤§ä¼šæ›´ä¿å®ˆï¼‰
    - å¢åŠ è®­ç»ƒè½®æ•°
    """)


def interactive_test():
    """äº¤äº’å¼æµ‹è¯• DPO æ¨¡å‹"""
    print("=" * 60)
    print("ğŸ’¬ DPO æ¨¡å‹äº¤äº’æµ‹è¯•")
    print("=" * 60)
    
    tokenizer = load_tokenizer()
    model = load_dpo_model(tokenizer)
    
    print("\nâœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¼€å§‹å¯¹è¯ï¼ˆè¾“å…¥ q é€€å‡ºï¼‰\n")
    
    while True:
        question = input("ä½ : ").strip()
        if question.lower() == 'q':
            print("å†è§ï¼")
            break
        if not question:
            continue
        
        response = chat(model, tokenizer, question)
        print(f"AI: {response}\n")


def test_dpo_only():
    """åªæµ‹è¯• DPO æ¨¡å‹"""
    print("=" * 60)
    print("ğŸ§ª DPO æ¨¡å‹æµ‹è¯•")
    print("=" * 60)
    
    tokenizer = load_tokenizer()
    model = load_dpo_model(tokenizer)
    
    test_questions = [
        "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
        "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
        "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„çŸ­è¯—",
        "Python æœ‰å“ªäº›ä¼˜ç‚¹ï¼Ÿ",
        "å¦‚ä½•ä¿æŒå¥åº·ï¼Ÿ",
    ]
    
    for q in test_questions:
        print(f"\nã€é—®é¢˜ã€‘{q}")
        print("-" * 40)
        resp = chat(model, tokenizer, q)
        print(f"ã€å›ç­”ã€‘{resp}")
    
    # è¿›å…¥äº¤äº’æ¨¡å¼
    print("\n" + "=" * 60)
    print("ğŸ’¬ è¿›å…¥äº¤äº’æ¨¡å¼ï¼ˆè¾“å…¥ q é€€å‡ºï¼‰")
    print("=" * 60)
    
    while True:
        question = input("\nä½ : ").strip()
        if question.lower() == 'q':
            break
        if not question:
            continue
        response = chat(model, tokenizer, question)
        print(f"AI: {response}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "--compare":
            compare_sft_dpo()
        elif sys.argv[1] == "--chat":
            interactive_test()
    else:
        # é»˜è®¤ï¼šæµ‹è¯• + å¯¹æ¯”
        try:
            test_dpo_only()
        except FileNotFoundError as e:
            print(f"âš ï¸ {e}")
            print("è¯·å…ˆè¿è¡Œ DPO è®­ç»ƒ: python lora_dpo_qwen3_1.7b.py")
