# test_pretrain.py
"""
æµ‹è¯•ç»§ç»­é¢„è®­ç»ƒåçš„æ¨¡å‹

ç»§ç»­é¢„è®­ç»ƒçš„æ¨¡å‹ä¸»è¦å¢å¼ºäº†ï¼š
- é¢†åŸŸçŸ¥è¯†ï¼ˆå¦‚ä¸­æ–‡ç»´åŸºç™¾ç§‘çŸ¥è¯†ï¼‰
- è¯­è¨€å»ºæ¨¡èƒ½åŠ›

æµ‹è¯•æ–¹å¼ï¼šç»™å®šå¼€å¤´ï¼Œè®©æ¨¡å‹ç»­å†™
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ===== é…ç½® =====
# é¢„è®­ç»ƒåçš„æ¨¡å‹è·¯å¾„
PRETRAIN_MODEL_PATH = "./qwen3_1.7b_pretrain"

# åŸå§‹æ¨¡å‹è·¯å¾„ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
ORIGINAL_MODEL_PATH = "/public/huggingface-models/Qwen/Qwen3-1.7B"


def load_model(model_path):
    """åŠ è½½æ¨¡å‹"""
    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    model.eval()
    return model, tokenizer


def generate_text(model, tokenizer, prompt, max_new_tokens=150):
    """
    æ–‡æœ¬ç»­å†™ï¼ˆé¢„è®­ç»ƒæ¨¡å‹çš„æµ‹è¯•æ–¹å¼ï¼‰
    ç›´æ¥ç»™å¼€å¤´ï¼Œè®©æ¨¡å‹ç»­å†™
    """
    inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.pad_token_id,
        )
    
    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated


def test_model(model, tokenizer, model_name):
    """æµ‹è¯•æ¨¡å‹çš„ç»­å†™èƒ½åŠ›"""
    print(f"\n{'='*60}")
    print(f"ğŸ“ {model_name} ç»­å†™æµ‹è¯•")
    print("="*60)
    
    # æµ‹è¯•æ–‡æœ¬å¼€å¤´ï¼ˆé€‚åˆé¢„è®­ç»ƒæ¨¡å‹ï¼‰
    test_prompts = [
        "äººå·¥æ™ºèƒ½æ˜¯",
        "ä¸­å›½çš„é¦–éƒ½åŒ—äº¬æ˜¯ä¸€åº§",
        "æœºå™¨å­¦ä¹ çš„ä¸»è¦æ–¹æ³•åŒ…æ‹¬",
        "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯å¯ä»¥ç”¨äº",
        "æ·±åº¦å­¦ä¹ åœ¨è¿‘å¹´æ¥å–å¾—äº†",
    ]
    
    for prompt in test_prompts:
        print(f"\nã€å¼€å¤´ã€‘{prompt}")
        print("-" * 40)
        generated = generate_text(model, tokenizer, prompt)
        print(f"ã€ç»­å†™ã€‘{generated}")


def compare_models():
    """å¯¹æ¯”åŸå§‹æ¨¡å‹å’Œé¢„è®­ç»ƒåçš„æ¨¡å‹"""
    print("=" * 60)
    print("ğŸ”¬ é¢„è®­ç»ƒæ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•é¢„è®­ç»ƒåçš„æ¨¡å‹
    try:
        pretrain_model, pretrain_tokenizer = load_model(PRETRAIN_MODEL_PATH)
        test_model(pretrain_model, pretrain_tokenizer, "é¢„è®­ç»ƒåæ¨¡å‹")
        
        # é‡Šæ”¾æ˜¾å­˜
        del pretrain_model
        torch.cuda.empty_cache()
    except Exception as e:
        print(f"âš ï¸ é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("   è¯·ç¡®è®¤æ¨¡å‹å·²è®­ç»ƒå®Œæˆå¹¶ä¿å­˜åˆ°å¯¹åº”è·¯å¾„")
    
    # æµ‹è¯•åŸå§‹æ¨¡å‹ï¼ˆå¯¹æ¯”ï¼‰
    print("\n" + "=" * 60)
    print("ğŸ“Š åŠ è½½åŸå§‹æ¨¡å‹è¿›è¡Œå¯¹æ¯”...")
    print("=" * 60)
    
    try:
        original_model, original_tokenizer = load_model(ORIGINAL_MODEL_PATH)
        test_model(original_model, original_tokenizer, "åŸå§‹æ¨¡å‹")
        
        del original_model
        torch.cuda.empty_cache()
    except Exception as e:
        print(f"âš ï¸ åŸå§‹æ¨¡å‹åŠ è½½å¤±è´¥: {e}")


def test_single_model():
    """åªæµ‹è¯•é¢„è®­ç»ƒåçš„æ¨¡å‹"""
    print("=" * 60)
    print("ğŸ§ª é¢„è®­ç»ƒæ¨¡å‹æµ‹è¯•")
    print("=" * 60)
    
    model, tokenizer = load_model(PRETRAIN_MODEL_PATH)
    test_model(model, tokenizer, "é¢„è®­ç»ƒåæ¨¡å‹")
    
    # äº¤äº’å¼æµ‹è¯•
    print("\n" + "=" * 60)
    print("ğŸ’¬ äº¤äº’å¼æµ‹è¯•ï¼ˆè¾“å…¥ q é€€å‡ºï¼‰")
    print("=" * 60)
    
    while True:
        prompt = input("\nè¯·è¾“å…¥æ–‡æœ¬å¼€å¤´: ").strip()
        if prompt.lower() == 'q':
            break
        if not prompt:
            continue
        
        generated = generate_text(model, tokenizer, prompt)
        print(f"ã€ç»­å†™ã€‘{generated}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--compare":
        # å¯¹æ¯”æ¨¡å¼
        compare_models()
    else:
        # å•ç‹¬æµ‹è¯•
        test_single_model()
