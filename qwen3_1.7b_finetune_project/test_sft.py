# test_sft.py
"""
æµ‹è¯• LoRA SFT åçš„æ¨¡å‹

SFT åçš„æ¨¡å‹ä¸»è¦å¢å¼ºäº†ï¼š
- æŒ‡ä»¤éµå¾ªèƒ½åŠ›
- å¯¹è¯èƒ½åŠ›
- å›ç­”é—®é¢˜çš„èƒ½åŠ›

æµ‹è¯•æ–¹å¼ï¼šä½¿ç”¨å¯¹è¯æ ¼å¼æé—®
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# ===== é…ç½® =====
# åŸºåº§æ¨¡å‹è·¯å¾„ï¼ˆé¢„è®­ç»ƒåçš„æ¨¡å‹æˆ–åŸå§‹æ¨¡å‹ï¼‰
BASE_MODEL_PATH = "./qwen3_1.7b_pretrain"
# BASE_MODEL_PATH = "/public/huggingface-models/Qwen/Qwen3-1.7B"

# LoRA SFT æƒé‡è·¯å¾„
LORA_SFT_PATH = "./qwen3_1.7b_lora_sft"


def load_sft_model():
    """åŠ è½½ LoRA SFT æ¨¡å‹"""
    print("ğŸ“¦ åŠ è½½åŸºåº§æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(LORA_SFT_PATH, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    print("ğŸ”§ åŠ è½½ LoRA æƒé‡...")
    model = PeftModel.from_pretrained(
        base_model,
        LORA_SFT_PATH,
        torch_dtype=torch.bfloat16,
    )
    model.eval()
    
    return model, tokenizer


def load_base_model():
    """åŠ è½½åŸºåº§æ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”ï¼‰"""
    print("ğŸ“¦ åŠ è½½åŸºåº§æ¨¡å‹ï¼ˆæ—  LoRAï¼‰...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    model.eval()
    
    return model, tokenizer


def chat(model, tokenizer, question, max_new_tokens=256):
    """
    å¯¹è¯ç”Ÿæˆï¼ˆSFT æ¨¡å‹çš„æµ‹è¯•æ–¹å¼ï¼‰
    ä½¿ç”¨ ChatML æ ¼å¼
    """
    # æ„å»ºå¯¹è¯æ ¼å¼
    prompt = f"<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
    
    inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # æå– assistant å›å¤
    if "assistant" in response:
        response = response.split("assistant")[-1].strip()
    
    # æ¸…ç†ç»“æŸæ ‡è®°
    if "<|im_end|>" in response:
        response = response.split("<|im_end|>")[0].strip()
    
    return response


def test_model(model, tokenizer, model_name):
    """æµ‹è¯•æ¨¡å‹çš„å¯¹è¯èƒ½åŠ›"""
    print(f"\n{'='*60}")
    print(f"ğŸ’¬ {model_name} å¯¹è¯æµ‹è¯•")
    print("="*60)
    
    # æµ‹è¯•é—®é¢˜ï¼ˆé€‚åˆ SFT æ¨¡å‹ï¼‰
    test_questions = [
        "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
        "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿç»™æˆ‘ä¸€äº›å»ºè®®",
        "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„çŸ­è¯—",
        "Python æœ‰å“ªäº›ä¼˜ç‚¹ï¼Ÿ",
        "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
        "å¸®æˆ‘å†™ä¸€æ®µè‡ªæˆ‘ä»‹ç»ï¼Œæˆ‘æ˜¯ä¸€åå¤§å­¦ç”Ÿ",
    ]
    
    for question in test_questions:
        print(f"\nã€é—®é¢˜ã€‘{question}")
        print("-" * 40)
        response = chat(model, tokenizer, question)
        print(f"ã€å›ç­”ã€‘{response[:500]}")  # æˆªæ–­æ˜¾ç¤º


def compare_models():
    """å¯¹æ¯”åŸºåº§æ¨¡å‹å’Œ SFT æ¨¡å‹"""
    print("=" * 60)
    print("ğŸ”¬ SFT æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    test_questions = [
        "è¯·ä»‹ç»ä¸€ä¸‹åŒ—äº¬",
        "å¦‚ä½•ä¿æŒå¥åº·ï¼Ÿ",
        "å†™ä¸€ä¸ªç®€çŸ­çš„æ•…äº‹å¼€å¤´",
    ]
    
    # æµ‹è¯• SFT æ¨¡å‹
    print("\nğŸ“¦ åŠ è½½ LoRA SFT æ¨¡å‹...")
    try:
        sft_model, sft_tokenizer = load_sft_model()
        
        print("\n" + "=" * 60)
        print("ğŸ’¬ LoRA SFT æ¨¡å‹å›å¤")
        print("=" * 60)
        
        for question in test_questions:
            print(f"\nã€é—®é¢˜ã€‘{question}")
            response = chat(sft_model, sft_tokenizer, question)
            print(f"ã€SFT å›ç­”ã€‘{response[:300]}")
        
        del sft_model
        torch.cuda.empty_cache()
        
    except Exception as e:
        print(f"âš ï¸ SFT æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("   è¯·ç¡®è®¤ LoRA SFT è®­ç»ƒå·²å®Œæˆ")
        return
    
    # æµ‹è¯•åŸºåº§æ¨¡å‹ï¼ˆå¯¹æ¯”ï¼‰
    print("\n" + "=" * 60)
    print("ğŸ“Š åŸºåº§æ¨¡å‹å›å¤ï¼ˆå¯¹æ¯”ï¼‰")
    print("=" * 60)
    
    try:
        base_model, base_tokenizer = load_base_model()
        
        for question in test_questions:
            print(f"\nã€é—®é¢˜ã€‘{question}")
            response = chat(base_model, base_tokenizer, question)
            print(f"ã€åŸºåº§å›ç­”ã€‘{response[:300]}")
        
        del base_model
        torch.cuda.empty_cache()
        
    except Exception as e:
        print(f"âš ï¸ åŸºåº§æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    # å¯¹æ¯”æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š å¯¹æ¯”æ€»ç»“")
    print("=" * 60)
    print("""
    | æ¨¡å‹      | ç‰¹ç‚¹ |
    |-----------|------|
    | åŸºåº§æ¨¡å‹  | ç»­å†™èƒ½åŠ›å¼ºï¼Œä½†å¯èƒ½ä¸æ“…é•¿å¯¹è¯ |
    | SFT æ¨¡å‹  | æ›´å¥½åœ°éµå¾ªæŒ‡ä»¤ï¼Œå›ç­”æ›´æœ‰å¸®åŠ© |
    
    é€šè¿‡ LoRA SFTï¼Œæ¨¡å‹å­¦ä¼šäº†ï¼š
    1. ç†è§£ç”¨æˆ·çš„é—®é¢˜æ„å›¾
    2. æŒ‰ç…§æŒ‡ä»¤æ ¼å¼å›ç­”
    3. ç”Ÿæˆæ›´æœ‰å¸®åŠ©ã€æ›´ç›¸å…³çš„å›å¤
    """)


def interactive_chat():
    """äº¤äº’å¼å¯¹è¯"""
    print("=" * 60)
    print("ğŸ’¬ äº¤äº’å¼å¯¹è¯æµ‹è¯•")
    print("=" * 60)
    
    model, tokenizer = load_sft_model()
    
    print("\nâœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¼€å§‹å¯¹è¯ï¼ˆè¾“å…¥ q é€€å‡ºï¼‰\n")
    
    while True:
        question = input("ä½ : ").strip()
        if question.lower() == 'q':
            print("å†è§ï¼")
            break
        if not question:
            continue
        
        response = chat(model, tokenizer, question)
        print(f"AI: {response}\n")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "--compare":
            # å¯¹æ¯”æ¨¡å¼
            compare_models()
        elif sys.argv[1] == "--chat":
            # äº¤äº’å¯¹è¯
            interactive_chat()
    else:
        # é»˜è®¤ï¼šæµ‹è¯• SFT æ¨¡å‹
        try:
            model, tokenizer = load_sft_model()
            test_model(model, tokenizer, "LoRA SFT æ¨¡å‹")
            
            # è¿›å…¥äº¤äº’æ¨¡å¼
            print("\n" + "=" * 60)
            print("ğŸ’¬ è¿›å…¥äº¤äº’æ¨¡å¼ï¼ˆè¾“å…¥ q é€€å‡ºï¼‰")
            print("=" * 60)
            
            while True:
                question = input("\nä½ : ").strip()
                if question.lower() == 'q':
                    break
                if not question:
                    continue
                response = chat(model, tokenizer, question)
                print(f"AI: {response}")
                
        except Exception as e:
            print(f"âš ï¸ é”™è¯¯: {e}")
            print("è¯·ç¡®è®¤ LoRA SFT è®­ç»ƒå·²å®Œæˆ")
