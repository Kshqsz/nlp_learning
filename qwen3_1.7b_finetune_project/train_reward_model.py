# train_reward_model.py
"""
Qwen3-1.7B å¥–åŠ±æ¨¡å‹è®­ç»ƒ

è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°æ¨¡å‹è¾“å‡ºçš„è´¨é‡
å¥–åŠ±æ¨¡å‹ç”¨äºåç»­çš„ PPO å¼ºåŒ–å­¦ä¹ è®­ç»ƒ

è¿è¡Œæ–¹å¼ï¼š
  python train_reward_model.py
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)
from peft import LoraConfig, get_peft_model, TaskType
import json

# ===== é…ç½® =====
# åŸºåº§æ¨¡å‹è·¯å¾„
BASE_MODEL_PATH = "/root/data/hsk-models/qwen3_1.7b_lora_sft"  # SFT åçš„æ¨¡å‹
ORIGINAL_MODEL_PATH = "/public/huggingface-models/Qwen/Qwen3-1.7B"

# è¾“å‡ºè·¯å¾„
OUTPUT_DIR = "/root/data/hsk-models/qwen3_1.7b_reward_model"

# è¶…å‚æ•°
MAX_LENGTH = 512
BATCH_SIZE = 8
GRADIENT_ACCUMULATION_STEPS = 4  # æœ‰æ•ˆ batch = 16
LEARNING_RATE = 5e-5
NUM_EPOCHS = 3
NUM_SAMPLES = 10000  # å¥–åŠ±æ¨¡å‹æ•°æ®é‡

# LoRA é…ç½®
LORA_R = 16
LORA_ALPHA = 32
LORA_DROPOUT = 0.1
TARGET_MODULES = [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj",
]


def load_reward_dataset():
    """åŠ è½½æˆ–åˆ›å»ºå¥–åŠ±æ¨¡å‹è®­ç»ƒæ•°æ®é›†"""
    print("\nğŸ“Š åŠ è½½å¥–åŠ±æ¨¡å‹è®­ç»ƒæ•°æ®...")
    
    # å°è¯•åŠ è½½æœ¬åœ°æ•°æ®
    local_reward_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "dpo_zh.jsonl")
    
    if os.path.exists(local_reward_path):
        print(f"   ä½¿ç”¨æœ¬åœ° SFT æ•°æ®: {local_reward_path}")
        dataset = load_dataset("json", data_files=local_reward_path, split="train")
        
        # å°† SFT æ•°æ®è½¬æ¢ä¸ºå¥–åŠ±æ¨¡å‹æ ¼å¼
        # åœ¨è¿™ä¸ªç®€åŒ–ç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬å‡è®¾é«˜è´¨é‡çš„ SFT æ•°æ®å¯¹åº”å¥–åŠ± 1ï¼Œ
        # éœ€è¦å¦å¤–å‡†å¤‡åä¾‹æ•°æ®å¯¹åº”å¥–åŠ± 0
        
        if len(dataset) > NUM_SAMPLES:
            dataset = dataset.shuffle(seed=42).select(range(NUM_SAMPLES))
        
        print(f"   âœ… åŠ è½½ {len(dataset)} æ¡æ•°æ®")
        return dataset
    
    # å¦‚æœæ²¡æœ‰æœ¬åœ°æ•°æ®ï¼Œåˆ›å»ºæ¼”ç¤ºæ•°æ®
    print("   âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°æ•°æ®ï¼Œåˆ›å»ºæ¼”ç¤ºæ•°æ®...")
    
    # æ¼”ç¤ºæ•°æ®æ ¼å¼ï¼šgood response å’Œ bad response å¯¹
    demo_data = []
    demo_examples = [
        {
            "question": "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
            "good_response": "å­¦ä¹ ç¼–ç¨‹éœ€è¦ï¼š1) å­¦ä¹ åŸºç¡€è¯­æ³•å’Œæ¦‚å¿µ 2) é€šè¿‡é¡¹ç›®å®è·µ 3) é˜…è¯»ä¼˜è´¨ä»£ç  4) æŒç»­åˆ·é¢˜è®­ç»ƒã€‚å»ºè®®ä» Python æˆ– JavaScript å¼€å§‹ã€‚",
            "bad_response": "ç¼–ç¨‹å¾ˆéš¾ã€‚",
        },
        {
            "question": "Python çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
            "good_response": "Python å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š1) è¯­æ³•ç®€æ´æ˜“å­¦ 2) åº“ç”Ÿæ€ä¸°å¯Œ 3) åº”ç”¨å¹¿æ³›ï¼ˆWebã€æ•°æ®ç§‘å­¦ã€AI ç­‰ï¼‰4) ç¤¾åŒºæ´»è·ƒ 5) è·¨å¹³å°å…¼å®¹ã€‚",
            "bad_response": "Python ä¸é”™ã€‚",
        },
        {
            "question": "æ€æ ·ä¿æŒèº«ä½“å¥åº·ï¼Ÿ",
            "good_response": "ä¿æŒå¥åº·éœ€è¦ï¼š1) è§„å¾‹è¿åŠ¨ï¼ˆæ¯å‘¨ 3-5 æ¬¡ï¼‰2) å‡è¡¡é¥®é£Ÿ 3) å……è¶³ç¡çœ ï¼ˆ7-9 å°æ—¶ï¼‰4) å‹åŠ›ç®¡ç† 5) å®šæœŸä½“æ£€ã€‚",
            "bad_response": "å¤šè¿åŠ¨ã€‚",
        },
    ]
    
    # æ‰©å±•æ¼”ç¤ºæ•°æ®åˆ°æ‰€éœ€æ•°é‡
    for _ in range(NUM_SAMPLES // len(demo_examples)):
        demo_data.extend(demo_examples)
    
    # åˆ›å»ºæ•°æ®é›†
    from datasets import Dataset
    dataset = Dataset.from_dict({
        "question": [d["question"] for d in demo_data],
        "good_response": [d["good_response"] for d in demo_data],
        "bad_response": [d["bad_response"] for d in demo_data],
    })
    
    print(f"   âœ… åˆ›å»ºæ¼”ç¤ºæ•°æ®é›†ï¼š{len(dataset)} æ¡")
    return dataset


def preprocess_reward_data(examples, tokenizer):
    """
    å°†æ•°æ®è½¬æ¢ä¸ºå¥–åŠ±æ¨¡å‹æ ¼å¼
    
    å¥–åŠ±æ¨¡å‹çš„è¾“å…¥æ ¼å¼ï¼š
    - å¯¹äº good responseï¼š[prompt, good_response] â†’ æ ‡ç­¾ 1
    - å¯¹äº bad responseï¼š[prompt, bad_response] â†’ æ ‡ç­¾ 0
    """
    input_ids_list = []
    labels_list = []
    attention_mask_list = []
    
    # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
    if "input" in examples and "target" in examples:
        # SFT æ•°æ®æ ¼å¼ï¼š{input, target}
        # æ‰€æœ‰ SFT æ•°æ®å‡è®¾éƒ½æ˜¯é«˜è´¨é‡çš„ï¼Œæ ‡ç­¾ä¸º 1
        for inp, target in zip(examples["input"], examples["target"]):
            if len(inp) > 400 or len(target) > 400:
                continue
            
            # æ„å»ºå®Œæ•´æ–‡æœ¬
            prompt = f"<|im_start|>user\n{inp}<|im_end|>\n<|im_start|>assistant\n"
            response = f"{target}<|im_end|>"
            full_text = prompt + response
            
            # Tokenize
            tokenized = tokenizer(
                full_text,
                max_length=MAX_LENGTH,
                truncation=True,
                padding=False,
                add_special_tokens=False,
            )
            
            input_ids_list.append(tokenized["input_ids"])
            labels_list.append(1)  # é«˜è´¨é‡å›å¤ï¼Œæ ‡ç­¾ä¸º 1
            attention_mask_list.append(tokenized["attention_mask"])
    
    elif "question" in examples and ("response_chosen" in examples or "good_response" in examples):
        # åå¥½å¯¹æ•°æ®æ ¼å¼ï¼š{question, good_response, bad_response} æˆ– {question, response_chosen, response_rejected}
        
        # ç»Ÿä¸€åˆ—å
        if "response_chosen" in examples:
            good_responses = examples["response_chosen"]
            bad_responses = examples["response_rejected"]
        else:
            good_responses = examples["good_response"]
            bad_responses = examples["bad_response"]

        for question, good_resp, bad_resp in zip(
            examples["question"],
            good_responses,
            bad_responses
        ):
            if len(question) > 400 or len(good_resp) > 400 or len(bad_resp) > 400:
                continue
            
            # å¤„ç† good response (æ ‡ç­¾ 1)
            prompt = f"<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
            good_full = prompt + f"{good_resp}<|im_end|>"
            
            tokenized_good = tokenizer(
                good_full,
                max_length=MAX_LENGTH,
                truncation=True,
                padding=False,
                add_special_tokens=False,
            )
            
            input_ids_list.append(tokenized_good["input_ids"])
            labels_list.append(1)
            attention_mask_list.append(tokenized_good["attention_mask"])
            
            # å¤„ç† bad response (æ ‡ç­¾ 0)
            bad_full = prompt + f"{bad_resp}<|im_end|>"
            
            tokenized_bad = tokenizer(
                bad_full,
                max_length=MAX_LENGTH,
                truncation=True,
                padding=False,
                add_special_tokens=False,
            )
            
            input_ids_list.append(tokenized_bad["input_ids"])
            labels_list.append(0)
            attention_mask_list.append(tokenized_bad["attention_mask"])
    
    return {
        "input_ids": input_ids_list,
        "attention_mask": attention_mask_list,
        "labels": labels_list,
    }


def create_reward_model(model_path, tokenizer):
    """åˆ›å»ºå¥–åŠ±æ¨¡å‹"""
    print("ğŸ“¦ åˆ›å»ºå¥–åŠ±æ¨¡å‹...")
    
    # ä» causal LM åŠ è½½åŸºç¡€æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )
    
    # æ·»åŠ  value headï¼ˆç”¨äºè¯„åˆ†ï¼‰
    # ç®€å•æ–¹æ³•ï¼šä½¿ç”¨æœ€åä¸€ä¸ª token çš„ hidden state æ¥é¢„æµ‹åˆ†æ•°
    hidden_size = model.config.hidden_size
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚ä½œä¸º reward head
    class RewardHead(torch.nn.Module):
        def __init__(self, hidden_size):
            super().__init__()
            self.linear = torch.nn.Linear(hidden_size, 1)
        
        def forward(self, hidden_states):
            # ä½¿ç”¨æœ€åä¸€ä¸ª token çš„ hidden state
            last_hidden_state = hidden_states[:, -1, :]
            return self.linear(last_hidden_state)
    
    # æ·»åŠ  reward head
    model.reward_head = RewardHead(hidden_size).to(model.device)
    
    return model


def main():
    print("=" * 60)
    print("ğŸ¯ Qwen3-1.7B å¥–åŠ±æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)
    
    # åŠ è½½ tokenizer
    print("\nğŸ“¦ åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(ORIGINAL_MODEL_PATH, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ•°æ®
    dataset = load_reward_dataset()
    
    # é¢„å¤„ç†æ•°æ®
    print("\nğŸ”„ å¤„ç†æ•°æ®...")
    processed_dataset = dataset.map(
        lambda x: preprocess_reward_data(x, tokenizer),
        batched=True,
        remove_columns=dataset.column_names,
        desc="Processing reward data",
        num_proc=4,
    )
    
    # è¿‡æ»¤ç©ºæ ·æœ¬
    processed_dataset = processed_dataset.filter(lambda x: len(x["input_ids"]) > 0)
    print(f"âœ… æœ‰æ•ˆæ ·æœ¬: {len(processed_dataset)}")
    
    # åŠ è½½æ¨¡å‹
    print("\nğŸ“¦ åŠ è½½åŸºåº§æ¨¡å‹...")
    if os.path.exists(BASE_MODEL_PATH):
        print(f"   ä½¿ç”¨ SFT æ¨¡å‹: {BASE_MODEL_PATH}")
        model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_PATH,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map="auto",
        )
    else:
        print(f"   ä½¿ç”¨åŸå§‹æ¨¡å‹: {ORIGINAL_MODEL_PATH}")
        model = AutoModelForCausalLM.from_pretrained(
            ORIGINAL_MODEL_PATH,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map="auto",
        )
    
    # é…ç½® LoRA
    print("\nğŸ”§ é…ç½® LoRA...")
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=LORA_DROPOUT,
        target_modules=TARGET_MODULES,
        bias="none",
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # æ·»åŠ  reward head
    hidden_size = model.config.hidden_size
    
    class RewardHead(torch.nn.Module):
        def __init__(self, hidden_size):
            super().__init__()
            self.linear = torch.nn.Linear(hidden_size, 1)
        
        def forward(self, hidden_states):
            last_hidden_state = hidden_states[:, -1, :]
            return self.linear(last_hidden_state)
    
    model.reward_head = RewardHead(hidden_size).to(model.device)
    model.enable_input_require_grads()
    
    # è‡ªå®šä¹‰ Data Collator
    class RewardDataCollator:
        def __init__(self, tokenizer, max_length=512):
            self.tokenizer = tokenizer
            self.max_length = max_length
        
        def __call__(self, features):
            # è·å–æœ€å¤§é•¿åº¦
            max_len = max(len(f["input_ids"]) for f in features)
            max_len = min(max_len, self.max_length)
            
            batch_input_ids = []
            batch_attention_mask = []
            batch_labels = []
            
            for feature in features:
                input_ids = feature["input_ids"][:max_len]
                attention_mask = feature["attention_mask"][:max_len]
                
                # Padding
                pad_len = max_len - len(input_ids)
                input_ids = input_ids + [self.tokenizer.pad_token_id] * pad_len
                attention_mask = attention_mask + [0] * pad_len
                
                batch_input_ids.append(input_ids)
                batch_attention_mask.append(attention_mask)
                batch_labels.append(float(feature["labels"]))
            
            return {
                "input_ids": torch.tensor(batch_input_ids, dtype=torch.long),
                "attention_mask": torch.tensor(batch_attention_mask, dtype=torch.long),
                "labels": torch.tensor(batch_labels, dtype=torch.float),
            }
    
    # è‡ªå®šä¹‰ Trainerï¼ˆæ”¯æŒ reward headï¼‰
    class RewardTrainer(Trainer):
        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
            # å‰å‘ä¼ æ’­
            outputs = model(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                output_hidden_states=True,
            )
            
            hidden_states = outputs.hidden_states[-1]  # æœ€åä¸€å±‚çš„ hidden states
            rewards = model.reward_head(hidden_states).squeeze(-1)
            
            # è®¡ç®—äºŒåˆ†ç±»æŸå¤±
            labels = inputs["labels"]
            loss = torch.nn.functional.binary_cross_entropy_with_logits(rewards, labels)
            
            if return_outputs:
                return loss, (rewards, labels)
            return loss
    
    # è®­ç»ƒé…ç½®
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        
        # è®­ç»ƒå‚æ•°
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        
        # å­¦ä¹ ç‡
        learning_rate=LEARNING_RATE,
        lr_scheduler_type="cosine",
        warmup_steps=100,
        weight_decay=0.01,
        max_grad_norm=1.0,
        
        # æ˜¾å­˜ä¼˜åŒ–
        bf16=True,
        gradient_checkpointing=True,
        gradient_checkpointing_kwargs={"use_reentrant": False},
        optim="adamw_torch",
        
        # æ—¥å¿—å’Œä¿å­˜
        logging_steps=10,
        save_steps=200,
        save_total_limit=2,
        
        # å…¶ä»–
        report_to="none",
        dataloader_num_workers=4,
        dataloader_pin_memory=True,
        seed=42,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("\n" + "=" * 60)
    print("ğŸ‹ï¸ å¼€å§‹å¥–åŠ±æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)
    print(f"   è®­ç»ƒæ ·æœ¬: {len(processed_dataset)}")
    print(f"   Batch Size: {BATCH_SIZE}")
    print(f"   æ¢¯åº¦ç´¯ç§¯: {GRADIENT_ACCUMULATION_STEPS}")
    print(f"   æœ‰æ•ˆ Batch: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")
    print(f"   Learning Rate: {LEARNING_RATE}")
    print(f"   Epochs: {NUM_EPOCHS}")
    
    if torch.cuda.is_available():
        print(f"   è®­ç»ƒå‰æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    
    data_collator = RewardDataCollator(tokenizer)
    
    trainer = RewardTrainer(
        model=model,
        args=training_args,
        train_dataset=processed_dataset,
        data_collator=data_collator,
    )
    
    trainer.train()
    
    # ä¿å­˜
    print(f"\nğŸ’¾ ä¿å­˜å¥–åŠ±æ¨¡å‹åˆ° {OUTPUT_DIR}...")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)

    # å•ç‹¬ä¿å­˜ reward head æƒé‡ï¼Œä¾¿äºåœ¨ PPO é˜¶æ®µåŠ è½½
    try:
        torch.save(model.reward_head.state_dict(), os.path.join(OUTPUT_DIR, "reward_head.pt"))
    except Exception as e:
        print(f"   âš ï¸ ä¿å­˜ reward_head.pt å¤±è´¥: {e}")
    
    # ä¿å­˜é…ç½®
    config = {
        "model_type": "reward_model",
        "base_model": BASE_MODEL_PATH,
        "lora_r": LORA_R,
        "lora_alpha": LORA_ALPHA,
    }
    with open(os.path.join(OUTPUT_DIR, "reward_config.json"), "w") as f:
        json.dump(config, f, indent=2)
    
    if torch.cuda.is_available():
        peak_memory = torch.cuda.max_memory_allocated() / 1024**3
        print(f"\nğŸ“Š æ˜¾å­˜å³°å€¼: {peak_memory:.2f} GB")
    
    print("\nâœ… å¥–åŠ±æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
    
    print("\n" + "=" * 60)
    print("ğŸ’¡ åç»­æ­¥éª¤")
    print("=" * 60)
    print("""
1. ä½¿ç”¨å¥–åŠ±æ¨¡å‹è¿›è¡Œ PPO è®­ç»ƒ:
   python lora_ppo_qwen3_1.7b.py

2. è¯„ä¼°æ¨¡å‹æ€§èƒ½:
   python eval_benchmarks.py --model_path /root/data/hsk-models/qwen3_1.7b_lora_ppo
""")


if __name__ == "__main__":
    main()
