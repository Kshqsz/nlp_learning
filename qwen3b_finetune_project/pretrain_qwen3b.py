# pretrain_qwen3b.py
"""
Qwen2.5-3B ç»§ç»­é¢„è®­ç»ƒï¼ˆå…¨é‡å¾®è°ƒï¼‰

ç¡¬ä»¶è¦æ±‚ï¼šNVIDIA 4090D (24GB)
æ˜¾å­˜ä¼˜åŒ–ï¼š
  - gradient_checkpointing: ç”¨è®¡ç®—æ¢æ˜¾å­˜
  - bf16: åŠç²¾åº¦è®­ç»ƒ
  - å° batch + æ¢¯åº¦ç´¯ç§¯

ç»§ç»­é¢„è®­ç»ƒ vs ä»é›¶é¢„è®­ç»ƒï¼š
  - ä»é›¶é¢„è®­ç»ƒï¼šéšæœºåˆå§‹åŒ–ï¼Œéœ€è¦æ•°ä¸‡äº¿ token
  - ç»§ç»­é¢„è®­ç»ƒï¼šåˆ©ç”¨å·²æœ‰çŸ¥è¯†ï¼Œæ•°ç™¾ä¸‡ token å³å¯å¢å¼ºç‰¹å®šé¢†åŸŸ
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)

# ===== é…ç½® =====
MODEL_NAME = "Qwen/Qwen2.5-3B"
OUTPUT_DIR = "./qwen3b_pretrain"
MAX_LENGTH = 512          # åºåˆ—é•¿åº¦ï¼ˆè¶Šé•¿æ˜¾å­˜è¶Šå¤§ï¼‰
BATCH_SIZE = 1            # 4090D ä¸Šéœ€è¦å¾ˆå°çš„ batch
GRADIENT_ACCUMULATION_STEPS = 16  # æœ‰æ•ˆ batch = 1 * 16 = 16
LEARNING_RATE = 1e-5      # ç»§ç»­é¢„è®­ç»ƒç”¨è¾ƒå°å­¦ä¹ ç‡
NUM_EPOCHS = 1
NUM_SAMPLES = 50000       # è®­ç»ƒæ ·æœ¬æ•°ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
SAVE_STEPS = 500
LOGGING_STEPS = 50

# ===== 1. åŠ è½½æ¨¡å‹å’Œ Tokenizer =====
print("=" * 60)
print("ğŸš€ Qwen2.5-3B ç»§ç»­é¢„è®­ç»ƒ")
print("=" * 60)
print(f"æ¨¡å‹: {MODEL_NAME}")
print(f"åºåˆ—é•¿åº¦: {MAX_LENGTH}")
print(f"Batch Size: {BATCH_SIZE} Ã— {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")

print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)

# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆç”¨è®¡ç®—æ¢æ˜¾å­˜ï¼Œå¿…é¡»å¼€å¯ï¼‰
model.gradient_checkpointing_enable()

# æ‰“å°æ¨¡å‹ä¿¡æ¯
total_params = sum(p.numel() for p in model.parameters())
print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
print(f"   å‚æ•°é‡: {total_params / 1e9:.2f}B")

if torch.cuda.is_available():
    print(f"   å½“å‰æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")


# ===== 2. åŠ è½½æ•°æ®é›† =====
print("\nğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®...")

# ä½¿ç”¨ä¸­æ–‡ç»´åŸºç™¾ç§‘æ•°æ®ï¼ˆé€‚åˆç»§ç»­é¢„è®­ç»ƒï¼‰
# ä¹Ÿå¯ä»¥æ›¿æ¢æˆä½ è‡ªå·±çš„é¢†åŸŸæ•°æ®
try:
    # å°è¯•åŠ è½½ä¸­æ–‡ç»´åŸºæ•°æ®
    raw_dataset = load_dataset(
        "pleisto/wikipedia-cn-20230720-filtered",
        split=f"train[:{NUM_SAMPLES}]"
    )
    text_column = "completion"
except:
    # å¤‡é€‰ï¼šä½¿ç”¨ firefly æ•°æ®
    print("âš ï¸ ç»´åŸºæ•°æ®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰æ•°æ®é›†...")
    raw_dataset = load_dataset(
        "YeungNLP/firefly-train-1.1M",
        split=f"train[:{NUM_SAMPLES}]"
    )
    text_column = None  # éœ€è¦æ‹¼æ¥ input + target

print(f"âœ… åŠ è½½ {len(raw_dataset)} æ¡æ•°æ®")


# ===== 3. æ•°æ®é¢„å¤„ç† =====
def tokenize_function(examples):
    """
    é¢„è®­ç»ƒæ•°æ®å¤„ç†ï¼š
    - çº¯æ–‡æœ¬ï¼Œæ— å¯¹è¯æ ¼å¼
    - ç›´æ¥é¢„æµ‹ä¸‹ä¸€ä¸ª token
    """
    if text_column and text_column in examples:
        # ç»´åŸºç™¾ç§‘æ•°æ®ï¼šç›´æ¥ä½¿ç”¨æ–‡æœ¬
        texts = examples[text_column]
    else:
        # Firefly æ•°æ®ï¼šæ‹¼æ¥ input + target
        texts = [
            f"{inp}\n{tgt}" 
            for inp, tgt in zip(examples["input"], examples["target"])
        ]
    
    # Tokenize
    tokenized = tokenizer(
        texts,
        truncation=True,
        max_length=MAX_LENGTH,
        padding=False,
        return_special_tokens_mask=True,
    )
    
    return tokenized


print("\nğŸ”„ å¤„ç†æ•°æ®...")
tokenized_dataset = raw_dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=raw_dataset.column_names,
    desc="Tokenizing",
    num_proc=4,  # å¤šè¿›ç¨‹åŠ é€Ÿ
)

# è¿‡æ»¤å¤ªçŸ­çš„æ ·æœ¬
tokenized_dataset = tokenized_dataset.filter(
    lambda x: len(x["input_ids"]) >= 64,
    desc="Filtering short samples"
)

print(f"âœ… å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ ·æœ¬: {len(tokenized_dataset)}")


# ===== 4. æ•°æ®æ•´ç†å™¨ =====
# ä½¿ç”¨ MLM=False çš„ DataCollatorï¼Œå³æ ‡å‡†çš„ CLMï¼ˆCausal LMï¼‰è®­ç»ƒ
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,  # å› æœè¯­è¨€æ¨¡å‹ï¼Œä¸æ˜¯ BERT çš„ MLM
)


# ===== 5. è®­ç»ƒé…ç½® =====
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    overwrite_output_dir=True,
    
    # è®­ç»ƒå‚æ•°
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    
    # å­¦ä¹ ç‡
    learning_rate=LEARNING_RATE,
    lr_scheduler_type="cosine",
    warmup_ratio=0.05,
    
    # æ˜¾å­˜ä¼˜åŒ–ï¼ˆå…³é”®ï¼ï¼‰
    bf16=True,
    gradient_checkpointing=True,
    optim="adamw_torch_fused",  # èåˆä¼˜åŒ–å™¨ï¼Œæ›´å¿«æ›´çœæ˜¾å­˜
    
    # æ—¥å¿—å’Œä¿å­˜
    logging_steps=LOGGING_STEPS,
    save_steps=SAVE_STEPS,
    save_total_limit=2,
    
    # å…¶ä»–
    report_to="none",
    dataloader_num_workers=4,
    remove_unused_columns=True,
)


# ===== 6. å¼€å§‹è®­ç»ƒ =====
print("\n" + "=" * 60)
print("ğŸ‹ï¸ å¼€å§‹ç»§ç»­é¢„è®­ç»ƒ")
print("=" * 60)
print(f"   è®­ç»ƒæ ·æœ¬: {len(tokenized_dataset)}")
print(f"   Batch Size: {BATCH_SIZE}")
print(f"   æ¢¯åº¦ç´¯ç§¯: {GRADIENT_ACCUMULATION_STEPS}")
print(f"   æœ‰æ•ˆ Batch: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")
print(f"   å­¦ä¹ ç‡: {LEARNING_RATE}")
print(f"   Epochs: {NUM_EPOCHS}")

if torch.cuda.is_available():
    print(f"   è®­ç»ƒå‰æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
)

# è®­ç»ƒ
trainer.train()


# ===== 7. ä¿å­˜æ¨¡å‹ =====
print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹åˆ° {OUTPUT_DIR}...")
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

# æ˜¾å­˜ç»Ÿè®¡
if torch.cuda.is_available():
    peak_memory = torch.cuda.max_memory_allocated() / 1024**3
    print(f"\nğŸ“Š æ˜¾å­˜å³°å€¼: {peak_memory:.2f} GB")

print("\nâœ… ç»§ç»­é¢„è®­ç»ƒå®Œæˆï¼")
print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")


# ===== 8. ç®€å•æµ‹è¯• =====
print("\n" + "=" * 60)
print("ğŸ§ª æµ‹è¯•ç”Ÿæˆæ•ˆæœ")
print("=" * 60)

# é‡æ–°åŠ è½½æ¨¡å‹æµ‹è¯•
del model
torch.cuda.empty_cache()

model = AutoModelForCausalLM.from_pretrained(
    OUTPUT_DIR,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

test_prompts = [
    "äººå·¥æ™ºèƒ½çš„å‘å±•",
    "ä¸­å›½çš„é¦–éƒ½åŒ—äº¬",
    "æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§",
]

for prompt in test_prompts:
    inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.pad_token_id,
        )
    
    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"\nè¾“å…¥: {prompt}")
    print(f"ç”Ÿæˆ: {generated}")
