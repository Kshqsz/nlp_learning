# pretrain_qwen3b_deepspeed.py
"""
Qwen2.5-3B ç»§ç»­é¢„è®­ç»ƒ - DeepSpeed ZeRO ä¼˜åŒ–ç‰ˆ

å¦‚æœæ™®é€šç‰ˆæœ¬ OOMï¼Œä½¿ç”¨è¿™ä¸ªè„šæœ¬ + DeepSpeed ZeRO-2 å¯ä»¥è¿›ä¸€æ­¥é™ä½æ˜¾å­˜

è¿è¡Œæ–¹å¼ï¼š
    accelerate launch --config_file ds_config.yaml pretrain_qwen3b_deepspeed.py
    
æˆ–è€…ï¼š
    deepspeed --num_gpus=1 pretrain_qwen3b_deepspeed.py
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)

# ===== é…ç½® =====
MODEL_NAME = "Qwen/Qwen2.5-3B"
OUTPUT_DIR = "./qwen3b_pretrain_ds"
MAX_LENGTH = 512
BATCH_SIZE = 2            # DeepSpeed å¯ä»¥ç¨å¤§ä¸€ç‚¹
GRADIENT_ACCUMULATION_STEPS = 8
LEARNING_RATE = 1e-5
NUM_EPOCHS = 1
NUM_SAMPLES = 50000

# ===== åŠ è½½æ¨¡å‹ =====
print("=" * 60)
print("ğŸš€ Qwen2.5-3B ç»§ç»­é¢„è®­ç»ƒ (DeepSpeed)")
print("=" * 60)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# æ³¨æ„ï¼šä½¿ç”¨ DeepSpeed æ—¶ä¸è¦ç”¨ device_map="auto"
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    # device_map="auto",  # DeepSpeed ä¼šè‡ªå·±å¤„ç†
)

model.gradient_checkpointing_enable()

total_params = sum(p.numel() for p in model.parameters())
print(f"å‚æ•°é‡: {total_params / 1e9:.2f}B")


# ===== åŠ è½½æ•°æ® =====
print("\nğŸ“Š åŠ è½½æ•°æ®...")

try:
    raw_dataset = load_dataset(
        "pleisto/wikipedia-cn-20230720-filtered",
        split=f"train[:{NUM_SAMPLES}]"
    )
    text_column = "completion"
except:
    raw_dataset = load_dataset(
        "YeungNLP/firefly-train-1.1M",
        split=f"train[:{NUM_SAMPLES}]"
    )
    text_column = None


def tokenize_function(examples):
    if text_column and text_column in examples:
        texts = examples[text_column]
    else:
        texts = [f"{inp}\n{tgt}" for inp, tgt in zip(examples["input"], examples["target"])]
    
    return tokenizer(
        texts,
        truncation=True,
        max_length=MAX_LENGTH,
        padding=False,
    )


tokenized_dataset = raw_dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=raw_dataset.column_names,
    num_proc=4,
)

tokenized_dataset = tokenized_dataset.filter(lambda x: len(x["input_ids"]) >= 64)
print(f"âœ… æœ‰æ•ˆæ ·æœ¬: {len(tokenized_dataset)}")


# ===== è®­ç»ƒé…ç½® (DeepSpeed) =====
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    overwrite_output_dir=True,
    
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    
    learning_rate=LEARNING_RATE,
    lr_scheduler_type="cosine",
    warmup_ratio=0.05,
    
    bf16=True,
    gradient_checkpointing=True,
    
    # DeepSpeed é…ç½®
    deepspeed={
        "zero_optimization": {
            "stage": 2,  # ZeRO-2ï¼šåˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦
            "offload_optimizer": {
                "device": "cpu",  # ä¼˜åŒ–å™¨çŠ¶æ€å¸è½½åˆ° CPU
                "pin_memory": True
            },
            "allgather_partitions": True,
            "allgather_bucket_size": 2e8,
            "reduce_scatter": True,
            "reduce_bucket_size": 2e8,
            "overlap_comm": True,
            "contiguous_gradients": True,
        },
        "bf16": {
            "enabled": True
        },
        "gradient_accumulation_steps": GRADIENT_ACCUMULATION_STEPS,
        "gradient_clipping": 1.0,
        "train_batch_size": "auto",
        "train_micro_batch_size_per_gpu": "auto",
    },
    
    logging_steps=50,
    save_steps=500,
    save_total_limit=2,
    report_to="none",
)


# ===== è®­ç»ƒ =====
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
)

print("\nğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
trainer.train()

print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹åˆ° {OUTPUT_DIR}...")
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print("âœ… å®Œæˆï¼")
