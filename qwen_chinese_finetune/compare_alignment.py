# compare_alignment.py
"""
å¯¹æ¯”å®éªŒï¼šSFT vs DPO vs PPO

æœ¬è„šæœ¬å¯¹æ¯”ä¸‰ç§å¯¹é½æ–¹æ³•çš„æ•ˆæœï¼š
1. SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰ï¼šåªå­¦ä¹ å¦‚ä½•å›ç­”ï¼Œæ²¡æœ‰åå¥½å¯¹é½
2. DPOï¼ˆç›´æ¥åå¥½ä¼˜åŒ–ï¼‰ï¼šç›´æ¥ä»åå¥½æ•°æ®å­¦ä¹ 
3. PPOï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰ï¼šé€šè¿‡å¥–åŠ±æ¨¡å‹åé¦ˆå­¦ä¹ 

å¯¹æ¯”ç»´åº¦ï¼š
- å›ç­”è´¨é‡ï¼ˆä¸»è§‚è¯„ä¼°ï¼‰
- å›ç­”é•¿åº¦åˆ†å¸ƒ
- è¯æ±‡å¤šæ ·æ€§
- å¥–åŠ±æ¨¡å‹åˆ†æ•°
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig
import numpy as np
from collections import Counter
import torch.nn as nn

# ===== é…ç½® =====
SFT_MODEL_PATH = "./qwen_sft"
DPO_MODEL_PATH = "./qwen_dpo"
PPO_MODEL_PATH = "./qwen_ppo"
REWARD_MODEL_PATH = "./qwen_reward_model"

# æµ‹è¯•é—®é¢˜
TEST_PROMPTS = [
    "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
    "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
    "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
    "æ¨èä¸€äº›å­¦ä¹ Pythonçš„æ–¹æ³•",
    "è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„åŒºåˆ«",
    "å¦‚ä½•ä¿æŒå¥åº·çš„ç”Ÿæ´»æ–¹å¼ï¼Ÿ",
    "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
    "å¦‚ä½•æé«˜è‹±è¯­å£è¯­ï¼Ÿ",
]

# ç”Ÿæˆå‚æ•°
GENERATION_CONFIG = {
    "max_new_tokens": 256,
    "do_sample": True,
    "temperature": 0.7,
    "top_p": 0.9,
}


# ===== 1. åŠ è½½æ¨¡å‹ =====
def load_model(model_path: str, model_name: str):
    """åŠ è½½æ¨¡å‹"""
    if not os.path.exists(model_path):
        print(f"âš ï¸ {model_name} ä¸å­˜åœ¨: {model_path}")
        return None, None
    
    print(f"åŠ è½½ {model_name} from {model_path}...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    model.eval()
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    return model, tokenizer


# ===== 2. åŠ è½½å¥–åŠ±æ¨¡å‹ =====
class QwenRewardModel(nn.Module):
    def __init__(self, base_model, hidden_size):
        super().__init__()
        self.model = base_model
        self.reward_head = nn.Linear(hidden_size, 1, bias=False)
        # ç¡®ä¿ reward_head ä¸ base_model åœ¨åŒä¸€è®¾å¤‡å’Œ dtype
        device = next(base_model.parameters()).device
        dtype = next(base_model.parameters()).dtype
        self.reward_head.to(device=device, dtype=dtype)
        
    def forward(self, input_ids, attention_mask=None):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        
        if attention_mask is not None:
            sequence_lengths = attention_mask.sum(dim=1) - 1
            batch_size = input_ids.shape[0]
            last_hidden_state = outputs.hidden_states[-1]
            pooled_output = last_hidden_state[
                torch.arange(batch_size, device=input_ids.device),
                sequence_lengths
            ]
        else:
            pooled_output = outputs.hidden_states[-1][:, -1, :]
        
        rewards = self.reward_head(pooled_output).squeeze(-1)
        return rewards


def load_reward_model():
    """åŠ è½½å¥–åŠ±æ¨¡å‹"""
    if not os.path.exists(REWARD_MODEL_PATH):
        print("âš ï¸ å¥–åŠ±æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°†è·³è¿‡å¥–åŠ±åˆ†æ•°è®¡ç®—")
        return None, None
    
    print(f"åŠ è½½å¥–åŠ±æ¨¡å‹ from {REWARD_MODEL_PATH}...")
    config = AutoConfig.from_pretrained(REWARD_MODEL_PATH, trust_remote_code=True)
    base_model = AutoModel.from_pretrained(
        REWARD_MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    reward_model = QwenRewardModel(base_model, config.hidden_size)
    reward_model.eval()
    
    tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_PATH, trust_remote_code=True)
    
    return reward_model, tokenizer


# ===== 3. ç”Ÿæˆå›ç­” =====
def generate_response(model, tokenizer, prompt: str) -> str:
    """ç”Ÿæˆå›ç­”"""
    full_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            **GENERATION_CONFIG
        )
    
    response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    if "<|im_end|>" in response:
        response = response.split("<|im_end|>")[0]
    
    return response.strip()


# ===== 4. è®¡ç®—å¥–åŠ±åˆ†æ•° =====
def compute_reward(reward_model, tokenizer, prompt: str, response: str) -> float:
    """ä½¿ç”¨å¥–åŠ±æ¨¡å‹è®¡ç®—åˆ†æ•°"""
    if reward_model is None:
        return 0.0
    
    full_text = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n{response}<|im_end|>"
    tokens = tokenizer(full_text, return_tensors="pt", max_length=512, truncation=True)
    tokens = {k: v.to(next(reward_model.parameters()).device) for k, v in tokens.items()}
    
    with torch.no_grad():
        reward = reward_model(**tokens).item()
    
    return reward


# ===== 5. è®¡ç®—æ–‡æœ¬æŒ‡æ ‡ =====
def compute_text_metrics(text: str) -> dict:
    """è®¡ç®—æ–‡æœ¬è´¨é‡æŒ‡æ ‡"""
    # é•¿åº¦
    length = len(text)
    
    # è¯æ±‡å¤šæ ·æ€§ï¼ˆunique chars / total charsï¼‰
    if length > 0:
        diversity = len(set(text)) / length
    else:
        diversity = 0
    
    # é‡å¤ç‡ï¼ˆæ£€æµ‹é‡å¤çš„ n-gramï¼‰
    words = list(text)
    if len(words) >= 3:
        trigrams = [tuple(words[i:i+3]) for i in range(len(words)-2)]
        unique_trigrams = len(set(trigrams))
        total_trigrams = len(trigrams)
        repetition = 1 - (unique_trigrams / total_trigrams) if total_trigrams > 0 else 0
    else:
        repetition = 0
    
    return {
        "length": length,
        "diversity": diversity,
        "repetition": repetition
    }


# ===== 6. å¯¹æ¯”å®éªŒ =====
def run_comparison():
    """è¿è¡Œå¯¹æ¯”å®éªŒ"""
    print("=" * 80)
    print("ğŸ”¬ SFT vs DPO vs PPO å¯¹æ¯”å®éªŒ")
    print("=" * 80)
    
    # åŠ è½½æ‰€æœ‰æ¨¡å‹
    models = {}
    
    sft_model, sft_tokenizer = load_model(SFT_MODEL_PATH, "SFT")
    if sft_model:
        models["SFT"] = (sft_model, sft_tokenizer)
    
    dpo_model, dpo_tokenizer = load_model(DPO_MODEL_PATH, "DPO")
    if dpo_model:
        models["DPO"] = (dpo_model, dpo_tokenizer)
    
    ppo_model, ppo_tokenizer = load_model(PPO_MODEL_PATH, "PPO")
    if ppo_model:
        models["PPO"] = (ppo_model, ppo_tokenizer)
    
    if not models:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return
    
    # åŠ è½½å¥–åŠ±æ¨¡å‹
    reward_model, rm_tokenizer = load_reward_model()
    
    # å­˜å‚¨ç»“æœ
    results = {name: {"responses": [], "rewards": [], "metrics": []} for name in models}
    
    # å¯¹æ¯ä¸ªé—®é¢˜ç”Ÿæˆå›ç­”
    print("\n" + "=" * 80)
    print("ğŸ“ ç”Ÿæˆå›ç­”å¹¶è¯„ä¼°")
    print("=" * 80)
    
    for i, prompt in enumerate(TEST_PROMPTS):
        print(f"\n{'â”€' * 80}")
        print(f"é—®é¢˜ {i+1}: {prompt}")
        print("â”€" * 80)
        
        for name, (model, tokenizer) in models.items():
            # ç”Ÿæˆå›ç­”
            response = generate_response(model, tokenizer, prompt)
            results[name]["responses"].append(response)
            
            # è®¡ç®—å¥–åŠ±
            if reward_model:
                reward = compute_reward(reward_model, rm_tokenizer, prompt, response)
            else:
                reward = 0.0
            results[name]["rewards"].append(reward)
            
            # è®¡ç®—æ–‡æœ¬æŒ‡æ ‡
            metrics = compute_text_metrics(response)
            results[name]["metrics"].append(metrics)
            
            # æ‰“å°å›ç­”
            print(f"\nğŸ”µ {name} æ¨¡å‹:")
            print(f"   å›ç­”: {response[:200]}{'...' if len(response) > 200 else ''}")
            print(f"   é•¿åº¦: {metrics['length']} | å¤šæ ·æ€§: {metrics['diversity']:.3f} | å¥–åŠ±: {reward:.4f}")
    
    # æ±‡æ€»ç»Ÿè®¡
    print("\n" + "=" * 80)
    print("ğŸ“Š æ±‡æ€»ç»Ÿè®¡")
    print("=" * 80)
    
    print("\n### å„æ¨¡å‹å¹³å‡æŒ‡æ ‡")
    print(f"{'æ¨¡å‹':<10} {'å¹³å‡é•¿åº¦':<12} {'å¹³å‡å¤šæ ·æ€§':<12} {'å¹³å‡é‡å¤ç‡':<12} {'å¹³å‡å¥–åŠ±':<12}")
    print("-" * 60)
    
    for name in models:
        avg_length = np.mean([m["length"] for m in results[name]["metrics"]])
        avg_diversity = np.mean([m["diversity"] for m in results[name]["metrics"]])
        avg_repetition = np.mean([m["repetition"] for m in results[name]["metrics"]])
        avg_reward = np.mean(results[name]["rewards"])
        
        print(f"{name:<10} {avg_length:<12.1f} {avg_diversity:<12.3f} {avg_repetition:<12.3f} {avg_reward:<12.4f}")
    
    # æ‰“å°ç»“è®º
    print("\n" + "=" * 80)
    print("ğŸ’¡ ç»“è®ºåˆ†æ")
    print("=" * 80)
    
    print("""
    SFT vs DPO vs PPO å¯¹æ¯”ï¼š
    
    1. SFT (ç›‘ç£å¾®è°ƒ):
       - åªå­¦ä¹ ã€Œå¦‚ä½•å›ç­”ã€ï¼Œæ²¡æœ‰å­¦ä¹ ã€Œä»€ä¹ˆæ˜¯å¥½å›ç­”ã€
       - å¯èƒ½ç”Ÿæˆæµç•…ä½†ä¸å¤Ÿæœ‰å¸®åŠ©çš„å›ç­”
       - æ˜¯ DPO/PPO çš„åŸºç¡€
    
    2. DPO (ç›´æ¥åå¥½ä¼˜åŒ–):
       - ç›´æ¥ä»åå¥½æ•°æ®å­¦ä¹ 
       - æ— éœ€è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œæ›´ç®€å•ç¨³å®š
       - å¯èƒ½è¿‡åº¦ä¼˜åŒ–æŸäº›è¡¨é¢ç‰¹å¾
    
    3. PPO (å¼ºåŒ–å­¦ä¹ ):
       - é€šè¿‡å¥–åŠ±æ¨¡å‹è·å¾—åé¦ˆ
       - å¯ä»¥åœ¨çº¿æ¢ç´¢å’Œæ”¹è¿›
       - è®­ç»ƒæ›´å¤æ‚ï¼Œéœ€è¦è°ƒå‚
       - å¯èƒ½å‡ºç°å¥–åŠ± hacking
    
    ç†è®ºä¸Šï¼šPPO > DPO > SFTï¼ˆåœ¨äººç±»åå¥½ä¸Šï¼‰
    å®é™…ä¸Šï¼šDPO é€šå¸¸ä¸ PPO æ•ˆæœç›¸å½“ï¼Œä½†æ›´ç®€å•ç¨³å®š
    """)


# ===== 7. å•ç‹¬å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹ =====
def compare_two_models(model1_path: str, model2_path: str, name1: str, name2: str):
    """è¯¦ç»†å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹"""
    print(f"\n{'=' * 80}")
    print(f"ğŸ”¬ {name1} vs {name2} è¯¦ç»†å¯¹æ¯”")
    print("=" * 80)
    
    model1, tokenizer1 = load_model(model1_path, name1)
    model2, tokenizer2 = load_model(model2_path, name2)
    
    if not model1 or not model2:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
        return
    
    reward_model, rm_tokenizer = load_reward_model()
    
    for prompt in TEST_PROMPTS[:3]:  # åªæµ‹è¯•å‰3ä¸ª
        print(f"\n{'â”€' * 80}")
        print(f"é—®é¢˜: {prompt}")
        
        # Model 1
        response1 = generate_response(model1, tokenizer1, prompt)
        reward1 = compute_reward(reward_model, rm_tokenizer, prompt, response1) if reward_model else 0
        
        # Model 2
        response2 = generate_response(model2, tokenizer2, prompt)
        reward2 = compute_reward(reward_model, rm_tokenizer, prompt, response2) if reward_model else 0
        
        print(f"\nğŸ”µ {name1}: (å¥–åŠ±: {reward1:.4f})")
        print(f"   {response1[:300]}")
        
        print(f"\nğŸŸ¢ {name2}: (å¥–åŠ±: {reward2:.4f})")
        print(f"   {response2[:300]}")
        
        # åˆ¤æ–­å“ªä¸ªæ›´å¥½
        if reward_model:
            winner = name1 if reward1 > reward2 else name2
            print(f"\n   ğŸ‘‘ å¥–åŠ±æ¨¡å‹åˆ¤æ–­: {winner} æ›´å¥½ (å·®å€¼: {abs(reward1-reward2):.4f})")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="å¯¹æ¯” SFT/DPO/PPO æ¨¡å‹")
    parser.add_argument("--mode", choices=["all", "sft-dpo", "sft-ppo", "dpo-ppo"], 
                        default="all", help="å¯¹æ¯”æ¨¡å¼")
    args = parser.parse_args()
    
    if args.mode == "all":
        run_comparison()
    elif args.mode == "sft-dpo":
        compare_two_models(SFT_MODEL_PATH, DPO_MODEL_PATH, "SFT", "DPO")
    elif args.mode == "sft-ppo":
        compare_two_models(SFT_MODEL_PATH, PPO_MODEL_PATH, "SFT", "PPO")
    elif args.mode == "dpo-ppo":
        compare_two_models(DPO_MODEL_PATH, PPO_MODEL_PATH, "DPO", "PPO")
