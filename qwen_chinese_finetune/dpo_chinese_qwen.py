# dpo_chinese_qwen.py
"""
DPO (Direct Preference Optimization) è®­ç»ƒè„šæœ¬

ä½¿ç”¨çœŸæ­£çš„äººç±»åå¥½æ•°æ®é›†ï¼šshibing624/DPO-En-Zh-20k-Preference
è¯¥æ•°æ®é›†åŒ…å«ï¼š
- 10k ä¸­æ–‡åå¥½å¯¹ï¼ˆæ¥è‡ª wenbopan/Chinese-dpo-pairsï¼‰
- 10k è‹±æ–‡åå¥½å¯¹ï¼ˆæ¥è‡ª argilla é«˜è´¨é‡æ•°æ®ï¼‰

æ¯æ¡æ•°æ®æ ¼å¼ï¼š
- system: ç³»ç»Ÿæç¤º
- history: å¤šè½®å¯¹è¯å†å² [[user1, assistant1], [user2, assistant2], ...]
- question: å½“å‰é—®é¢˜
- response_chosen: è¢«äººç±»é€‰ä¸­çš„å¥½å›ç­”
- response_rejected: è¢«äººç±»æ‹’ç»çš„å·®å›ç­”

è¿™æ˜¯çœŸæ­£çš„åå¥½æ•°æ®ï¼Œä¸æ˜¯ä¼ªé€ çš„ï¼
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments
)
from trl import DPOTrainer, DPOConfig
import torch

# ===== é…ç½® =====
SFT_MODEL_PATH = "./qwen_sft"          # â† ä½ çš„ SFT æ¨¡å‹è·¯å¾„
OUTPUT_DIR = "./qwen_dpo"
MAX_LENGTH = 512
MAX_PROMPT_LENGTH = 256                # prompt ä¸èƒ½å¤ªé•¿ï¼Œç•™ç©ºé—´ç»™å›ç­”
BATCH_SIZE = 2                         # DPO æ˜¾å­˜å ç”¨é«˜ï¼Œå»ºè®® 1~2
GRADIENT_ACCUMULATION_STEPS = 4        # æ¨¡æ‹Ÿæ›´å¤§ batch
LEARNING_RATE = 5e-6                   # DPO å­¦ä¹ ç‡é€šå¸¸æ¯” SFT å°
BETA = 0.1                             # DPO çš„ beta å‚æ•°ï¼Œæ§åˆ¶åç¦»å‚è€ƒæ¨¡å‹çš„ç¨‹åº¦
NUM_SAMPLES = 5000                     # ä½¿ç”¨å¤šå°‘æ ·æœ¬è®­ç»ƒï¼ˆä¸­æ–‡å…±10kï¼‰

# ===== 1. åŠ è½½ tokenizer å’Œ SFT æ¨¡å‹ =====
print(f"Loading SFT model from: {SFT_MODEL_PATH}")
tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL_PATH, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    SFT_MODEL_PATH,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# åŠ è½½å‚è€ƒæ¨¡å‹ï¼ˆDPO éœ€è¦ä¸€ä¸ªå†»ç»“çš„å‚è€ƒæ¨¡å‹ï¼‰
print("Loading reference model...")
ref_model = AutoModelForCausalLM.from_pretrained(
    SFT_MODEL_PATH,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# ä¿®å¤ pad tokenï¼ˆå¿…é¡»ï¼ï¼‰
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = model.config.eos_token_id

# ===== 2. åŠ è½½çœŸæ­£çš„ä¸­æ–‡åå¥½æ•°æ®é›† =====
print("Loading Chinese preference dataset: shibing624/DPO-En-Zh-20k-Preference")

# åŠ è½½ä¸­æ–‡å­é›†ï¼ˆzhï¼‰ï¼Œå…±10kæ ·æœ¬
raw_dataset = load_dataset(
    "shibing624/DPO-En-Zh-20k-Preference",
    name="zh",                          # ä¸­æ–‡å­é›†
    split=f"train[:{NUM_SAMPLES}]"      # å–å‰Næ¡
)

def format_preference_data(examples):
    """
    å°†åŸå§‹åå¥½æ•°æ®è½¬æ¢ä¸º DPO è®­ç»ƒæ ¼å¼
    
    åŸå§‹æ ¼å¼ï¼š
    - system: ç³»ç»Ÿæç¤ºï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
    - history: å¤šè½®å¯¹è¯å†å² [[user, assistant], ...]
    - question: å½“å‰é—®é¢˜
    - response_chosen: äººç±»é€‰æ‹©çš„å¥½å›ç­”
    - response_rejected: äººç±»æ‹’ç»çš„å·®å›ç­”
    
    ç›®æ ‡æ ¼å¼ï¼ˆDPO Trainer éœ€è¦ï¼‰ï¼š
    - prompt: å®Œæ•´çš„ç”¨æˆ·è¾“å…¥ï¼ˆåŒ…å«å†å²å¯¹è¯ï¼‰
    - chosen: å¥½å›ç­”
    - rejected: å·®å›ç­”
    """
    prompts = []
    chosens = []
    rejecteds = []
    
    for system, history, question, chosen, rejected in zip(
        examples["system"],
        examples["history"],
        examples["question"],
        examples["response_chosen"],
        examples["response_rejected"]
    ):
        # æ„å»º promptï¼ˆä½¿ç”¨ Qwen ChatML æ ¼å¼ï¼‰
        prompt_parts = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤ºï¼ˆå¦‚æœæœ‰ï¼‰
        if system and system.strip():
            prompt_parts.append(f"<|im_start|>system\n{system}<|im_end|>")
        
        # æ·»åŠ å†å²å¯¹è¯
        if history:
            for turn in history:
                if len(turn) >= 2:
                    user_msg, assistant_msg = turn[0], turn[1]
                    prompt_parts.append(f"<|im_start|>user\n{user_msg}<|im_end|>")
                    prompt_parts.append(f"<|im_start|>assistant\n{assistant_msg}<|im_end|>")
        
        # æ·»åŠ å½“å‰é—®é¢˜
        prompt_parts.append(f"<|im_start|>user\n{question}<|im_end|>")
        prompt_parts.append("<|im_start|>assistant\n")
        
        prompt = "\n".join(prompt_parts)
        
        # å›ç­”éƒ¨åˆ†ï¼ˆåŠ ä¸Šç»“æŸæ ‡è®°ï¼‰
        chosen_response = chosen + "<|im_end|>"
        rejected_response = rejected + "<|im_end|>"
        
        prompts.append(prompt)
        chosens.append(chosen_response)
        rejecteds.append(rejected_response)
    
    return {
        "prompt": prompts,
        "chosen": chosens,
        "rejected": rejecteds
    }

print("Formatting preference data...")
dataset = raw_dataset.map(
    format_preference_data,
    batched=True,
    remove_columns=raw_dataset.column_names,
    desc="Formatting preference pairs"
)

print(f"âœ… Loaded {len(dataset)} real human preference pairs!")

# å±•ç¤ºä¸€ä¸ªæ ·æœ¬
print("\n===== æ ·æœ¬å±•ç¤º =====")
print(f"Prompt:\n{dataset[0]['prompt'][:300]}...")
print(f"\nChosen (å¥½å›ç­”):\n{dataset[0]['chosen'][:200]}...")
print(f"\nRejected (å·®å›ç­”):\n{dataset[0]['rejected'][:200]}...")

# ===== 3. DPO è®­ç»ƒé…ç½® =====
dpo_config = DPOConfig(
    output_dir=OUTPUT_DIR,
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    learning_rate=LEARNING_RATE,
    beta=BETA,                          # DPO æ ¸å¿ƒå‚æ•°
    max_length=MAX_LENGTH,
    max_prompt_length=MAX_PROMPT_LENGTH,
    logging_steps=10,
    save_steps=200,
    save_total_limit=2,
    bf16=True,
    report_to="none",
    optim="adamw_torch",
    remove_unused_columns=False,
    gradient_checkpointing=True,        # èŠ‚çœæ˜¾å­˜
)

# ===== 4. åˆ›å»º DPO Trainer =====
print("Creating DPO Trainer...")
dpo_trainer = DPOTrainer(
    model=model,
    ref_model=ref_model,                # å‚è€ƒæ¨¡å‹ï¼ˆå†»ç»“ï¼‰
    args=dpo_config,
    train_dataset=dataset,
    processing_class=tokenizer,
)

# ===== 5. å¼€å§‹ DPO è®­ç»ƒ =====
print("ğŸš€ Starting DPO training...")
print(f"   - Beta: {BETA}")
print(f"   - Learning Rate: {LEARNING_RATE}")
print(f"   - Effective Batch Size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")
dpo_trainer.train()

# ===== 6. ä¿å­˜æ¨¡å‹ =====
dpo_trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print(f"âœ… DPO æ¨¡å‹å·²ä¿å­˜åˆ° {OUTPUT_DIR}")

# ===== 7. æµ‹è¯•æ¨ç† =====
print("\n===== æµ‹è¯• DPO æ¨¡å‹ =====")
model.eval()

test_prompt = "<|im_start|>user\nè¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½<|im_end|>\n<|im_start|>assistant\n"
inputs = tokenizer(test_prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.pad_token_id,
    )

response = tokenizer.decode(outputs[0], skip_special_tokens=False)
print(f"ç”Ÿæˆç»“æœ:\n{response}")