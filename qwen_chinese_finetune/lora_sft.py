# lora_sft.py
"""
LoRA (Low-Rank Adaptation) é«˜æ•ˆå¾®è°ƒ

LoRA æ˜¯ä»€ä¹ˆï¼š
- å†»ç»“é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼Œåªè®­ç»ƒä½ç§©åˆ†è§£çŸ©é˜µ
- å¤§å¹…å‡å°‘å¯è®­ç»ƒå‚æ•°é‡ï¼ˆé€šå¸¸ < 1%ï¼‰
- æ˜¾å­˜å ç”¨æ›´å°ï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«
- æ•ˆæœæ¥è¿‘å…¨é‡å¾®è°ƒ

LoRA åŸç†ï¼š
åŸå§‹æƒé‡: W (d Ã— d)
LoRA:     W' = W + Î”W = W + BA
          å…¶ä¸­ B (d Ã— r), A (r Ã— d), r << d (r é€šå¸¸å– 8, 16, 32)

å‚æ•°é‡å¯¹æ¯”ï¼ˆä»¥ d=1024, r=8 ä¸ºä¾‹ï¼‰ï¼š
- å…¨é‡å¾®è°ƒ: 1024 Ã— 1024 = 1,048,576 å‚æ•°
- LoRA:     1024 Ã— 8 + 8 Ã— 1024 = 16,384 å‚æ•° (èŠ‚çœ 98.4%)

æœ¬è„šæœ¬ä½¿ç”¨ peft åº“å®ç° LoRA å¾®è°ƒ
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    PeftModel,
)
import numpy as np

# ===== é…ç½® =====
# åŸºåº§æ¨¡å‹ï¼ˆå¯ä»¥ç”¨ SFT æ¨¡å‹æˆ–åŸå§‹é¢„è®­ç»ƒæ¨¡å‹ï¼‰
BASE_MODEL_PATH = "./qwen_sft"  # ä½¿ç”¨ SFT æ¨¡å‹ä½œä¸ºåŸºåº§
# BASE_MODEL_PATH = "Qwen/Qwen1.5-0.5B"  # æˆ–ç›´æ¥ä½¿ç”¨åŸå§‹æ¨¡å‹

OUTPUT_DIR = "./qwen_lora_sft"
MAX_LENGTH = 512
BATCH_SIZE = 4
GRADIENT_ACCUMULATION_STEPS = 4  # æœ‰æ•ˆ batch = 16
LEARNING_RATE = 2e-4              # LoRA å¯ä»¥ç”¨æ›´å¤§çš„å­¦ä¹ ç‡
NUM_EPOCHS = 2
NUM_SAMPLES = 5000                # è®­ç»ƒæ ·æœ¬æ•°

# LoRA é…ç½®
LORA_R = 8                        # LoRA ç§©ï¼ˆrankï¼‰
LORA_ALPHA = 16                   # LoRA ç¼©æ”¾ç³»æ•°
LORA_DROPOUT = 0.05               # LoRA dropout
TARGET_MODULES = [                # è¦åº”ç”¨ LoRA çš„æ¨¡å—
    "q_proj",                     # Query æŠ•å½±
    "k_proj",                     # Key æŠ•å½±
    "v_proj",                     # Value æŠ•å½±
    "o_proj",                     # Output æŠ•å½±
    "gate_proj",                  # FFN gate
    "up_proj",                    # FFN up
    "down_proj",                  # FFN down
]


# ===== 1. åŠ è½½æ¨¡å‹å’Œ Tokenizer =====
print("=" * 60)
print("ğŸš€ LoRA é«˜æ•ˆå¾®è°ƒ")
print("=" * 60)

print("\nğŸ“¦ åŠ è½½åŸºåº§æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_PATH,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# æ‰“å°åŸå§‹æ¨¡å‹å‚æ•°é‡
total_params = sum(p.numel() for p in model.parameters())
print(f"åŸºåº§æ¨¡å‹å‚æ•°é‡: {total_params / 1e6:.2f}M")


# ===== 2. é…ç½® LoRA =====
print("\nğŸ”§ é…ç½® LoRA...")

lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,     # å› æœè¯­è¨€æ¨¡å‹ä»»åŠ¡
    r=LORA_R,                          # LoRA ç§©
    lora_alpha=LORA_ALPHA,             # ç¼©æ”¾ç³»æ•°
    lora_dropout=LORA_DROPOUT,         # Dropout
    target_modules=TARGET_MODULES,     # ç›®æ ‡æ¨¡å—
    bias="none",                       # ä¸è®­ç»ƒ bias
)

# åº”ç”¨ LoRA
model = get_peft_model(model, lora_config)

# æ‰“å° LoRA å‚æ•°é‡
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
all_params = sum(p.numel() for p in model.parameters())
print(f"LoRA å¯è®­ç»ƒå‚æ•°: {trainable_params / 1e6:.4f}M")
print(f"æ€»å‚æ•°é‡: {all_params / 1e6:.2f}M")
print(f"å¯è®­ç»ƒæ¯”ä¾‹: {100 * trainable_params / all_params:.2f}%")

# æ‰“å° LoRA é…ç½®
model.print_trainable_parameters()


# ===== 3. åŠ è½½æ•°æ®é›† =====
print("\nğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®...")

raw_dataset = load_dataset(
    "YeungNLP/firefly-train-1.1M",
    split=f"train[:{NUM_SAMPLES}]"
)


def preprocess_function(examples):
    """é¢„å¤„ç†æ•°æ®ï¼šæ„å»ºå¯¹è¯æ ¼å¼"""
    input_ids_list = []
    labels_list = []
    attention_mask_list = []
    
    for kind, inp, target in zip(examples["kind"], examples["input"], examples["target"]):
        # è·³è¿‡è¿‡é•¿çš„æ ·æœ¬
        if len(inp) > 300 or len(target) > 300:
            continue
        
        # æ„å»ºå¯¹è¯æ ¼å¼
        prompt = f"<|im_start|>user\n{inp}<|im_end|>\n<|im_start|>assistant\n"
        full_text = f"{prompt}{target}<|im_end|>"
        
        # Tokenize
        tokenized = tokenizer(
            full_text,
            max_length=MAX_LENGTH,
            truncation=True,
            padding=False,
        )
        
        input_ids = tokenized["input_ids"]
        attention_mask = tokenized["attention_mask"]
        
        # è®¡ç®— prompt é•¿åº¦ï¼Œç”¨äº labels æ©ç 
        prompt_tokens = tokenizer(prompt, add_special_tokens=False)["input_ids"]
        prompt_len = len(prompt_tokens)
        
        # Labels: prompt éƒ¨åˆ†ä¸º -100ï¼ˆä¸è®¡ç®— lossï¼‰
        labels = [-100] * prompt_len + input_ids[prompt_len:]
        
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        if len(labels) < len(input_ids):
            labels = labels + [-100] * (len(input_ids) - len(labels))
        elif len(labels) > len(input_ids):
            labels = labels[:len(input_ids)]
        
        input_ids_list.append(input_ids)
        labels_list.append(labels)
        attention_mask_list.append(attention_mask)
    
    return {
        "input_ids": input_ids_list,
        "labels": labels_list,
        "attention_mask": attention_mask_list,
    }


# å¤„ç†æ•°æ®
dataset = raw_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=raw_dataset.column_names,
    desc="Processing data"
)

# è¿‡æ»¤ç©ºæ ·æœ¬
dataset = dataset.filter(lambda x: len(x["input_ids"]) > 0)

print(f"âœ… å¤„ç†åæ ·æœ¬æ•°: {len(dataset)}")


# ===== 4. è®­ç»ƒé…ç½® =====
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    overwrite_output_dir=True,
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    learning_rate=LEARNING_RATE,
    lr_scheduler_type="cosine",
    warmup_ratio=0.05,
    bf16=True,
    logging_steps=20,
    save_steps=500,
    save_total_limit=2,
    report_to="none",
    gradient_checkpointing=True,  # èŠ‚çœæ˜¾å­˜
    optim="adamw_torch",
)


# ===== 5. è®­ç»ƒ =====
print("\n" + "=" * 60)
print("ğŸ‹ï¸ å¼€å§‹ LoRA è®­ç»ƒ")
print("=" * 60)
print(f"   - è®­ç»ƒæ ·æœ¬: {len(dataset)}")
print(f"   - Batch Size: {BATCH_SIZE}")
print(f"   - æ¢¯åº¦ç´¯ç§¯: {GRADIENT_ACCUMULATION_STEPS}")
print(f"   - æœ‰æ•ˆ Batch: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")
print(f"   - Learning Rate: {LEARNING_RATE}")
print(f"   - LoRA Rank: {LORA_R}")
print(f"   - LoRA Alpha: {LORA_ALPHA}")

data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    padding=True,
    return_tensors="pt"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=data_collator,
)

trainer.train()


# ===== 6. ä¿å­˜ LoRA æƒé‡ =====
print(f"\nğŸ’¾ ä¿å­˜ LoRA æƒé‡åˆ° {OUTPUT_DIR}...")

# åªä¿å­˜ LoRA æƒé‡ï¼ˆå¾ˆå°ï¼Œé€šå¸¸å‡ å MBï¼‰
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print("âœ… LoRA è®­ç»ƒå®Œæˆï¼")
print(f"\nğŸ“ LoRA æƒé‡å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
print("   æ³¨æ„ï¼šè¿™åªæ˜¯ LoRA å¢é‡æƒé‡ï¼Œéœ€è¦é…åˆåŸºåº§æ¨¡å‹ä½¿ç”¨")
print("   å¦‚éœ€åˆå¹¶æƒé‡ï¼Œè¯·è¿è¡Œ merge_lora.py")
