# merge_lora.py
"""
åˆå¹¶ LoRA æƒé‡åˆ°åŸºåº§æ¨¡å‹

ä¸ºä»€ä¹ˆè¦åˆå¹¶ï¼š
1. éƒ¨ç½²æ›´ç®€å•ï¼šåªéœ€è¦ä¸€ä¸ªæ¨¡å‹æ–‡ä»¶
2. æ¨ç†æ›´å¿«ï¼šä¸éœ€è¦é¢å¤–çš„ LoRA å‰å‘è®¡ç®—
3. å…¼å®¹æ€§å¥½ï¼šåˆå¹¶åå°±æ˜¯æ™®é€šçš„ Transformers æ¨¡å‹

åˆå¹¶å…¬å¼ï¼š
W_merged = W_base + Î”W = W_base + (BA * scaling)
å…¶ä¸­ scaling = lora_alpha / lora_r

æœ¬è„šæœ¬å°† LoRA æƒé‡åˆå¹¶åˆ°åŸºåº§æ¨¡å‹ï¼Œè¾“å‡ºå®Œæ•´æ¨¡å‹
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import shutil

# ===== é…ç½® =====
BASE_MODEL_PATH = "Qwen/Qwen1.5-0.5B"    # åŸºåº§æ¨¡å‹
LORA_PATH = "./qwen_lora_sft"            # LoRA æƒé‡
MERGED_OUTPUT_PATH = "./qwen_lora_merged"   # åˆå¹¶åçš„è¾“å‡ºè·¯å¾„


def merge_and_save():
    """åˆå¹¶ LoRA æƒé‡å¹¶ä¿å­˜"""
    print("=" * 60)
    print("ğŸ”€ åˆå¹¶ LoRA æƒé‡åˆ°åŸºåº§æ¨¡å‹")
    print("=" * 60)
    
    # 1. åŠ è½½åŸºåº§æ¨¡å‹
    print("\nğŸ“¦ åŠ è½½åŸºåº§æ¨¡å‹...")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    # 2. åŠ è½½ LoRA æƒé‡
    print("ğŸ”§ åŠ è½½ LoRA æƒé‡...")
    model = PeftModel.from_pretrained(
        base_model,
        LORA_PATH,
        torch_dtype=torch.bfloat16,
    )
    
    # 3. åˆå¹¶æƒé‡
    print("ğŸ”€ åˆå¹¶æƒé‡...")
    # merge_and_unload() ä¼šï¼š
    # - å°† LoRA æƒé‡åˆå¹¶åˆ°åŸºåº§æ¨¡å‹
    # - ç§»é™¤ LoRA å±‚ï¼Œæ¢å¤åŸå§‹ç»“æ„
    merged_model = model.merge_and_unload()
    
    # 4. ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
    print(f"\nğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ° {MERGED_OUTPUT_PATH}...")
    merged_model.save_pretrained(MERGED_OUTPUT_PATH)
    
    # 5. ä¿å­˜ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(LORA_PATH, trust_remote_code=True)
    tokenizer.save_pretrained(MERGED_OUTPUT_PATH)
    
    print("\nâœ… åˆå¹¶å®Œæˆï¼")
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°å¯¹æ¯”
    print("\nğŸ“Š æ–‡ä»¶å¤§å°å¯¹æ¯”:")
    
    # LoRA æƒé‡å¤§å°
    lora_size = get_folder_size(LORA_PATH)
    print(f"   LoRA æƒé‡: {lora_size:.2f} MB")
    
    # åˆå¹¶åæ¨¡å‹å¤§å°
    merged_size = get_folder_size(MERGED_OUTPUT_PATH)
    print(f"   åˆå¹¶åæ¨¡å‹: {merged_size:.2f} MB")
    
    print(f"""
ğŸ¯ è¯´æ˜ï¼š
   - LoRA æƒé‡å¾ˆå°ï¼ˆé€šå¸¸åªæœ‰å‡ å MBï¼‰
   - åˆå¹¶åæ¨¡å‹å¤§å° â‰ˆ åŸºåº§æ¨¡å‹å¤§å°
   
   ä½¿ç”¨åœºæ™¯ï¼š
   - å¼€å‘/å®éªŒé˜¶æ®µï¼šä½¿ç”¨ LoRA æƒé‡ï¼ˆæ–¹ä¾¿åˆ‡æ¢ã€èŠ‚çœå­˜å‚¨ï¼‰
   - éƒ¨ç½²é˜¶æ®µï¼šä½¿ç”¨åˆå¹¶åæ¨¡å‹ï¼ˆæ¨ç†æ›´å¿«ã€æ›´ç®€å•ï¼‰
""")
    
    return merged_model, tokenizer


def get_folder_size(path):
    """è·å–æ–‡ä»¶å¤¹å¤§å°ï¼ˆMBï¼‰"""
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            if os.path.isfile(filepath):
                total_size += os.path.getsize(filepath)
    return total_size / (1024 * 1024)


def test_merged_model(model, tokenizer):
    """æµ‹è¯•åˆå¹¶åçš„æ¨¡å‹"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•åˆå¹¶åçš„æ¨¡å‹")
    print("=" * 60)
    
    test_prompts = [
        "è¯·ä»‹ç»ä¸€ä¸‹åŒ—äº¬",
        "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
    ]
    
    for prompt in test_prompts:
        full_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
        
        inputs = tokenizer(full_prompt, return_tensors="pt")
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        if "assistant" in response:
            response = response.split("assistant")[-1].strip()
        
        print(f"\nã€é—®é¢˜ã€‘{prompt}")
        print(f"ã€å›ç­”ã€‘{response}")


def compare_lora_vs_merged():
    """
    å¯¹æ¯” LoRA æ¨ç†å’Œåˆå¹¶æ¨¡å‹æ¨ç†
    éªŒè¯ä¸¤è€…è¾“å‡ºæ˜¯å¦ä¸€è‡´
    """
    print("\n" + "=" * 60)
    print("ğŸ” éªŒè¯ LoRA å’Œåˆå¹¶æ¨¡å‹è¾“å‡ºä¸€è‡´æ€§")
    print("=" * 60)
    
    # åŠ è½½åˆå¹¶åçš„æ¨¡å‹
    print("\nåŠ è½½åˆå¹¶åæ¨¡å‹...")
    merged_model = AutoModelForCausalLM.from_pretrained(
        MERGED_OUTPUT_PATH,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    # åŠ è½½ LoRA æ¨¡å‹
    print("åŠ è½½ LoRA æ¨¡å‹...")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    lora_model = PeftModel.from_pretrained(
        base_model,
        LORA_PATH,
        torch_dtype=torch.bfloat16,
    )
    
    tokenizer = AutoTokenizer.from_pretrained(MERGED_OUTPUT_PATH, trust_remote_code=True)
    
    # æµ‹è¯•è¾“å…¥
    test_input = "ä½ å¥½"
    full_prompt = f"<|im_start|>user\n{test_input}<|im_end|>\n<|im_start|>assistant\n"
    inputs = tokenizer(full_prompt, return_tensors="pt")
    inputs = {k: v.to(merged_model.device) for k, v in inputs.items()}
    
    # è·å– logitsï¼ˆä¸ä½¿ç”¨é‡‡æ ·ï¼Œç¡®ä¿ç¡®å®šæ€§ï¼‰
    with torch.no_grad():
        merged_logits = merged_model(**inputs).logits
        lora_logits = lora_model(**inputs).logits
    
    # è®¡ç®—å·®å¼‚
    diff = (merged_logits - lora_logits).abs().mean().item()
    print(f"\nå¹³å‡ logits å·®å¼‚: {diff:.6f}")
    
    if diff < 1e-4:
        print("âœ… éªŒè¯é€šè¿‡ï¼šLoRA å’Œåˆå¹¶æ¨¡å‹è¾“å‡ºåŸºæœ¬ä¸€è‡´")
    else:
        print("âš ï¸ å­˜åœ¨å·®å¼‚ï¼Œå¯èƒ½æ˜¯æ•°å€¼ç²¾åº¦é—®é¢˜")


if __name__ == "__main__":
    # åˆå¹¶å¹¶ä¿å­˜
    merged_model, tokenizer = merge_and_save()
    
    # æµ‹è¯•åˆå¹¶åçš„æ¨¡å‹
    test_merged_model(merged_model, tokenizer)
    
    # å¯é€‰ï¼šéªŒè¯ä¸€è‡´æ€§
    # compare_lora_vs_merged()
