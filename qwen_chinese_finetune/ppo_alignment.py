# ppo_alignment.py
"""
RLHF ç¬¬äºŒæ­¥ï¼šPPO (Proximal Policy Optimization) å¯¹é½è®­ç»ƒ

PPO æ˜¯ä»€ä¹ˆï¼š
- ä¸€ç§ç­–ç•¥æ¢¯åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•
- åœ¨ RLHF ä¸­ï¼Œç”¨å¥–åŠ±æ¨¡å‹çš„åˆ†æ•°ä½œä¸ºåé¦ˆä¿¡å·
- ä¼˜åŒ–ç­–ç•¥ï¼ˆLLMï¼‰ç”Ÿæˆé«˜å¥–åŠ±çš„å›ç­”

RLHF å®Œæ•´æµç¨‹ï¼š
1. SFT æ¨¡å‹ï¼ˆå·²å®Œæˆï¼‰â†’ ä¼šéµå¾ªæŒ‡ä»¤
2. Reward Modelï¼ˆä¸Šä¸€æ­¥ï¼‰â†’ å­¦ä¼šæ‰“åˆ†
3. PPO è®­ç»ƒï¼ˆæœ¬è„šæœ¬ï¼‰â†’ ä¼˜åŒ–ç”Ÿæˆé«˜åˆ†å›ç­”

PPO çš„å…³é”®ç»„ä»¶ï¼š
- Actorï¼ˆç­–ç•¥æ¨¡å‹ï¼‰ï¼šç”Ÿæˆå›ç­”ï¼Œå°±æ˜¯è¦è®­ç»ƒçš„ LLM
- Criticï¼ˆä»·å€¼æ¨¡å‹ï¼‰ï¼šé¢„æµ‹çŠ¶æ€ä»·å€¼ï¼Œå¸®åŠ©è®¡ç®—ä¼˜åŠ¿å‡½æ•°
- Reward Modelï¼šç»™ç”Ÿæˆçš„å›ç­”æ‰“åˆ†
- Reference Modelï¼šKL çº¦æŸï¼Œé˜²æ­¢æ¨¡å‹è·‘å

ä½¿ç”¨ trl åº“ç®€åŒ–å®ç°ï¼ˆä¸ DPO ä½¿ç”¨åŒä¸€ä¸ªåº“ï¼‰
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForSequenceClassification,
    AutoModel,
    AutoConfig,
)
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
from trl.core import LengthSampler
import numpy as np
from typing import List
import torch.nn as nn

# ===== é…ç½® =====
SFT_MODEL_PATH = "./qwen_sft"              # SFT æ¨¡å‹ï¼ˆä½œä¸ºåˆå§‹ç­–ç•¥ï¼‰
REWARD_MODEL_PATH = "./qwen_reward_model"  # å¥–åŠ±æ¨¡å‹
OUTPUT_DIR = "./qwen_ppo"
MAX_LENGTH = 512
MAX_NEW_TOKENS = 128                        # ç”Ÿæˆçš„æœ€å¤§æ–° token æ•°
BATCH_SIZE = 4                              # PPO mini-batch size
MINI_BATCH_SIZE = 2                         # PPO å†…éƒ¨ mini-batch
GRADIENT_ACCUMULATION_STEPS = 4
LEARNING_RATE = 1e-5
PPO_EPOCHS = 4                              # æ¯ä¸ª batch çš„ PPO æ›´æ–°è½®æ•°
NUM_TRAIN_STEPS = 500                       # æ€»è®­ç»ƒæ­¥æ•°
KL_PENALTY = 0.1                            # KL æ•£åº¦æƒ©ç½šç³»æ•°
GAMMA = 1.0                                 # æŠ˜æ‰£å› å­
LAM = 0.95                                  # GAE lambda
NUM_SAMPLES = 2000                          # ä½¿ç”¨å¤šå°‘ prompt è®­ç»ƒ


# ===== 1. åŠ è½½æ¨¡å‹ =====
print("=" * 60)
print("ğŸš€ PPO å¯¹é½è®­ç»ƒ")
print("=" * 60)

print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")

# åŠ è½½ tokenizer
tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL_PATH, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# åŠ è½½ç­–ç•¥æ¨¡å‹ï¼ˆå¸¦ Value Headï¼‰
# trl æä¾›çš„ AutoModelForCausalLMWithValueHead ä¼šè‡ªåŠ¨æ·»åŠ ä»·å€¼å¤´
print("åŠ è½½ç­–ç•¥æ¨¡å‹ (Actor + Critic)...")
model = AutoModelForCausalLMWithValueHead.from_pretrained(
    SFT_MODEL_PATH,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# åŠ è½½å‚è€ƒæ¨¡å‹ï¼ˆå†»ç»“ï¼Œç”¨äº KL çº¦æŸï¼‰
print("åŠ è½½å‚è€ƒæ¨¡å‹ (Reference)...")
ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    SFT_MODEL_PATH,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# åŠ è½½å¥–åŠ±æ¨¡å‹
print("åŠ è½½å¥–åŠ±æ¨¡å‹ (Reward Model)...")

# è‡ªå®šä¹‰å¥–åŠ±æ¨¡å‹ç±»ï¼ˆä¸ reward_model.py ç›¸åŒï¼‰
class QwenRewardModel(nn.Module):
    def __init__(self, base_model, hidden_size):
        super().__init__()
        self.model = base_model
        self.reward_head = nn.Linear(hidden_size, 1, bias=False)
        
    def forward(self, input_ids, attention_mask=None):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        
        if attention_mask is not None:
            sequence_lengths = attention_mask.sum(dim=1) - 1
            batch_size = input_ids.shape[0]
            last_hidden_state = outputs.hidden_states[-1]
            pooled_output = last_hidden_state[
                torch.arange(batch_size, device=input_ids.device),
                sequence_lengths
            ]
        else:
            pooled_output = outputs.hidden_states[-1][:, -1, :]
        
        rewards = self.reward_head(pooled_output).squeeze(-1)
        return rewards


# æ£€æŸ¥å¥–åŠ±æ¨¡å‹æ˜¯å¦å­˜åœ¨
if os.path.exists(REWARD_MODEL_PATH):
    config = AutoConfig.from_pretrained(REWARD_MODEL_PATH, trust_remote_code=True)
    base_rm = AutoModel.from_pretrained(
        REWARD_MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    reward_model = QwenRewardModel(base_rm, config.hidden_size)
    print(f"âœ… ä» {REWARD_MODEL_PATH} åŠ è½½å¥–åŠ±æ¨¡å‹")
else:
    print(f"âš ï¸ æœªæ‰¾åˆ°å¥–åŠ±æ¨¡å‹ {REWARD_MODEL_PATH}")
    print("   å°†ä½¿ç”¨ç®€åŒ–çš„å¥–åŠ±å‡½æ•°ï¼ˆåŸºäºå›ç­”é•¿åº¦å’Œå…³é”®è¯ï¼‰")
    reward_model = None


# ===== 2. åŠ è½½æ•°æ®é›†ï¼ˆåªéœ€è¦ promptï¼‰=====
print("\nğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®...")
raw_dataset = load_dataset(
    "shibing624/DPO-En-Zh-20k-Preference",
    name="zh",
    split=f"train[:{NUM_SAMPLES}]"
)


def extract_prompts(examples):
    """æå– prompt ç”¨äº PPO è®­ç»ƒ"""
    prompts = []
    
    for system, history, question in zip(
        examples["system"],
        examples["history"],
        examples["question"]
    ):
        prompt_parts = []
        if system and system.strip():
            prompt_parts.append(f"<|im_start|>system\n{system}<|im_end|>")
        
        if history:
            for turn in history:
                if len(turn) >= 2:
                    prompt_parts.append(f"<|im_start|>user\n{turn[0]}<|im_end|>")
                    prompt_parts.append(f"<|im_start|>assistant\n{turn[1]}<|im_end|>")
        
        prompt_parts.append(f"<|im_start|>user\n{question}<|im_end|>")
        prompt_parts.append("<|im_start|>assistant\n")
        
        prompts.append("\n".join(prompt_parts))
    
    return {"prompt": prompts}


dataset = raw_dataset.map(
    extract_prompts,
    batched=True,
    remove_columns=raw_dataset.column_names
)

print(f"âœ… åŠ è½½ {len(dataset)} ä¸ªè®­ç»ƒ prompt")


# ===== 3. PPO é…ç½® =====
ppo_config = PPOConfig(
    model_name=SFT_MODEL_PATH,
    learning_rate=LEARNING_RATE,
    batch_size=BATCH_SIZE,
    mini_batch_size=MINI_BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    ppo_epochs=PPO_EPOCHS,
    gamma=GAMMA,
    lam=LAM,
    cliprange=0.2,                    # PPO clip èŒƒå›´
    cliprange_value=0.2,              # Value å‡½æ•° clip èŒƒå›´
    vf_coef=0.1,                      # Value loss ç³»æ•°
    kl_penalty="kl",                  # KL æƒ©ç½šç±»å‹
    init_kl_coef=KL_PENALTY,          # åˆå§‹ KL ç³»æ•°
    target_kl=0.1,                    # ç›®æ ‡ KLï¼ˆè‡ªé€‚åº”è°ƒæ•´ï¼‰
    log_with=None,                    # ä¸ä½¿ç”¨ wandb
    seed=42,
)


# ===== 4. å¥–åŠ±å‡½æ•° =====
def compute_rewards(
    prompts: List[str],
    responses: List[str],
    reward_model=None
) -> List[float]:
    """
    è®¡ç®—ç”Ÿæˆå›ç­”çš„å¥–åŠ±
    
    å¦‚æœæœ‰å¥–åŠ±æ¨¡å‹ï¼Œä½¿ç”¨ RM æ‰“åˆ†
    å¦åˆ™ä½¿ç”¨ç®€åŒ–çš„è§„åˆ™ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰
    """
    rewards = []
    
    if reward_model is not None:
        # ä½¿ç”¨è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹
        reward_model.eval()
        
        for prompt, response in zip(prompts, responses):
            full_text = f"{prompt}{response}<|im_end|>"
            
            tokens = tokenizer(
                full_text,
                return_tensors="pt",
                max_length=MAX_LENGTH,
                truncation=True
            )
            tokens = {k: v.to(model.pretrained_model.device) for k, v in tokens.items()}
            
            with torch.no_grad():
                reward = reward_model(**tokens).item()
            
            rewards.append(reward)
    else:
        # ç®€åŒ–çš„å¥–åŠ±å‡½æ•°ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼Œå®é™…åº”ä½¿ç”¨ RMï¼‰
        for response in responses:
            reward = 0.0
            
            # å¥–åŠ±é€‚ä¸­é•¿åº¦çš„å›ç­”ï¼ˆä¸å¤ªçŸ­ä¹Ÿä¸å¤ªé•¿ï¼‰
            length = len(response)
            if 50 <= length <= 300:
                reward += 1.0
            elif length < 20:
                reward -= 1.0
            
            # æƒ©ç½šé‡å¤
            if len(set(response)) / max(len(response), 1) < 0.3:
                reward -= 0.5
            
            # å¥–åŠ±åŒ…å«ä¸€äº›æ­£é¢è¯æ±‡
            positive_words = ["è°¢è°¢", "å¸®åŠ©", "äº†è§£", "å­¦ä¹ ", "æ–¹æ³•", "æ­¥éª¤"]
            for word in positive_words:
                if word in response:
                    reward += 0.2
            
            rewards.append(reward)
    
    return rewards


# ===== 5. PPO è®­ç»ƒå¾ªç¯ =====
def train_ppo():
    """PPO ä¸»è®­ç»ƒå¾ªç¯"""
    print("\n" + "=" * 60)
    print("ğŸ‹ï¸ å¼€å§‹ PPO è®­ç»ƒ")
    print("=" * 60)
    print(f"   - è®­ç»ƒæ­¥æ•°: {NUM_TRAIN_STEPS}")
    print(f"   - Batch Size: {BATCH_SIZE}")
    print(f"   - PPO Epochs: {PPO_EPOCHS}")
    print(f"   - Learning Rate: {LEARNING_RATE}")
    print(f"   - KL Penalty: {KL_PENALTY}")
    
    # åˆ›å»º PPO Trainer
    ppo_trainer = PPOTrainer(
        config=ppo_config,
        model=model,
        ref_model=ref_model,
        tokenizer=tokenizer,
        dataset=dataset,
    )
    
    # ç”Ÿæˆé…ç½®
    generation_kwargs = {
        "max_new_tokens": MAX_NEW_TOKENS,
        "do_sample": True,
        "temperature": 0.7,
        "top_p": 0.9,
        "pad_token_id": tokenizer.pad_token_id,
        "eos_token_id": tokenizer.eos_token_id,
    }
    
    # è®­ç»ƒç»Ÿè®¡
    all_rewards = []
    all_kls = []
    
    for step, batch in enumerate(ppo_trainer.dataloader):
        if step >= NUM_TRAIN_STEPS:
            break
        
        # è·å– prompt
        prompt_tensors = batch["input_ids"]
        
        # ç”Ÿæˆå›ç­”
        response_tensors = ppo_trainer.generate(
            prompt_tensors,
            **generation_kwargs
        )
        
        # è§£ç 
        prompts = [tokenizer.decode(p, skip_special_tokens=False) for p in prompt_tensors]
        responses = [tokenizer.decode(r[len(p):], skip_special_tokens=True) 
                    for p, r in zip(prompt_tensors, response_tensors)]
        
        # è®¡ç®—å¥–åŠ±
        rewards = compute_rewards(prompts, responses, reward_model)
        reward_tensors = [torch.tensor(r) for r in rewards]
        
        # PPO æ›´æ–°
        stats = ppo_trainer.step(prompt_tensors, response_tensors, reward_tensors)
        
        # è®°å½•ç»Ÿè®¡
        all_rewards.extend(rewards)
        
        # æ‰“å°è¿›åº¦
        if step % 10 == 0:
            mean_reward = np.mean(rewards)
            print(f"Step {step}/{NUM_TRAIN_STEPS} | "
                  f"Mean Reward: {mean_reward:.4f} | "
                  f"KL: {stats.get('objective/kl', 0):.4f}")
    
    # ä¿å­˜æ¨¡å‹
    print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹åˆ° {OUTPUT_DIR}...")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    print(f"âœ… PPO è®­ç»ƒå®Œæˆï¼")
    print(f"   å¹³å‡å¥–åŠ±: {np.mean(all_rewards):.4f}")
    
    return model


# ===== 6. æµ‹è¯• PPO æ¨¡å‹ =====
def test_ppo_model():
    """æµ‹è¯• PPO è®­ç»ƒåçš„æ¨¡å‹"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯• PPO æ¨¡å‹")
    print("=" * 60)
    
    # åŠ è½½ PPO æ¨¡å‹
    if os.path.exists(OUTPUT_DIR):
        test_model = AutoModelForCausalLM.from_pretrained(
            OUTPUT_DIR,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        test_tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR, trust_remote_code=True)
    else:
        print(f"âš ï¸ PPO æ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨å½“å‰è®­ç»ƒçš„æ¨¡å‹")
        test_model = model.pretrained_model
        test_tokenizer = tokenizer
    
    test_model.eval()
    
    test_prompts = [
        "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
        "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
        "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
    ]
    
    for prompt in test_prompts:
        print(f"\né—®é¢˜: {prompt}")
        print("-" * 40)
        
        full_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
        inputs = test_tokenizer(full_prompt, return_tensors="pt").to(test_model.device)
        
        with torch.no_grad():
            outputs = test_model.generate(
                **inputs,
                max_new_tokens=200,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=test_tokenizer.pad_token_id,
            )
        
        response = test_tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
        if "<|im_end|>" in response:
            response = response.split("<|im_end|>")[0]
        
        print(f"å›ç­”: {response}")


if __name__ == "__main__":
    # PPO è®­ç»ƒ
    train_ppo()
    
    # æµ‹è¯•æ¨¡å‹
    test_ppo_model()
