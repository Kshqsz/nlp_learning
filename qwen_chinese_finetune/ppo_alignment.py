# ppo_alignment.py
"""
RLHF ç¬¬äºŒæ­¥ï¼šPPO (Proximal Policy Optimization) å¯¹é½è®­ç»ƒ

PPO æ˜¯ä»€ä¹ˆï¼š
- ä¸€ç§ç­–ç•¥æ¢¯åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•
- åœ¨ RLHF ä¸­ï¼Œç”¨å¥–åŠ±æ¨¡å‹çš„åˆ†æ•°ä½œä¸ºåé¦ˆä¿¡å·
- ä¼˜åŒ–ç­–ç•¥ï¼ˆLLMï¼‰ç”Ÿæˆé«˜å¥–åŠ±çš„å›ç­”

RLHF å®Œæ•´æµç¨‹ï¼š
1. SFT æ¨¡å‹ï¼ˆå·²å®Œæˆï¼‰â†’ ä¼šéµå¾ªæŒ‡ä»¤
2. Reward Modelï¼ˆä¸Šä¸€æ­¥ï¼‰â†’ å­¦ä¼šæ‰“åˆ†
3. PPO è®­ç»ƒï¼ˆæœ¬è„šæœ¬ï¼‰â†’ ä¼˜åŒ–ç”Ÿæˆé«˜åˆ†å›ç­”

PPO çš„å…³é”®ç»„ä»¶ï¼š
- Actorï¼ˆç­–ç•¥æ¨¡å‹ï¼‰ï¼šç”Ÿæˆå›ç­”ï¼Œå°±æ˜¯è¦è®­ç»ƒçš„ LLM
- Criticï¼ˆä»·å€¼æ¨¡å‹ï¼‰ï¼šé¢„æµ‹çŠ¶æ€ä»·å€¼ï¼Œå¸®åŠ©è®¡ç®—ä¼˜åŠ¿å‡½æ•°
- Reward Modelï¼šç»™ç”Ÿæˆçš„å›ç­”æ‰“åˆ†
- Reference Modelï¼šKL çº¦æŸï¼Œé˜²æ­¢æ¨¡å‹è·‘å

æœ¬å®ç°ä½¿ç”¨æ‰‹åŠ¨ PPO è®­ç»ƒå¾ªç¯ï¼Œä¸ä¾èµ– trl çš„ PPOTrainer
è¿™æ ·å¯ä»¥æ›´æ¸…æ™°åœ°ç†è§£ PPO çš„å·¥ä½œåŸç†ï¼ŒåŒæ—¶é¿å…ç‰ˆæœ¬å…¼å®¹é—®é¢˜
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
import torch.nn as nn
import torch.nn.functional as F
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModel,
    AutoConfig,
    get_linear_schedule_with_warmup,
)
from tqdm import tqdm
import numpy as np
from typing import List, Dict, Tuple

# ===== é…ç½® =====
SFT_MODEL_PATH = "./qwen_sft"              # SFT æ¨¡å‹ï¼ˆä½œä¸ºåˆå§‹ç­–ç•¥ï¼‰
REWARD_MODEL_PATH = "./qwen_reward_model"  # å¥–åŠ±æ¨¡å‹
OUTPUT_DIR = "./qwen_ppo"
MAX_LENGTH = 512
MAX_NEW_TOKENS = 64                         # ç”Ÿæˆçš„æœ€å¤§æ–° token æ•°ï¼ˆå‡å°‘ä»¥èŠ‚çœæ˜¾å­˜ï¼‰
BATCH_SIZE = 2                              # æ¯æ‰¹å¤„ç†çš„ prompt æ•°é‡ï¼ˆå‡å°‘ä»¥èŠ‚çœæ˜¾å­˜ï¼‰
GRADIENT_ACCUMULATION_STEPS = 2             # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæœ‰æ•ˆ batch = 4ï¼‰
LEARNING_RATE = 1e-5
NUM_TRAIN_STEPS = 200                       # æ€»è®­ç»ƒæ­¥æ•°
KL_COEF = 0.1                               # KL æ•£åº¦æƒ©ç½šç³»æ•°
CLIP_RANGE = 0.2                            # PPO clip èŒƒå›´
VALUE_CLIP_RANGE = 0.2                      # Value å‡½æ•° clip èŒƒå›´
VALUE_COEF = 0.5                            # Value loss ç³»æ•°
ENTROPY_COEF = 0.01                         # ç†µå¥–åŠ±ç³»æ•°
GAE_LAMBDA = 0.95                           # GAE lambda
GAMMA = 1.0                                 # æŠ˜æ‰£å› å­
NUM_PPO_EPOCHS = 2                          # æ¯ä¸ª batch çš„ PPO æ›´æ–°è½®æ•°ï¼ˆå‡å°‘ä»¥èŠ‚çœæ˜¾å­˜ï¼‰
NUM_SAMPLES = 1000                          # ä½¿ç”¨å¤šå°‘ prompt è®­ç»ƒ


# ===== 1. Value Head æ¨¡å— =====
class ValueHead(nn.Module):
    """
    ä»·å€¼å¤´æ¨¡å—
    
    å°† LLM çš„éšè—çŠ¶æ€æ˜ å°„åˆ°æ ‡é‡ä»·å€¼
    ç”¨äº Actor-Critic ä¸­çš„ Critic éƒ¨åˆ†
    """
    def __init__(self, hidden_size: int, dtype=torch.bfloat16):
        super().__init__()
        self.dense = nn.Linear(hidden_size, hidden_size, dtype=dtype)
        self.out_proj = nn.Linear(hidden_size, 1, dtype=dtype)
    
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        # hidden_states: (batch, seq_len, hidden_size)
        # å–æœ€åä¸€ä¸ª token çš„éšè—çŠ¶æ€
        x = hidden_states[:, -1, :]  # (batch, hidden_size)
        x = torch.tanh(self.dense(x))
        value = self.out_proj(x).squeeze(-1)  # (batch,)
        return value


# ===== 2. ç­–ç•¥æ¨¡å‹ï¼ˆå¸¦ Value Headï¼‰=====
class PolicyModelWithValueHead(nn.Module):
    """
    ç­–ç•¥æ¨¡å‹ + ä»·å€¼å¤´
    
    - policy_model: åŸå§‹ LLMï¼Œè´Ÿè´£ç”Ÿæˆ token
    - value_head: é¢„æµ‹çŠ¶æ€ä»·å€¼
    """
    def __init__(self, model_path: str, dtype=torch.bfloat16):
        super().__init__()
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        self.policy_model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=dtype,
            device_map="auto"
        )
        
        # è·å–éšè—ç»´åº¦
        hidden_size = self.policy_model.config.hidden_size
        
        # æ·»åŠ ä»·å€¼å¤´
        self.value_head = ValueHead(hidden_size, dtype=dtype)
        # å°† value_head ç§»åŠ¨åˆ°ä¸ policy_model ç›¸åŒçš„è®¾å¤‡
        self.value_head.to(self.policy_model.device)
    
    @property
    def device(self):
        return self.policy_model.device
    
    def parameters(self):
        """è¿”å›æ‰€æœ‰éœ€è¦è®­ç»ƒçš„å‚æ•°"""
        # åˆå¹¶ policy_model å’Œ value_head çš„å‚æ•°
        for param in self.policy_model.parameters():
            yield param
        for param in self.value_head.parameters():
            yield param
    
    def train(self, mode=True):
        """è®¾ç½®è®­ç»ƒæ¨¡å¼"""
        self.policy_model.train(mode)
        self.value_head.train(mode)
        return self
    
    def eval(self):
        """è®¾ç½®è¯„ä¼°æ¨¡å¼"""
        self.policy_model.eval()
        self.value_head.eval()
        return self
    
    def forward(
        self, 
        input_ids: torch.Tensor, 
        attention_mask: torch.Tensor = None,
        return_value: bool = False
    ):
        """
        å‰å‘ä¼ æ’­
        
        è¿”å›ï¼š
        - logits: token é¢„æµ‹ logits
        - value: çŠ¶æ€ä»·å€¼ï¼ˆå¦‚æœ return_value=Trueï¼‰
        """
        outputs = self.policy_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=return_value
        )
        
        logits = outputs.logits
        
        if return_value:
            hidden_states = outputs.hidden_states[-1]
            value = self.value_head(hidden_states)
            return logits, value
        
        return logits
    
    def generate(self, **kwargs):
        """ç”Ÿæˆæ–‡æœ¬"""
        return self.policy_model.generate(**kwargs)
    
    def save_pretrained(self, save_directory: str):
        """ä¿å­˜æ¨¡å‹"""
        os.makedirs(save_directory, exist_ok=True)
        # ä¿å­˜ policy model
        self.policy_model.save_pretrained(save_directory)
        # ä¿å­˜ value head
        torch.save(
            self.value_head.state_dict(),
            os.path.join(save_directory, "value_head.pt")
        )


# ===== 3. å¥–åŠ±æ¨¡å‹ï¼ˆä¸ reward_model.py ç›¸åŒï¼‰=====
class QwenRewardModel(nn.Module):
    """å¥–åŠ±æ¨¡å‹ï¼šç»™å›ç­”æ‰“åˆ†"""
    def __init__(self, base_model, hidden_size):
        super().__init__()
        self.model = base_model
        self.reward_head = nn.Linear(hidden_size, 1, bias=False)
        # ç¡®ä¿ reward_head ä¸ base_model åœ¨åŒä¸€è®¾å¤‡å’Œ dtype
        device = next(base_model.parameters()).device
        dtype = next(base_model.parameters()).dtype
        self.reward_head.to(device=device, dtype=dtype)
        
    def forward(self, input_ids, attention_mask=None):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        
        if attention_mask is not None:
            sequence_lengths = attention_mask.sum(dim=1) - 1
            batch_size = input_ids.shape[0]
            last_hidden_state = outputs.hidden_states[-1]
            pooled_output = last_hidden_state[
                torch.arange(batch_size, device=input_ids.device),
                sequence_lengths
            ]
        else:
            pooled_output = outputs.hidden_states[-1][:, -1, :]
        
        rewards = self.reward_head(pooled_output).squeeze(-1)
        return rewards


# ===== 4. PPO æ ¸å¿ƒå‡½æ•° =====
def compute_log_probs(
    logits: torch.Tensor,
    labels: torch.Tensor,
    mask: torch.Tensor = None
) -> torch.Tensor:
    """
    è®¡ç®— token çº§åˆ«çš„ log æ¦‚ç‡
    
    logits: (batch, seq_len, vocab_size)
    labels: (batch, seq_len)
    mask: (batch, seq_len) - åªè®¡ç®—ç”Ÿæˆéƒ¨åˆ†çš„ log prob
    
    è¿”å›: (batch,) - æ¯ä¸ªæ ·æœ¬çš„å¹³å‡ log prob
    """
    log_probs = F.log_softmax(logits, dim=-1)
    # å–å‡º labels å¯¹åº”ä½ç½®çš„ log prob
    # labels[:, 1:] å› ä¸ºè¦é¢„æµ‹ä¸‹ä¸€ä¸ª token
    selected_log_probs = torch.gather(
        log_probs[:, :-1, :], 
        dim=-1, 
        index=labels[:, 1:].unsqueeze(-1)
    ).squeeze(-1)  # (batch, seq_len-1)
    
    if mask is not None:
        # mask ä¹Ÿè¦å¯¹é½
        mask = mask[:, 1:]
        selected_log_probs = selected_log_probs * mask
        # è¿”å›å¹³å‡ log prob
        return selected_log_probs.sum(dim=-1) / (mask.sum(dim=-1) + 1e-8)
    
    return selected_log_probs.mean(dim=-1)


def compute_advantages(
    rewards: torch.Tensor,
    values: torch.Tensor,
    gamma: float = 1.0,
    lam: float = 0.95
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    ä½¿ç”¨ GAE (Generalized Advantage Estimation) è®¡ç®—ä¼˜åŠ¿å‡½æ•°
    
    åœ¨æˆ‘ä»¬çš„åœºæ™¯ä¸­ï¼Œæ¯ä¸ª episode åªæœ‰ä¸€æ­¥ï¼ˆç”Ÿæˆä¸€ä¸ªå®Œæ•´å›ç­”ï¼‰
    æ‰€ä»¥ç®€åŒ–ä¸ºï¼šadvantage = reward - value
    
    è¿”å›ï¼šadvantages, returns
    """
    # ç®€åŒ–ç‰ˆï¼šå•æ­¥ episode
    # advantage = reward - value (ç›¸å½“äº TD error)
    # return = reward (å› ä¸ºæ²¡æœ‰åç»­çŠ¶æ€)
    advantages = rewards - values.detach()
    returns = rewards
    
    return advantages, returns


def ppo_loss(
    old_log_probs: torch.Tensor,
    new_log_probs: torch.Tensor,
    advantages: torch.Tensor,
    clip_range: float = 0.2
) -> torch.Tensor:
    """
    PPO çš„ Clipped Surrogate Loss
    
    L = min(r * A, clip(r, 1-Îµ, 1+Îµ) * A)
    
    å…¶ä¸­ r = exp(new_log_prob - old_log_prob) æ˜¯æ¦‚ç‡æ¯”
    """
    # æ¦‚ç‡æ¯”
    ratio = torch.exp(new_log_probs - old_log_probs)
    
    # Clipped ç‰ˆæœ¬
    clipped_ratio = torch.clamp(ratio, 1.0 - clip_range, 1.0 + clip_range)
    
    # å–æœ€å°å€¼ï¼ˆæ‚²è§‚ä¼°è®¡ï¼‰
    surrogate1 = ratio * advantages
    surrogate2 = clipped_ratio * advantages
    
    # PPO loss æ˜¯è´Ÿçš„ï¼ˆå› ä¸ºè¦æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±ï¼‰
    loss = -torch.min(surrogate1, surrogate2).mean()
    
    return loss


def value_loss(
    values: torch.Tensor,
    old_values: torch.Tensor,
    returns: torch.Tensor,
    clip_range: float = 0.2
) -> torch.Tensor:
    """
    Value å‡½æ•° lossï¼ˆå¸¦ clippingï¼‰
    
    ç±»ä¼¼äº PPO çš„ç­–ç•¥ lossï¼Œå¯¹ value çš„æ›´æ–°ä¹Ÿåš clip
    """
    # Clipped value
    clipped_values = old_values + torch.clamp(
        values - old_values, 
        -clip_range, 
        clip_range
    )
    
    # ä¸¤ç§ loss
    loss1 = (values - returns) ** 2
    loss2 = (clipped_values - returns) ** 2
    
    # å–æœ€å¤§å€¼ï¼ˆæ‚²è§‚ä¼°è®¡ï¼‰
    loss = 0.5 * torch.max(loss1, loss2).mean()
    
    return loss


# ===== 5. æ•°æ®åŠ è½½ =====
def load_prompts(tokenizer, num_samples: int) -> List[Dict]:
    """åŠ è½½è®­ç»ƒ prompt"""
    print("ğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®...")
    
    raw_dataset = load_dataset(
        "shibing624/DPO-En-Zh-20k-Preference",
        name="zh",
        split=f"train[:{num_samples}]"
    )
    
    prompts = []
    for item in raw_dataset:
        prompt_parts = []
        
        if item["system"] and item["system"].strip():
            prompt_parts.append(f"<|im_start|>system\n{item['system']}<|im_end|>")
        
        if item["history"]:
            for turn in item["history"]:
                if len(turn) >= 2:
                    prompt_parts.append(f"<|im_start|>user\n{turn[0]}<|im_end|>")
                    prompt_parts.append(f"<|im_start|>assistant\n{turn[1]}<|im_end|>")
        
        prompt_parts.append(f"<|im_start|>user\n{item['question']}<|im_end|>")
        prompt_parts.append("<|im_start|>assistant\n")
        
        prompt_text = "\n".join(prompt_parts)
        prompts.append({"text": prompt_text})
    
    print(f"âœ… åŠ è½½ {len(prompts)} ä¸ªè®­ç»ƒ prompt")
    return prompts


# ===== 6. PPO è®­ç»ƒå¾ªç¯ =====
def train_ppo():
    """PPO ä¸»è®­ç»ƒå¾ªç¯"""
    print("=" * 60)
    print("ğŸš€ PPO å¯¹é½è®­ç»ƒ")
    print("=" * 60)
    
    # ===== åŠ è½½ tokenizer =====
    print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL_PATH, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ===== åŠ è½½ç­–ç•¥æ¨¡å‹ï¼ˆActor + Criticï¼‰=====
    print("åŠ è½½ç­–ç•¥æ¨¡å‹ (Policy + Value Head)...")
    policy_model = PolicyModelWithValueHead(SFT_MODEL_PATH)
    
    # ===== åŠ è½½å‚è€ƒæ¨¡å‹ï¼ˆå†»ç»“ï¼‰=====
    print("åŠ è½½å‚è€ƒæ¨¡å‹ (Reference)...")
    ref_model = AutoModelForCausalLM.from_pretrained(
        SFT_MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    ref_model.eval()
    for param in ref_model.parameters():
        param.requires_grad = False
    
    # ===== åŠ è½½å¥–åŠ±æ¨¡å‹ =====
    print("åŠ è½½å¥–åŠ±æ¨¡å‹ (Reward Model)...")
    if os.path.exists(REWARD_MODEL_PATH):
        config = AutoConfig.from_pretrained(REWARD_MODEL_PATH, trust_remote_code=True)
        base_rm = AutoModel.from_pretrained(
            REWARD_MODEL_PATH,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        reward_model = QwenRewardModel(base_rm, config.hidden_size)
        reward_model.eval()
        for param in reward_model.parameters():
            param.requires_grad = False
        print(f"âœ… ä» {REWARD_MODEL_PATH} åŠ è½½å¥–åŠ±æ¨¡å‹")
        use_reward_model = True
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°å¥–åŠ±æ¨¡å‹ {REWARD_MODEL_PATH}")
        print("   å°†ä½¿ç”¨ç®€åŒ–çš„å¥–åŠ±å‡½æ•°ï¼ˆåŸºäºå›ç­”é•¿åº¦ï¼‰")
        reward_model = None
        use_reward_model = False
    
    # ===== åŠ è½½æ•°æ® =====
    prompts = load_prompts(tokenizer, NUM_SAMPLES)
    
    # ===== è®¾ç½®ä¼˜åŒ–å™¨ =====
    optimizer = torch.optim.AdamW(
        policy_model.parameters(),
        lr=LEARNING_RATE,
        betas=(0.9, 0.95)
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=10,
        num_training_steps=NUM_TRAIN_STEPS
    )
    
    # ===== è®­ç»ƒç»Ÿè®¡ =====
    all_rewards = []
    all_kls = []
    
    print("\n" + "=" * 60)
    print("ğŸ‹ï¸ å¼€å§‹ PPO è®­ç»ƒ")
    print("=" * 60)
    print(f"   - è®­ç»ƒæ­¥æ•°: {NUM_TRAIN_STEPS}")
    print(f"   - Batch Size: {BATCH_SIZE}")
    print(f"   - æ¢¯åº¦ç´¯ç§¯: {GRADIENT_ACCUMULATION_STEPS}")
    print(f"   - æœ‰æ•ˆ Batch: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")
    print(f"   - PPO Epochs: {NUM_PPO_EPOCHS}")
    print(f"   - Learning Rate: {LEARNING_RATE}")
    print(f"   - KL Coefficient: {KL_COEF}")
    
    # ===== ä¸»è®­ç»ƒå¾ªç¯ =====
    prompt_idx = 0
    optimizer.zero_grad()  # åˆå§‹åŒ–æ¢¯åº¦
    
    for step in tqdm(range(NUM_TRAIN_STEPS), desc="PPO Training"):
        # é‡‡æ ·ä¸€ä¸ª batch çš„ prompt
        batch_prompts = []
        for _ in range(BATCH_SIZE):
            batch_prompts.append(prompts[prompt_idx % len(prompts)])
            prompt_idx += 1
        
        # ===== ç”Ÿæˆå›ç­” =====
        policy_model.eval()
        
        generated_texts = []
        full_sequences = []
        prompt_lengths = []
        
        for prompt_data in batch_prompts:
            prompt_text = prompt_data["text"]
            inputs = tokenizer(prompt_text, return_tensors="pt").to(policy_model.device)
            prompt_len = inputs["input_ids"].shape[1]
            prompt_lengths.append(prompt_len)
            
            with torch.no_grad():
                outputs = policy_model.generate(
                    **inputs,
                    max_new_tokens=MAX_NEW_TOKENS,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
            
            full_seq = outputs[0]
            response = tokenizer.decode(full_seq[prompt_len:], skip_special_tokens=True)
            if "<|im_end|>" in response:
                response = response.split("<|im_end|>")[0]
            
            generated_texts.append(response)
            full_sequences.append(full_seq)
        
        # ===== è®¡ç®—å¥–åŠ± =====
        rewards = []
        
        for prompt_data, response in zip(batch_prompts, generated_texts):
            if use_reward_model and reward_model is not None:
                # ä½¿ç”¨å¥–åŠ±æ¨¡å‹æ‰“åˆ†
                full_text = f"{prompt_data['text']}{response}<|im_end|>"
                tokens = tokenizer(
                    full_text,
                    return_tensors="pt",
                    max_length=MAX_LENGTH,
                    truncation=True
                ).to(next(reward_model.parameters()).device)
                
                with torch.no_grad():
                    reward = reward_model(**tokens).item()
            else:
                # ç®€åŒ–çš„å¥–åŠ±å‡½æ•°
                reward = 0.0
                length = len(response)
                if 50 <= length <= 300:
                    reward += 1.0
                elif length < 20:
                    reward -= 1.0
                # æƒ©ç½šé‡å¤
                if len(response) > 0 and len(set(response)) / len(response) < 0.3:
                    reward -= 0.5
            
            rewards.append(reward)
        
        rewards = torch.tensor(rewards, dtype=torch.float32, device=policy_model.device)
        all_rewards.append(rewards.mean().item())
        
        # æ¸…ç†ç”Ÿæˆé˜¶æ®µçš„æ˜¾å­˜
        torch.cuda.empty_cache()
        
        # ===== è®¡ç®— old log probs å’Œ values =====
        policy_model.eval()
        
        # Pad sequences to same length
        max_len = max(seq.shape[0] for seq in full_sequences)
        padded_input_ids = []
        attention_masks = []
        response_masks = []
        
        for seq, prompt_len in zip(full_sequences, prompt_lengths):
            seq_len = seq.shape[0]
            padding_len = max_len - seq_len
            
            if padding_len > 0:
                padded_seq = F.pad(seq, (0, padding_len), value=tokenizer.pad_token_id)
                attn_mask = torch.cat([
                    torch.ones(seq_len, device=seq.device),
                    torch.zeros(padding_len, device=seq.device)
                ])
            else:
                padded_seq = seq
                attn_mask = torch.ones(seq_len, device=seq.device)
            
            # Response mask: åªå¯¹ç”Ÿæˆçš„ token è®¡ç®— loss
            resp_mask = torch.zeros(max_len, device=seq.device)
            resp_mask[prompt_len:seq_len] = 1.0
            
            padded_input_ids.append(padded_seq)
            attention_masks.append(attn_mask)
            response_masks.append(resp_mask)
        
        input_ids = torch.stack(padded_input_ids)  # (batch, max_len)
        attention_mask = torch.stack(attention_masks)  # (batch, max_len)
        response_mask = torch.stack(response_masks)  # (batch, max_len)
        
        with torch.no_grad():
            # Policy model log probs and values
            logits, values = policy_model(
                input_ids, attention_mask, return_value=True
            )
            old_log_probs = compute_log_probs(logits, input_ids, response_mask)
            old_values = values
            
            # Reference model log probs (for KL penalty)
            ref_logits = ref_model(input_ids, attention_mask=attention_mask).logits
            ref_log_probs = compute_log_probs(ref_logits, input_ids, response_mask)
        
        # KL divergence
        kl = (old_log_probs - ref_log_probs).mean()
        all_kls.append(kl.item())
        
        # è®¡ç®— advantages å’Œ returns
        # å¥–åŠ±éœ€è¦å‡å» KL æƒ©ç½š
        adjusted_rewards = rewards - KL_COEF * kl
        advantages, returns = compute_advantages(
            adjusted_rewards, old_values, GAMMA, GAE_LAMBDA
        )
        
        # æ ‡å‡†åŒ– advantages
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # ===== PPO æ›´æ–° =====
        policy_model.train()
        
        for ppo_epoch in range(NUM_PPO_EPOCHS):
            # Forward pass
            new_logits, new_values = policy_model(
                input_ids, attention_mask, return_value=True
            )
            new_log_probs = compute_log_probs(new_logits, input_ids, response_mask)
            
            # Policy loss (PPO clipped)
            policy_loss = ppo_loss(
                old_log_probs.detach(),
                new_log_probs,
                advantages,
                CLIP_RANGE
            )
            
            # Value loss
            v_loss = value_loss(
                new_values,
                old_values.detach(),
                returns,
                VALUE_CLIP_RANGE
            )
            
            # Entropy bonus (é¼“åŠ±æ¢ç´¢) - ç®€åŒ–è®¡ç®—ä»¥èŠ‚çœæ˜¾å­˜
            # åªåœ¨ response éƒ¨åˆ†è®¡ç®—ç†µ
            response_logits = new_logits[:, :-1, :] * response_mask[:, 1:].unsqueeze(-1)
            log_probs_all = F.log_softmax(response_logits, dim=-1)
            probs_all = F.softmax(response_logits, dim=-1)
            entropy = -(probs_all * log_probs_all).sum(dim=-1)
            entropy = (entropy * response_mask[:, 1:]).sum() / (response_mask[:, 1:].sum() + 1e-8)
            
            # Total loss (é™¤ä»¥æ¢¯åº¦ç´¯ç§¯æ­¥æ•°)
            total_loss = (policy_loss + VALUE_COEF * v_loss - ENTROPY_COEF * entropy) / GRADIENT_ACCUMULATION_STEPS
            
            # Backward
            total_loss.backward()
            
            # æ¢¯åº¦ç´¯ç§¯ï¼šæ¯ GRADIENT_ACCUMULATION_STEPS æ­¥æ›´æ–°ä¸€æ¬¡
            if (ppo_epoch + 1) % GRADIENT_ACCUMULATION_STEPS == 0 or ppo_epoch == NUM_PPO_EPOCHS - 1:
                torch.nn.utils.clip_grad_norm_(policy_model.parameters(), 1.0)
                optimizer.step()
                optimizer.zero_grad()
            
            # æ¸…ç†ä¸­é—´å˜é‡
            del new_logits, new_values, response_logits, log_probs_all, probs_all
        
        # æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
        
        scheduler.step()
        
        # ===== æ—¥å¿— =====
        if (step + 1) % 10 == 0:
            avg_reward = np.mean(all_rewards[-10:])
            avg_kl = np.mean(all_kls[-10:])
            print(f"\n[Step {step+1}/{NUM_TRAIN_STEPS}] "
                  f"Avg Reward: {avg_reward:.4f}, "
                  f"Avg KL: {avg_kl:.4f}, "
                  f"Policy Loss: {policy_loss.item():.4f}")
    
    # ===== ä¿å­˜æ¨¡å‹ =====
    print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹åˆ° {OUTPUT_DIR}...")
    policy_model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    print("âœ… PPO è®­ç»ƒå®Œæˆï¼")
    
    # æ‰“å°è®­ç»ƒç»Ÿè®¡
    print(f"\nğŸ“ˆ è®­ç»ƒç»Ÿè®¡:")
    print(f"   å¹³å‡å¥–åŠ±: {np.mean(all_rewards):.4f}")
    print(f"   æœ€ç»ˆå¥–åŠ±: {np.mean(all_rewards[-10:]):.4f}")
    print(f"   å¹³å‡ KL: {np.mean(all_kls):.4f}")
    
    return policy_model


# ===== 7. æµ‹è¯• PPO æ¨¡å‹ =====
def test_ppo_model():
    """æµ‹è¯• PPO è®­ç»ƒåçš„æ¨¡å‹"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯• PPO æ¨¡å‹")
    print("=" * 60)
    
    # åŠ è½½ PPO æ¨¡å‹
    if os.path.exists(OUTPUT_DIR):
        test_model = AutoModelForCausalLM.from_pretrained(
            OUTPUT_DIR,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        test_tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR, trust_remote_code=True)
        print(f"âœ… ä» {OUTPUT_DIR} åŠ è½½ PPO æ¨¡å‹")
    else:
        print(f"âš ï¸ PPO æ¨¡å‹ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒ")
        return
    
    test_model.eval()
    
    test_prompts = [
        "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
        "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
        "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
    ]
    
    for prompt in test_prompts:
        print(f"\né—®é¢˜: {prompt}")
        print("-" * 40)
        
        full_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
        inputs = test_tokenizer(full_prompt, return_tensors="pt").to(test_model.device)
        
        with torch.no_grad():
            outputs = test_model.generate(
                **inputs,
                max_new_tokens=200,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=test_tokenizer.pad_token_id,
            )
        
        response = test_tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:], 
            skip_special_tokens=True
        )
        if "<|im_end|>" in response:
            response = response.split("<|im_end|>")[0]
        
        print(f"å›ç­”: {response}")


if __name__ == "__main__":
    # PPO è®­ç»ƒ
    train_ppo()
    
    # æµ‹è¯•æ¨¡å‹
    test_ppo_model()
