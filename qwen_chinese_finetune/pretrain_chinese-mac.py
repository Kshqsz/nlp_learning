from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForLanguageModeling,
    TrainingArguments,
    Trainer
)
import torch

# ===== 1. é…ç½®ï¼ˆä¸­æ–‡ + Apple M4 MPS ä¼˜åŒ–ï¼‰=====
MODEL_NAME = "Qwen/Qwen1.5-0.5B"
DATASET_NAME = "pleisto/wikipedia-cn-20230720-filtered"
OUTPUT_DIR = "./qwen_pretrained"
MAX_LENGTH = 512
NUM_TRAIN_EPOCHS = 1

# æ£€æµ‹è®¾å¤‡
if torch.backends.mps.is_available():
    print("ğŸ ä½¿ç”¨ Apple M4 MPS åŠ é€Ÿ")
else:
    print("âš ï¸ MPS ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")

# ===== 2. åŠ è½½ tokenizer å’Œ model =====
print("Loading tokenizer and model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME, 
    trust_remote_code=True,
    torch_dtype=torch.float32,  # MPS å¯¹ float32 æ”¯æŒæ›´å¥½
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = model.config.eos_token_id

# ===== 3. åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›† =====
print("Loading dataset...")
dataset = load_dataset(DATASET_NAME, split="train[:3000]")

def tokenize_function(examples):
    return tokenizer(
        examples["completion"], 
        truncation=True,
        padding=False,
        max_length=MAX_LENGTH,
        return_overflowing_tokens=True
    )

print("Tokenizing dataset...")
tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=dataset.column_names,
    desc="Tokenizing"
)

# ===== 4. æ•°æ®æ•´ç†å™¨ =====
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

# ===== 5. è®­ç»ƒé…ç½®ï¼ˆApple M4 MPS ä¼˜åŒ–ï¼‰=====
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    overwrite_output_dir=True,
    num_train_epochs=NUM_TRAIN_EPOCHS,
    per_device_train_batch_size=2,          # â† M4 ç»Ÿä¸€å†…å­˜ï¼Œbatch è®¾å°ä¸€ç‚¹
    save_steps=500,
    logging_steps=50,
    learning_rate=2e-5,
    weight_decay=0.01,
    fp16=False,                             # â† MPS ä¸æ”¯æŒ fp16 è®­ç»ƒ
    bf16=False,                             # â† MPS ä¸æ”¯æŒ bf16 è®­ç»ƒ
    report_to="none",
    dataloader_num_workers=0,               # â† macOS å¤šè¿›ç¨‹æœ‰é—®é¢˜ï¼Œå¿…é¡»è®¾ä¸º 0
    optim="adamw_torch",
    gradient_accumulation_steps=4,          # â† ç´¯ç§¯æ¢¯åº¦ï¼Œç­‰æ•ˆ batch_size = 2*4 = 8
    use_mps_device=True,                    # â† å¯ç”¨ MPS
)

# ===== 6. åˆ›å»º Trainer =====
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
    processing_class=tokenizer,
)

# ===== 7. å¼€å§‹è®­ç»ƒ =====
print("ğŸš€ Starting Chinese continued pretraining on Apple M4...")
trainer.train()

# ===== 8. ä¿å­˜æ¨¡å‹ =====
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print(f"âœ… ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹å·²ä¿å­˜åˆ° {OUTPUT_DIR}")