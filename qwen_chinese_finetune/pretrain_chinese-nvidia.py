# pretrain_chinese.py
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForLanguageModeling,
    TrainingArguments,
    Trainer
)

# ===== 1. é…ç½®ï¼ˆä¸­æ–‡ + CUDA ä¼˜åŒ–ï¼‰=====
MODEL_NAME = "Qwen/Qwen1.5-0.5B"
DATASET_NAME = "pleisto/wikipedia-cn-20230720-filtered"
OUTPUT_DIR = "./qwen_pretrained"
MAX_LENGTH = 512
NUM_TRAIN_EPOCHS = 1

# ===== 2. åŠ è½½ tokenizer å’Œ model =====
print("Loading tokenizer and model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, trust_remote_code=True)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = model.config.eos_token_id

# ===== 3. åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›† =====
print("Loading dataset...")
dataset = load_dataset(DATASET_NAME, split="train[:3000]")

def tokenize_function(examples):
    return tokenizer(
        examples["completion"], 
        truncation=True,
        padding=False,
        max_length=MAX_LENGTH,
        return_overflowing_tokens=True
    )

print("Tokenizing dataset...")
tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=dataset.column_names,
    desc="Tokenizing"
)

# ===== 4. æ•°æ®æ•´ç†å™¨ =====
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

# ===== 5. è®­ç»ƒé…ç½®ï¼ˆNVIDIA 4090D ä¼˜åŒ–ï¼‰=====
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    overwrite_output_dir=True,
    num_train_epochs=NUM_TRAIN_EPOCHS,
    per_device_train_batch_size=8,          # â† å…³é”®ï¼š4090D æ˜¾å­˜å¤§ï¼Œå¯è®¾ 8~16
    save_steps=500,
    logging_steps=100,
    learning_rate=2e-5,
    weight_decay=0.01,
    fp16=True,                              # â† NVIDIA æ¨èç”¨ fp16ï¼ˆæ›´å¿«ã€æ›´çœå†…å­˜ï¼‰
    # bf16=True ä¹Ÿå¯ï¼Œä½† fp16 åœ¨ consumer GPU ä¸Šæ›´æˆç†Ÿ
    report_to="none",
    dataloader_num_workers=4,               # â† åˆ©ç”¨å¤šè¿›ç¨‹åŠ é€Ÿæ•°æ®åŠ è½½
    optim="adamw_torch",
    gradient_accumulation_steps=1,          # å¦‚æœ batch å¤ªå¤§å¯è°ƒé«˜ï¼Œè¿™é‡Œä¸éœ€è¦
)

# ===== 6. åˆ›å»º Trainer =====
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
    processing_class=tokenizer,  # æ–°ç‰ˆæœ¬æ¨èç”¨ processing_class
)

# ===== 7. å¼€å§‹è®­ç»ƒ =====
print("ğŸš€ Starting Chinese continued pretraining on NVIDIA 4090D...")
trainer.train()

# ===== 8. ä¿å­˜æ¨¡å‹ =====
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print(f"âœ… ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹å·²ä¿å­˜åˆ° {OUTPUT_DIR}")