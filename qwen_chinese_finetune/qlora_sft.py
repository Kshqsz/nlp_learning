# qlora_sft.py
"""
QLoRA (Quantized LoRA) é‡åŒ– + LoRA å¾®è°ƒ

QLoRA æ˜¯ä»€ä¹ˆï¼š
- 4-bit é‡åŒ– + LoRA çš„ç»„åˆ
- è¿›ä¸€æ­¥é™ä½æ˜¾å­˜å ç”¨ï¼ˆå¯åœ¨æ¶ˆè´¹çº§æ˜¾å¡è®­ç»ƒå¤§æ¨¡å‹ï¼‰
- ç”¨ NF4 é‡åŒ–æ ¼å¼å­˜å‚¨åŸºåº§æ¨¡å‹
- åªè®­ç»ƒ LoRA å‚æ•°ï¼ˆåå‘ä¼ æ’­æ—¶ç”¨ bf16ï¼‰

æ˜¾å­˜å¯¹æ¯”ï¼ˆä»¥ Qwen-7B ä¸ºä¾‹ï¼‰ï¼š
- Full Fine-tuning:  ~60GB
- LoRA (fp16):       ~15GB  
- QLoRA (4-bit):     ~6GB   â† å¯åœ¨ 4090 ä¸Šè®­ç»ƒ 7B æ¨¡å‹ï¼

QLoRA å…³é”®æŠ€æœ¯ï¼š
1. NF4 é‡åŒ–ï¼šä¸“ä¸ºæ­£æ€åˆ†å¸ƒæƒé‡è®¾è®¡çš„ 4-bit é‡åŒ–
2. åŒé‡é‡åŒ–ï¼šé‡åŒ–é‡åŒ–å¸¸æ•°ï¼Œè¿›ä¸€æ­¥å‹ç¼©
3. åˆ†é¡µä¼˜åŒ–å™¨ï¼šå¤„ç†æ˜¾å­˜å³°å€¼

æœ¬è„šæœ¬æ¼”ç¤º QLoRA çš„ä½¿ç”¨æ–¹æ³•
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    BitsAndBytesConfig,  # é‡åŒ–é…ç½®
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    prepare_model_for_kbit_training,  # QLoRA å¿…éœ€
)

# ===== é…ç½® =====
# å¯¹äºå°æ¨¡å‹ï¼ˆ0.5Bï¼‰ä½¿ç”¨é‡åŒ–æ„ä¹‰ä¸å¤§ï¼Œè¿™é‡Œä¸»è¦æ˜¯æ¼”ç¤º
# å®é™…åº”ç”¨ä¸­ï¼ŒQLoRA æ›´é€‚åˆ 7B+ çš„å¤§æ¨¡å‹
BASE_MODEL_PATH = "Qwen/Qwen1.5-0.5B"
OUTPUT_DIR = "./qwen_qlora_sft"
MAX_LENGTH = 512
BATCH_SIZE = 4
GRADIENT_ACCUMULATION_STEPS = 4
LEARNING_RATE = 2e-4
NUM_EPOCHS = 2
NUM_SAMPLES = 5000

# LoRA é…ç½®
LORA_R = 8
LORA_ALPHA = 16
LORA_DROPOUT = 0.05
TARGET_MODULES = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]


# ===== 1. é…ç½® 4-bit é‡åŒ– =====
print("=" * 60)
print("ğŸš€ QLoRA (4-bit é‡åŒ– + LoRA) å¾®è°ƒ")
print("=" * 60)

# BitsAndBytes 4-bit é‡åŒ–é…ç½®
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,                   # ä½¿ç”¨ 4-bit é‡åŒ–
    bnb_4bit_quant_type="nf4",          # NF4 é‡åŒ–ç±»å‹ï¼ˆæ¨èï¼‰
    bnb_4bit_compute_dtype=torch.bfloat16,  # è®¡ç®—æ—¶ä½¿ç”¨ bf16
    bnb_4bit_use_double_quant=True,     # åŒé‡é‡åŒ–ï¼ˆè¿›ä¸€æ­¥å‹ç¼©ï¼‰
)

print("\nğŸ“¦ åŠ è½½ 4-bit é‡åŒ–æ¨¡å‹...")

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_PATH,
    trust_remote_code=True,
    quantization_config=quantization_config,  # åº”ç”¨ 4-bit é‡åŒ–
    device_map="auto",
)

# æ‰“å°é‡åŒ–åæ˜¾å­˜å ç”¨
print(f"æ¨¡å‹åŠ è½½å®Œæˆ")
if torch.cuda.is_available():
    print(f"GPU æ˜¾å­˜å ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")


# ===== 2. å‡†å¤‡æ¨¡å‹ç”¨äº k-bit è®­ç»ƒ =====
print("\nğŸ”§ å‡†å¤‡ QLoRA è®­ç»ƒ...")

# å…³é”®æ­¥éª¤ï¼šä¸º k-bit è®­ç»ƒå‡†å¤‡æ¨¡å‹
model = prepare_model_for_kbit_training(
    model,
    use_gradient_checkpointing=True  # ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœæ˜¾å­˜
)


# ===== 3. é…ç½® LoRA =====
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    lora_dropout=LORA_DROPOUT,
    target_modules=TARGET_MODULES,
    bias="none",
)

model = get_peft_model(model, lora_config)

# æ‰“å°å‚æ•°ä¿¡æ¯
print("\nğŸ“Š å‚æ•°ç»Ÿè®¡:")
model.print_trainable_parameters()


# ===== 4. åŠ è½½æ•°æ® =====
print("\nğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®...")

raw_dataset = load_dataset(
    "YeungNLP/firefly-train-1.1M",
    split=f"train[:{NUM_SAMPLES}]"
)


def preprocess_function(examples):
    """é¢„å¤„ç†æ•°æ®"""
    input_ids_list = []
    labels_list = []
    attention_mask_list = []
    
    for inp, target in zip(examples["input"], examples["target"]):
        if len(inp) > 300 or len(target) > 300:
            continue
        
        prompt = f"<|im_start|>user\n{inp}<|im_end|>\n<|im_start|>assistant\n"
        full_text = f"{prompt}{target}<|im_end|>"
        
        tokenized = tokenizer(
            full_text,
            max_length=MAX_LENGTH,
            truncation=True,
            padding=False,
        )
        
        input_ids = tokenized["input_ids"]
        attention_mask = tokenized["attention_mask"]
        
        prompt_tokens = tokenizer(prompt, add_special_tokens=False)["input_ids"]
        prompt_len = len(prompt_tokens)
        
        labels = [-100] * prompt_len + input_ids[prompt_len:]
        
        if len(labels) < len(input_ids):
            labels = labels + [-100] * (len(input_ids) - len(labels))
        elif len(labels) > len(input_ids):
            labels = labels[:len(input_ids)]
        
        input_ids_list.append(input_ids)
        labels_list.append(labels)
        attention_mask_list.append(attention_mask)
    
    return {
        "input_ids": input_ids_list,
        "labels": labels_list,
        "attention_mask": attention_mask_list,
    }


dataset = raw_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=raw_dataset.column_names,
    desc="Processing data"
)

dataset = dataset.filter(lambda x: len(x["input_ids"]) > 0)
print(f"âœ… å¤„ç†åæ ·æœ¬æ•°: {len(dataset)}")


# ===== 5. è®­ç»ƒé…ç½® =====
# QLoRA ç‰¹å®šä¼˜åŒ–
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    overwrite_output_dir=True,
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    learning_rate=LEARNING_RATE,
    lr_scheduler_type="cosine",
    warmup_ratio=0.05,
    bf16=True,
    logging_steps=20,
    save_steps=500,
    save_total_limit=2,
    report_to="none",
    gradient_checkpointing=True,
    # QLoRA ç‰¹å®šé…ç½®
    optim="paged_adamw_8bit",  # åˆ†é¡µ 8-bit AdamW ä¼˜åŒ–å™¨
    max_grad_norm=0.3,        # æ¢¯åº¦è£å‰ª
)


# ===== 6. è®­ç»ƒ =====
print("\n" + "=" * 60)
print("ğŸ‹ï¸ å¼€å§‹ QLoRA è®­ç»ƒ")
print("=" * 60)
print(f"   - é‡åŒ–: 4-bit NF4")
print(f"   - è®­ç»ƒæ ·æœ¬: {len(dataset)}")
print(f"   - Batch Size: {BATCH_SIZE}")
print(f"   - æ¢¯åº¦ç´¯ç§¯: {GRADIENT_ACCUMULATION_STEPS}")
print(f"   - æœ‰æ•ˆ Batch: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")
print(f"   - Learning Rate: {LEARNING_RATE}")
print(f"   - LoRA Rank: {LORA_R}")

if torch.cuda.is_available():
    print(f"   - è®­ç»ƒå‰æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    padding=True,
    return_tensors="pt"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=data_collator,
)

trainer.train()


# ===== 7. ä¿å­˜ =====
print(f"\nğŸ’¾ ä¿å­˜ QLoRA æƒé‡åˆ° {OUTPUT_DIR}...")

model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print("âœ… QLoRA è®­ç»ƒå®Œæˆï¼")
print(f"\nğŸ“ æƒé‡å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")

# æ˜¾å­˜å ç”¨æ€»ç»“
if torch.cuda.is_available():
    peak_memory = torch.cuda.max_memory_allocated() / 1024**3
    print(f"\nğŸ“Š æ˜¾å­˜å³°å€¼: {peak_memory:.2f} GB")

print("""
ğŸ’¡ QLoRA vs LoRA å¯¹æ¯”ï¼š
   
   | æ–¹æ³•    | åŸºåº§å­˜å‚¨ | æ˜¾å­˜å ç”¨ | é€‚ç”¨åœºæ™¯ |
   |---------|----------|----------|----------|
   | LoRA    | FP16/BF16| ä¸­ç­‰     | ä¸­ç­‰æ˜¾å­˜ |
   | QLoRA   | 4-bit    | è¾ƒå°     | æœ‰é™æ˜¾å­˜ |
   
   å¯¹äº 0.5B å°æ¨¡å‹ï¼ŒQLoRA ä¼˜åŠ¿ä¸æ˜æ˜¾
   å¯¹äº 7B+ å¤§æ¨¡å‹ï¼ŒQLoRA å¯èŠ‚çœ 60%+ æ˜¾å­˜ï¼
""")
