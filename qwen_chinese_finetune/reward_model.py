# reward_model.py
"""
RLHF ç¬¬ä¸€æ­¥ï¼šè®­ç»ƒå¥–åŠ±æ¨¡å‹ (Reward Model)

å¥–åŠ±æ¨¡å‹çš„ä½œç”¨ï¼š
- å­¦ä¹ äººç±»åå¥½ï¼Œç»™æ¨¡å‹å›ç­”æ‰“åˆ†
- è¾“å…¥ï¼šprompt + response
- è¾“å‡ºï¼šä¸€ä¸ªæ ‡é‡åˆ†æ•°ï¼ˆè¶Šé«˜è¡¨ç¤ºäººç±»è¶Šå–œæ¬¢ï¼‰

è®­ç»ƒæ–¹å¼ï¼š
- ä½¿ç”¨åå¥½æ•°æ®ï¼ˆchosen vs rejectedï¼‰
- ç›®æ ‡ï¼šè®© chosen çš„åˆ†æ•°é«˜äº rejected
- æŸå¤±å‡½æ•°ï¼š-log(sigmoid(r_chosen - r_rejected))

æ•°æ®é›†ï¼šshibing624/DPO-En-Zh-20k-Preferenceï¼ˆä¸ DPO ä½¿ç”¨ç›¸åŒæ•°æ®é›†ï¼‰
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
import torch.nn as nn
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    PreTrainedModel,
    AutoModel,
    AutoConfig
)
from typing import Optional, Dict, List
import numpy as np

# ===== é…ç½® =====
SFT_MODEL_PATH = "./qwen_sft"           # ä» SFT æ¨¡å‹åˆå§‹åŒ– RM
OUTPUT_DIR = "./qwen_reward_model"
MAX_LENGTH = 512
BATCH_SIZE = 4
GRADIENT_ACCUMULATION_STEPS = 4         # æœ‰æ•ˆ batch = 16
LEARNING_RATE = 1e-5
NUM_EPOCHS = 1
NUM_SAMPLES = 5000                      # ä½¿ç”¨å¤šå°‘æ ·æœ¬è®­ç»ƒ


# ===== 1. è‡ªå®šä¹‰å¥–åŠ±æ¨¡å‹ =====
class QwenRewardModel(PreTrainedModel):
    """
    å¥–åŠ±æ¨¡å‹æ¶æ„ï¼š
    
    Qwen Base Model (å†»ç»“æˆ–å¾®è°ƒ)
           â†“
    å–æœ€åä¸€ä¸ª token çš„éšè—çŠ¶æ€
           â†“
    Linear å±‚ (hidden_size â†’ 1)
           â†“
    è¾“å‡ºï¼šæ ‡é‡å¥–åŠ±åˆ†æ•°
    """
    
    def __init__(self, config, base_model=None):
        super().__init__(config)
        
        if base_model is not None:
            self.model = base_model
        else:
            self.model = AutoModel.from_pretrained(
                SFT_MODEL_PATH,
                trust_remote_code=True,
                torch_dtype=torch.bfloat16
            )
        
        # å¥–åŠ±å¤´ï¼šå°†éšè—çŠ¶æ€æ˜ å°„åˆ°æ ‡é‡åˆ†æ•°
        self.reward_head = nn.Linear(config.hidden_size, 1, bias=False)
        
    def forward(
        self,
        input_ids: torch.LongTensor,
        attention_mask: Optional[torch.Tensor] = None,
        **kwargs
    ):
        # è·å–æœ€åä¸€å±‚éšè—çŠ¶æ€
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
            **kwargs
        )
        
        # å–æœ€åä¸€ä¸ª token çš„éšè—çŠ¶æ€ï¼ˆç±»ä¼¼ [CLS] çš„ä½œç”¨ï¼‰
        # å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œæ‰¾åˆ°æœ€åä¸€ä¸ªé padding token
        if attention_mask is not None:
            # æ‰¾åˆ°æ¯ä¸ªåºåˆ—æœ€åä¸€ä¸ª 1 çš„ä½ç½®
            sequence_lengths = attention_mask.sum(dim=1) - 1
            batch_size = input_ids.shape[0]
            
            # è·å–æœ€åä¸€ä¸ª token çš„éšè—çŠ¶æ€
            last_hidden_state = outputs.hidden_states[-1]
            pooled_output = last_hidden_state[
                torch.arange(batch_size, device=input_ids.device),
                sequence_lengths
            ]
        else:
            # æ²¡æœ‰ attention_maskï¼Œç›´æ¥å–æœ€åä¸€ä¸ª
            pooled_output = outputs.hidden_states[-1][:, -1, :]
        
        # é€šè¿‡å¥–åŠ±å¤´å¾—åˆ°åˆ†æ•°
        rewards = self.reward_head(pooled_output).squeeze(-1)
        
        return rewards


# ===== 2. ç®€åŒ–ç‰ˆï¼šä½¿ç”¨ AutoModelForSequenceClassification =====
# å¦‚æœä¸Šé¢çš„è‡ªå®šä¹‰æ¨¡å‹æœ‰é—®é¢˜ï¼Œå¯ä»¥ç”¨è¿™ä¸ªç®€åŒ–ç‰ˆ
def load_reward_model_simple():
    """ä½¿ç”¨ HuggingFace çš„åºåˆ—åˆ†ç±»æ¨¡å‹ä½œä¸ºå¥–åŠ±æ¨¡å‹"""
    from transformers import AutoModelForSequenceClassification
    
    model = AutoModelForSequenceClassification.from_pretrained(
        SFT_MODEL_PATH,
        num_labels=1,  # è¾“å‡ºä¸€ä¸ªåˆ†æ•°
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    return model


# ===== 3. åŠ è½½æ•°æ®é›† =====
print("Loading preference dataset...")
tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL_PATH, trust_remote_code=True)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# åŠ è½½ä¸ DPO ç›¸åŒçš„æ•°æ®é›†
raw_dataset = load_dataset(
    "shibing624/DPO-En-Zh-20k-Preference",
    name="zh",
    split=f"train[:{NUM_SAMPLES}]"
)


def preprocess_reward_data(examples):
    """
    å°†åå¥½æ•°æ®å¤„ç†æˆå¥–åŠ±æ¨¡å‹è®­ç»ƒæ ¼å¼
    
    å¯¹äºæ¯æ¡æ•°æ®ï¼Œç”Ÿæˆä¸¤ä¸ªæ ·æœ¬ï¼š
    - chosen æ ·æœ¬ï¼ˆæ ‡ç­¾ä¸º 1ï¼‰
    - rejected æ ·æœ¬ï¼ˆæ ‡ç­¾ä¸º 0ï¼‰
    
    ä½†å®é™…ä¸Šæˆ‘ä»¬ä½¿ç”¨ pairwise lossï¼Œæ‰€ä»¥ä¸éœ€è¦æ˜¾å¼æ ‡ç­¾
    """
    chosen_input_ids = []
    chosen_attention_mask = []
    rejected_input_ids = []
    rejected_attention_mask = []
    
    for system, history, question, chosen, rejected in zip(
        examples["system"],
        examples["history"],
        examples["question"],
        examples["response_chosen"],
        examples["response_rejected"]
    ):
        # æ„å»º promptï¼ˆä¸ DPO ç›¸åŒçš„æ ¼å¼ï¼‰
        prompt_parts = []
        if system and system.strip():
            prompt_parts.append(f"<|im_start|>system\n{system}<|im_end|>")
        
        if history:
            for turn in history:
                if len(turn) >= 2:
                    prompt_parts.append(f"<|im_start|>user\n{turn[0]}<|im_end|>")
                    prompt_parts.append(f"<|im_start|>assistant\n{turn[1]}<|im_end|>")
        
        prompt_parts.append(f"<|im_start|>user\n{question}<|im_end|>")
        prompt = "\n".join(prompt_parts)
        
        # chosen å®Œæ•´åºåˆ—
        chosen_text = f"{prompt}\n<|im_start|>assistant\n{chosen}<|im_end|>"
        # rejected å®Œæ•´åºåˆ—
        rejected_text = f"{prompt}\n<|im_start|>assistant\n{rejected}<|im_end|>"
        
        # Tokenize
        chosen_tokens = tokenizer(
            chosen_text,
            max_length=MAX_LENGTH,
            truncation=True,
            padding="max_length"
        )
        rejected_tokens = tokenizer(
            rejected_text,
            max_length=MAX_LENGTH,
            truncation=True,
            padding="max_length"
        )
        
        chosen_input_ids.append(chosen_tokens["input_ids"])
        chosen_attention_mask.append(chosen_tokens["attention_mask"])
        rejected_input_ids.append(rejected_tokens["input_ids"])
        rejected_attention_mask.append(rejected_tokens["attention_mask"])
    
    return {
        "chosen_input_ids": chosen_input_ids,
        "chosen_attention_mask": chosen_attention_mask,
        "rejected_input_ids": rejected_input_ids,
        "rejected_attention_mask": rejected_attention_mask,
    }


print("Preprocessing data...")
dataset = raw_dataset.map(
    preprocess_reward_data,
    batched=True,
    remove_columns=raw_dataset.column_names,
    desc="Processing preference pairs"
)

print(f"âœ… Processed {len(dataset)} preference pairs")


# ===== 4. è‡ªå®šä¹‰ Trainerï¼ˆPairwise Ranking Lossï¼‰=====
class RewardModelTrainer(Trainer):
    """
    å¥–åŠ±æ¨¡å‹è®­ç»ƒå™¨
    
    ä½¿ç”¨ Pairwise Ranking Loss:
    loss = -log(sigmoid(r_chosen - r_rejected))
    
    ç›®æ ‡ï¼šè®© chosen çš„åˆ†æ•°é«˜äº rejected
    """
    
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        # è·å– chosen å’Œ rejected çš„åˆ†æ•°
        chosen_rewards = model(
            input_ids=inputs["chosen_input_ids"],
            attention_mask=inputs["chosen_attention_mask"]
        )
        rejected_rewards = model(
            input_ids=inputs["rejected_input_ids"],
            attention_mask=inputs["rejected_attention_mask"]
        )
        
        # Pairwise Ranking Loss
        # æˆ‘ä»¬å¸Œæœ› chosen_rewards > rejected_rewards
        # loss = -log(sigmoid(chosen - rejected))
        loss = -torch.nn.functional.logsigmoid(chosen_rewards - rejected_rewards).mean()
        
        if return_outputs:
            return loss, {
                "chosen_rewards": chosen_rewards,
                "rejected_rewards": rejected_rewards
            }
        return loss


# ===== 5. æ•°æ®æ•´ç†å™¨ =====
class RewardDataCollator:
    """å°† batch æ•°æ®æ•´ç†æˆæ¨¡å‹éœ€è¦çš„æ ¼å¼"""
    
    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:
        batch = {
            "chosen_input_ids": torch.tensor([f["chosen_input_ids"] for f in features]),
            "chosen_attention_mask": torch.tensor([f["chosen_attention_mask"] for f in features]),
            "rejected_input_ids": torch.tensor([f["rejected_input_ids"] for f in features]),
            "rejected_attention_mask": torch.tensor([f["rejected_attention_mask"] for f in features]),
        }
        return batch


# ===== 6. è®­ç»ƒ =====
def train_reward_model():
    print("=" * 60)
    print("ğŸ† å¼€å§‹è®­ç»ƒå¥–åŠ±æ¨¡å‹ (Reward Model)")
    print("=" * 60)
    
    # åŠ è½½é…ç½®å’ŒåŸºç¡€æ¨¡å‹
    config = AutoConfig.from_pretrained(SFT_MODEL_PATH, trust_remote_code=True)
    base_model = AutoModel.from_pretrained(
        SFT_MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    # åˆ›å»ºå¥–åŠ±æ¨¡å‹
    reward_model = QwenRewardModel(config, base_model)
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        learning_rate=LEARNING_RATE,
        bf16=True,
        logging_steps=10,
        save_steps=500,
        save_total_limit=2,
        report_to="none",
        remove_unused_columns=False,
        gradient_checkpointing=True,
    )
    
    # åˆ›å»º Trainer
    trainer = RewardModelTrainer(
        model=reward_model,
        args=training_args,
        train_dataset=dataset,
        data_collator=RewardDataCollator(),
    )
    
    # å¼€å§‹è®­ç»ƒ
    print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"   - æ ·æœ¬æ•°: {len(dataset)}")
    print(f"   - Batch Size: {BATCH_SIZE}")
    print(f"   - æ¢¯åº¦ç´¯ç§¯: {GRADIENT_ACCUMULATION_STEPS}")
    print(f"   - æœ‰æ•ˆ Batch: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")
    print(f"   - Learning Rate: {LEARNING_RATE}")
    
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print(f"âœ… å¥–åŠ±æ¨¡å‹å·²ä¿å­˜åˆ° {OUTPUT_DIR}")
    
    return reward_model


# ===== 7. æµ‹è¯•å¥–åŠ±æ¨¡å‹ =====
def test_reward_model():
    """æµ‹è¯•å¥–åŠ±æ¨¡å‹çš„æ‰“åˆ†æ•ˆæœ"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•å¥–åŠ±æ¨¡å‹")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹
    config = AutoConfig.from_pretrained(OUTPUT_DIR, trust_remote_code=True)
    base_model = AutoModel.from_pretrained(
        OUTPUT_DIR,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    reward_model = QwenRewardModel(config, base_model)
    reward_model.eval()
    
    tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR, trust_remote_code=True)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "prompt": "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
            "good_response": "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚è¿™åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€é—®é¢˜è§£å†³ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ç­‰èƒ½åŠ›ã€‚",
            "bad_response": "ä¸çŸ¥é“ã€‚"
        },
        {
            "prompt": "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
            "good_response": "å­¦ä¹ ç¼–ç¨‹å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤å¼€å§‹ï¼š1ï¼‰é€‰æ‹©ä¸€é—¨å…¥é—¨è¯­è¨€å¦‚Pythonï¼›2ï¼‰å­¦ä¹ åŸºç¡€è¯­æ³•å’Œæ¦‚å¿µï¼›3ï¼‰å¤šåšç»ƒä¹ é¡¹ç›®ï¼›4ï¼‰é˜…è¯»ä¼˜ç§€ä»£ç ï¼›5ï¼‰å‚ä¸å¼€æºç¤¾åŒºã€‚",
            "bad_response": "éšä¾¿å­¦å­¦å°±è¡Œäº†ã€‚"
        }
    ]
    
    for i, case in enumerate(test_cases):
        print(f"\n--- æµ‹è¯• {i+1} ---")
        print(f"é—®é¢˜: {case['prompt']}")
        
        # æ„å»ºå®Œæ•´è¾“å…¥
        good_text = f"<|im_start|>user\n{case['prompt']}<|im_end|>\n<|im_start|>assistant\n{case['good_response']}<|im_end|>"
        bad_text = f"<|im_start|>user\n{case['prompt']}<|im_end|>\n<|im_start|>assistant\n{case['bad_response']}<|im_end|>"
        
        good_tokens = tokenizer(good_text, return_tensors="pt", max_length=MAX_LENGTH, truncation=True)
        bad_tokens = tokenizer(bad_text, return_tensors="pt", max_length=MAX_LENGTH, truncation=True)
        
        with torch.no_grad():
            good_tokens = {k: v.to(reward_model.device) for k, v in good_tokens.items()}
            bad_tokens = {k: v.to(reward_model.device) for k, v in bad_tokens.items()}
            
            good_score = reward_model(**good_tokens).item()
            bad_score = reward_model(**bad_tokens).item()
        
        print(f"å¥½å›ç­”åˆ†æ•°: {good_score:.4f}")
        print(f"å·®å›ç­”åˆ†æ•°: {bad_score:.4f}")
        print(f"å·®å€¼: {good_score - bad_score:.4f} {'âœ…' if good_score > bad_score else 'âŒ'}")


if __name__ == "__main__":
    # è®­ç»ƒå¥–åŠ±æ¨¡å‹
    train_reward_model()
    
    # æµ‹è¯•å¥–åŠ±æ¨¡å‹
    test_reward_model()
