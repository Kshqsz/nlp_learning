# sft_chinese_qwen.py
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForSeq2Seq,
    TrainingArguments,
    Trainer,
    set_seed
)
import torch

# ===== é…ç½® =====
MODEL_PATH = "./qwen_pretrained"     # â† ä¿®æ­£ï¼šæŒ‡å‘é¢„è®­ç»ƒæ¨¡å‹çš„æ­£ç¡®è·¯å¾„
OUTPUT_DIR = "./qwen_sft"
MAX_LENGTH = 512
NUM_TRAIN_EPOCHS = 2
BATCH_SIZE = 8
LEARNING_RATE = 2e-5

set_seed(42)

# ===== 1. åŠ è½½ tokenizer å’Œæ¨¡å‹ =====
print(f"Loading tokenizer and model from: {MODEL_PATH}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = model.config.eos_token_id

# ===== 2. åŠ è½½ Firefly ä¸­æ–‡æŒ‡ä»¤æ•°æ®é›† =====
print("Loading Firefly dataset (Chinese instruction tuning data)...")
raw_dataset = load_dataset("json", data_files="./firefly-train-1.1M.jsonl", split="train[:10000]")

# ä¸å†è¿‡æ»¤ï¼Œç›´æ¥ä½¿ç”¨å…¨éƒ¨æ ·æœ¬ï¼Œé˜²æ­¢æ•°æ®ä¸ºç©º
dataset = raw_dataset
print(f"Loaded {len(dataset)} samples from local file.")

# ===== 3. åº”ç”¨ Qwen å¯¹è¯æ¨¡æ¿å¹¶ tokenize =====
def format_and_tokenize(examples):
    # é€‚é…æœ¬åœ°æ•°æ®é›†å­—æ®µåï¼šinput -> instruction, target -> output
    instructions = examples["input"]
    outputs = examples["target"]
    
    input_ids_list = []
    attention_mask_list = []
    labels_list = []
    
    for inst, out in zip(instructions, outputs):
        # æ„å»º Qwen å¯¹è¯æ ¼å¼
        prompt = f"<|im_start|>user\n{inst}<|im_end|>\n<|im_start|>assistant\n"
        full_text = prompt + out + "<|im_end|>"
        
        # Tokenize å®Œæ•´æ–‡æœ¬
        tokenized = tokenizer(
            full_text,
            truncation=True,
            max_length=MAX_LENGTH,
            padding=False,
        )
        
        input_ids = tokenized["input_ids"]
        attention_mask = tokenized["attention_mask"]
        
        # åˆ›å»º labelsï¼šåªå¯¹ assistant çš„å›å¤è®¡ç®— loss
        prompt_tokenized = tokenizer(prompt, add_special_tokens=False)
        prompt_len = len(prompt_tokenized["input_ids"])
        
        labels = [-100] * prompt_len + input_ids[prompt_len:]
        labels = labels[:len(input_ids)]
        
        input_ids_list.append(input_ids)
        attention_mask_list.append(attention_mask)
        labels_list.append(labels)
    
    return {
        "input_ids": input_ids_list,
        "attention_mask": attention_mask_list,
        "labels": labels_list
    }

print("Tokenizing dataset...")
tokenized_dataset = dataset.map(
    format_and_tokenize,
    batched=True,
    remove_columns=dataset.column_names,
    desc="Tokenizing"
)

# ===== 4. æ•°æ®æ•´ç†å™¨ =====
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    padding=True,
    return_tensors="pt"
)

# ===== 5. è®­ç»ƒé…ç½® =====
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    overwrite_output_dir=True,
    num_train_epochs=NUM_TRAIN_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    learning_rate=LEARNING_RATE,
    weight_decay=0.01,
    logging_steps=50,
    save_steps=500,
    save_total_limit=2,
    bf16=True,  # ä½¿ç”¨ bf16 ä»£æ›¿ fp16ï¼Œæ›´ç¨³å®š
    report_to="none",
    dataloader_num_workers=4,
    optim="adamw_torch",
    gradient_accumulation_steps=2,
    remove_unused_columns=False,
)

# ===== 6. åˆ›å»º Trainer =====
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
    processing_class=tokenizer,
)

# ===== 7. å¼€å§‹è®­ç»ƒ =====
print("ğŸš€ Starting SFT training...")
trainer.train()

# ===== 8. ä¿å­˜æ¨¡å‹ =====
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print(f"âœ… SFT æ¨¡å‹å·²ä¿å­˜åˆ° {OUTPUT_DIR}")