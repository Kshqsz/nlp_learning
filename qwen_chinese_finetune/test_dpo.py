# test_dpo.py
"""
DPO æ¨¡å‹æµ‹è¯•è„šæœ¬

åŠŸèƒ½ï¼š
1. å¯¹æ¯”æµ‹è¯• SFT æ¨¡å‹ vs DPO æ¨¡å‹
2. ä½¿ç”¨ç›¸åŒçš„ prompt ç”Ÿæˆå›ç­”ï¼Œè§‚å¯Ÿ DPO å¯¹é½æ•ˆæœ
3. æ”¯æŒè‡ªå®šä¹‰æµ‹è¯•é—®é¢˜
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ===== é…ç½® =====
SFT_MODEL_PATH = "./qwen_sft"    # SFT æ¨¡å‹è·¯å¾„
DPO_MODEL_PATH = "./qwen_dpo"    # DPO æ¨¡å‹è·¯å¾„

# ç”Ÿæˆå‚æ•°
GENERATION_CONFIG = {
    "max_new_tokens": 256,
    "do_sample": True,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1,
}

# æµ‹è¯•é—®é¢˜åˆ—è¡¨
TEST_PROMPTS = [
    "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
    "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
    "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
    "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
    "å¦‚ä½•ä¿æŒå¥åº·çš„ç”Ÿæ´»æ–¹å¼ï¼Ÿ",
    "è¯·ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šé‡å­åŠ›å­¦",
]


def build_prompt(user_input: str, system: str = None) -> str:
    """æ„å»º Qwen ChatML æ ¼å¼çš„ prompt"""
    parts = []
    if system:
        parts.append(f"<|im_start|>system\n{system}<|im_end|>")
    parts.append(f"<|im_start|>user\n{user_input}<|im_end|>")
    parts.append("<|im_start|>assistant\n")
    return "\n".join(parts)


def load_model(model_path: str):
    """åŠ è½½æ¨¡å‹å’Œ tokenizer"""
    print(f"Loading model from: {model_path}")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    model.eval()
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    return model, tokenizer


def generate_response(model, tokenizer, prompt: str) -> str:
    """ç”Ÿæˆå›ç­”"""
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            **GENERATION_CONFIG
        )
    
    # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
    generated_ids = outputs[0][inputs["input_ids"].shape[1]:]
    response = tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    # æ¸…ç†ç»“æŸæ ‡è®°
    if "<|im_end|>" in response:
        response = response.split("<|im_end|>")[0]
    
    return response.strip()


def compare_models():
    """å¯¹æ¯” SFT å’Œ DPO æ¨¡å‹"""
    print("=" * 60)
    print("ğŸ”¬ SFT vs DPO æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(SFT_MODEL_PATH):
        print(f"âŒ SFT æ¨¡å‹ä¸å­˜åœ¨: {SFT_MODEL_PATH}")
        print("   è¯·å…ˆè¿è¡Œ sft_chinese_qwen.py è®­ç»ƒ SFT æ¨¡å‹")
        return
    
    if not os.path.exists(DPO_MODEL_PATH):
        print(f"âŒ DPO æ¨¡å‹ä¸å­˜åœ¨: {DPO_MODEL_PATH}")
        print("   è¯·å…ˆè¿è¡Œ dpo_chinese_qwen.py è®­ç»ƒ DPO æ¨¡å‹")
        return
    
    # åŠ è½½ä¸¤ä¸ªæ¨¡å‹
    print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
    sft_model, sft_tokenizer = load_model(SFT_MODEL_PATH)
    dpo_model, dpo_tokenizer = load_model(DPO_MODEL_PATH)
    
    # å¯¹æ¯”æµ‹è¯•
    print("\n" + "=" * 60)
    print("ğŸ“ å¼€å§‹å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    for i, question in enumerate(TEST_PROMPTS, 1):
        print(f"\n{'â”€' * 60}")
        print(f"é—®é¢˜ {i}: {question}")
        print("â”€" * 60)
        
        prompt = build_prompt(question)
        
        # SFT æ¨¡å‹å›ç­”
        sft_response = generate_response(sft_model, sft_tokenizer, prompt)
        print(f"\nğŸ”µ SFT æ¨¡å‹å›ç­”:")
        print(f"   {sft_response[:500]}{'...' if len(sft_response) > 500 else ''}")
        
        # DPO æ¨¡å‹å›ç­”
        dpo_response = generate_response(dpo_model, dpo_tokenizer, prompt)
        print(f"\nğŸŸ¢ DPO æ¨¡å‹å›ç­”:")
        print(f"   {dpo_response[:500]}{'...' if len(dpo_response) > 500 else ''}")
    
    print("\n" + "=" * 60)
    print("âœ… å¯¹æ¯”æµ‹è¯•å®Œæˆ!")
    print("=" * 60)


def test_dpo_only():
    """åªæµ‹è¯• DPO æ¨¡å‹"""
    print("=" * 60)
    print("ğŸš€ DPO æ¨¡å‹æµ‹è¯•")
    print("=" * 60)
    
    if not os.path.exists(DPO_MODEL_PATH):
        print(f"âŒ DPO æ¨¡å‹ä¸å­˜åœ¨: {DPO_MODEL_PATH}")
        return
    
    model, tokenizer = load_model(DPO_MODEL_PATH)
    
    for i, question in enumerate(TEST_PROMPTS, 1):
        print(f"\n{'â”€' * 60}")
        print(f"é—®é¢˜ {i}: {question}")
        print("â”€" * 60)
        
        prompt = build_prompt(question)
        response = generate_response(model, tokenizer, prompt)
        print(f"å›ç­”: {response}")
    
    print("\n" + "=" * 60)
    print("âœ… æµ‹è¯•å®Œæˆ!")


def interactive_test():
    """äº¤äº’å¼æµ‹è¯•"""
    print("=" * 60)
    print("ğŸ’¬ DPO æ¨¡å‹äº¤äº’æµ‹è¯•")
    print("=" * 60)
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("è¾“å…¥ 'compare' åˆ‡æ¢åˆ°å¯¹æ¯”æ¨¡å¼")
    print("=" * 60)
    
    if not os.path.exists(DPO_MODEL_PATH):
        print(f"âŒ DPO æ¨¡å‹ä¸å­˜åœ¨: {DPO_MODEL_PATH}")
        return
    
    model, tokenizer = load_model(DPO_MODEL_PATH)
    
    while True:
        try:
            user_input = input("\nğŸ™‹ ä½ : ").strip()
            
            if not user_input:
                continue
            if user_input.lower() in ["quit", "exit"]:
                print("ğŸ‘‹ å†è§!")
                break
            if user_input.lower() == "compare":
                compare_models()
                continue
            
            prompt = build_prompt(user_input)
            response = generate_response(model, tokenizer, prompt)
            print(f"\nğŸ¤– DPOæ¨¡å‹: {response}")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§!")
            break


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="DPO æ¨¡å‹æµ‹è¯•è„šæœ¬")
    parser.add_argument(
        "--mode", 
        choices=["compare", "dpo", "interactive"],
        default="compare",
        help="æµ‹è¯•æ¨¡å¼: compare(å¯¹æ¯”SFTå’ŒDPO), dpo(åªæµ‹DPO), interactive(äº¤äº’å¼)"
    )
    parser.add_argument(
        "--sft-path",
        default="./qwen_sft",
        help="SFT æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--dpo-path",
        default="./qwen_dpo",
        help="DPO æ¨¡å‹è·¯å¾„"
    )
    
    args = parser.parse_args()
    
    # æ›´æ–°è·¯å¾„
    SFT_MODEL_PATH = args.sft_path
    DPO_MODEL_PATH = args.dpo_path
    
    if args.mode == "compare":
        compare_models()
    elif args.mode == "dpo":
        test_dpo_only()
    else:
        interactive_test()
