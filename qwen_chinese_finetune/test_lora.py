# test_lora.py
"""
æµ‹è¯• LoRA å¾®è°ƒåçš„æ¨¡å‹

åŒ…å«ï¼š
1. åŠ è½½ LoRA æƒé‡ï¼ˆä¸åˆå¹¶ï¼‰
2. å¯¹æ¯”åŸå§‹æ¨¡å‹å’Œ LoRA æ¨¡å‹çš„è¾“å‡º
3. æµ‹è¯•ä¸åŒç±»å‹çš„é—®é¢˜
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# ===== é…ç½® =====
BASE_MODEL_PATH = "Qwen/Qwen1.5-0.5B"    # åŸºåº§æ¨¡å‹
LORA_PATH = "./qwen_lora_sft"            # LoRA æƒé‡è·¯å¾„
# LORA_PATH = "./qwen_qlora_sft"         # æˆ– QLoRA æƒé‡


def load_model_with_lora():
    """åŠ è½½åŸºåº§æ¨¡å‹ + LoRA æƒé‡"""
    print("ğŸ“¦ åŠ è½½åŸºåº§æ¨¡å‹...")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    print("ğŸ”§ åŠ è½½ LoRA æƒé‡...")
    model = PeftModel.from_pretrained(
        base_model,
        LORA_PATH,
        torch_dtype=torch.bfloat16,
    )
    
    tokenizer = AutoTokenizer.from_pretrained(LORA_PATH, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    return model, tokenizer


def load_base_model():
    """åªåŠ è½½åŸºåº§æ¨¡å‹ï¼ˆå¯¹æ¯”ç”¨ï¼‰"""
    print("ğŸ“¦ åŠ è½½åŸå§‹åŸºåº§æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    return model, tokenizer


def generate_response(model, tokenizer, prompt, max_new_tokens=256):
    """ç”Ÿæˆå›å¤"""
    # æ„å»ºå¯¹è¯æ ¼å¼
    full_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    
    inputs = tokenizer(full_prompt, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # æå– assistant å›å¤
    if "assistant" in response:
        response = response.split("assistant")[-1].strip()
    
    return response


def main():
    print("=" * 60)
    print("ğŸ§ª LoRA æ¨¡å‹æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•é—®é¢˜
    test_prompts = [
        "è¯·ç”¨ç®€çŸ­çš„è¯ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
        "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
        "Python å’Œ Java æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "å¦‚ä½•ä¿æŒå¥åº·çš„ç”Ÿæ´»æ–¹å¼ï¼Ÿ",
        "å¸®æˆ‘å†™ä¸€æ®µæ•…äº‹å¼€å¤´ï¼Œä¸»è§’æ˜¯ä¸€ä¸ªæœºå™¨äºº",
    ]
    
    # åŠ è½½ LoRA æ¨¡å‹
    print("\n" + "-" * 40)
    lora_model, lora_tokenizer = load_model_with_lora()
    print("âœ… LoRA æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # æµ‹è¯• LoRA æ¨¡å‹
    print("\n" + "=" * 60)
    print("ğŸ“ LoRA æ¨¡å‹å›å¤")
    print("=" * 60)
    
    lora_responses = []
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\nã€é—®é¢˜ {i}ã€‘{prompt}")
        print("-" * 40)
        response = generate_response(lora_model, lora_tokenizer, prompt)
        print(f"ã€å›ç­”ã€‘{response}")
        lora_responses.append(response)
    
    # é‡Šæ”¾æ˜¾å­˜
    del lora_model
    torch.cuda.empty_cache()
    
    # åŠ è½½åŸå§‹æ¨¡å‹å¯¹æ¯”
    print("\n" + "=" * 60)
    print("ğŸ“ åŸå§‹æ¨¡å‹å›å¤ï¼ˆå¯¹æ¯”ï¼‰")
    print("=" * 60)
    
    base_model, base_tokenizer = load_base_model()
    
    base_responses = []
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\nã€é—®é¢˜ {i}ã€‘{prompt}")
        print("-" * 40)
        response = generate_response(base_model, base_tokenizer, prompt)
        print(f"ã€å›ç­”ã€‘{response}")
        base_responses.append(response)
    
    # å¯¹æ¯”æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š å¯¹æ¯”æ€»ç»“")
    print("=" * 60)
    
    print("""
    | æ¨¡å‹      | ç‰¹ç‚¹ |
    |-----------|------|
    | åŸå§‹æ¨¡å‹  | é€šç”¨é¢„è®­ç»ƒï¼Œå¯èƒ½ä¸æ“…é•¿å¯¹è¯ |
    | LoRA æ¨¡å‹ | ç»è¿‡æŒ‡ä»¤å¾®è°ƒï¼Œæ›´æ“…é•¿å¯¹è¯é—®ç­” |
    
    é€šè¿‡ LoRAï¼Œæˆ‘ä»¬åªè®­ç»ƒäº†çº¦ 1% çš„å‚æ•°ï¼Œ
    ä½†èƒ½è®©æ¨¡å‹æ›´å¥½åœ°éµå¾ªæŒ‡ä»¤ã€äº§ç”Ÿæœ‰å¸®åŠ©çš„å›å¤ã€‚
    """)


if __name__ == "__main__":
    main()
