# test_ppo.py
"""
æµ‹è¯• PPO è®­ç»ƒåçš„æ¨¡å‹

åŠŸèƒ½ï¼š
1. å•ç‹¬æµ‹è¯• PPO æ¨¡å‹
2. äº¤äº’å¼æµ‹è¯•
3. æ‰¹é‡æµ‹è¯•
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ===== é…ç½® =====
PPO_MODEL_PATH = "./qwen_ppo"

GENERATION_CONFIG = {
    "max_new_tokens": 256,
    "do_sample": True,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1,
}

TEST_PROMPTS = [
    "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
    "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
    "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
    "å†™ä¸€é¦–å…³äºç§‹å¤©çš„è¯—",
    "å¦‚ä½•ä¿æŒå¥åº·çš„ç”Ÿæ´»æ–¹å¼ï¼Ÿ",
]


def load_model(model_path: str):
    """åŠ è½½æ¨¡å‹"""
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
        return None, None
    
    print(f"åŠ è½½æ¨¡å‹ from {model_path}...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    model.eval()
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    return model, tokenizer


def generate_response(model, tokenizer, prompt: str) -> str:
    """ç”Ÿæˆå›ç­”"""
    full_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            **GENERATION_CONFIG
        )
    
    response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    if "<|im_end|>" in response:
        response = response.split("<|im_end|>")[0]
    
    return response.strip()


def batch_test():
    """æ‰¹é‡æµ‹è¯•"""
    print("=" * 60)
    print("ğŸ§ª PPO æ¨¡å‹æ‰¹é‡æµ‹è¯•")
    print("=" * 60)
    
    model, tokenizer = load_model(PPO_MODEL_PATH)
    if not model:
        return
    
    for i, prompt in enumerate(TEST_PROMPTS, 1):
        print(f"\n{'â”€' * 60}")
        print(f"é—®é¢˜ {i}: {prompt}")
        print("â”€" * 60)
        
        response = generate_response(model, tokenizer, prompt)
        print(f"å›ç­”: {response}")
    
    print("\n" + "=" * 60)
    print("âœ… æµ‹è¯•å®Œæˆ!")


def interactive_test():
    """äº¤äº’å¼æµ‹è¯•"""
    print("=" * 60)
    print("ğŸ’¬ PPO æ¨¡å‹äº¤äº’æµ‹è¯•")
    print("=" * 60)
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("=" * 60)
    
    model, tokenizer = load_model(PPO_MODEL_PATH)
    if not model:
        return
    
    while True:
        try:
            user_input = input("\nğŸ™‹ ä½ : ").strip()
            
            if not user_input:
                continue
            if user_input.lower() in ["quit", "exit"]:
                print("ğŸ‘‹ å†è§!")
                break
            
            response = generate_response(model, tokenizer, user_input)
            print(f"\nğŸ¤– PPOæ¨¡å‹: {response}")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§!")
            break


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æµ‹è¯• PPO æ¨¡å‹")
    parser.add_argument(
        "--mode",
        choices=["batch", "interactive"],
        default="batch",
        help="æµ‹è¯•æ¨¡å¼"
    )
    parser.add_argument(
        "--model-path",
        default="./qwen_ppo",
        help="PPO æ¨¡å‹è·¯å¾„"
    )
    
    args = parser.parse_args()
    PPO_MODEL_PATH = args.model_path
    
    if args.mode == "batch":
        batch_test()
    else:
        interactive_test()
