# test_sft.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ===== é…ç½® =====
MODEL_PATH = "./qwen_sft"  # SFT å¾®è°ƒåçš„æ¨¡å‹è·¯å¾„
# MODEL_PATH = "./qwen_pretrained"  # æˆ–è€…æµ‹è¯•é¢„è®­ç»ƒæ¨¡å‹

print(f"Loading model from: {MODEL_PATH}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    torch_dtype=torch.float16,
    device_map="auto"
)
model.eval()

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# ===== æµ‹è¯•å‡½æ•° =====
def chat(instruction: str, max_new_tokens: int = 256):
    """ä½¿ç”¨ Qwen å¯¹è¯æ ¼å¼è¿›è¡Œæ¨ç†"""
    prompt = f"<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n"
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    # è§£ç å¹¶æå– assistant å›å¤
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)
    
    # æå– assistant éƒ¨åˆ†
    if "<|im_start|>assistant\n" in full_response:
        response = full_response.split("<|im_start|>assistant\n")[-1]
        response = response.split("<|im_end|>")[0].strip()
    else:
        response = full_response[len(prompt):].strip()
    
    return response

# ===== æµ‹è¯•ç”¨ä¾‹ =====
if __name__ == "__main__":
    test_questions = [
        "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
        "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
        "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ",
    ]
    
    print("=" * 60)
    print("ğŸ§ª å¼€å§‹æµ‹è¯• Qwen SFT æ¨¡å‹")
    print("=" * 60)
    
    for i, question in enumerate(test_questions, 1):
        print(f"\nã€é—®é¢˜ {i}ã€‘{question}")
        print("-" * 40)
        response = chat(question)
        print(f"ã€å›ç­”ã€‘{response}")
        print("=" * 60)